{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CHAPTER5 회귀.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdyDe6UveMfXA+wkMK4bMu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeSeungwon89/Machine-learning_Theory/blob/master/CHAPTER5%20%ED%9A%8C%EA%B7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 회귀 소개**"
      ],
      "metadata": {
        "id": "df78pAKeesnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 일반 선형 회귀: 예측값과 실젯값의 **RSS(Residual Sum of Squares)**를 최소화할 수 있도록 회귀 계수를 최적화하며 규제를 적용하지 않은 모델입니다.\n",
        "\n",
        "- **릿지(Ridge)** 회귀: 선형 회귀에 **L2 규제**를 추가한 회귀 모델입니다. L2 규제는 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해 회귀 계수값을 더 작게 만듭니다.\n",
        "\n",
        "- **라쏘(Lasso)** 회귀: 선형 회귀에 **L1 규제(=피처 선택 기능)**를 추가한 회귀 모델입니다. L1 규제는 예측 형향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않도록 할 수 있습니다.\n",
        "\n",
        "- **엘라스틱넷(ElasticNet)**: **L2**, **L1** 규제를 결합한 회귀 모델입니다. 피처가 많은 데이터 세트에서 적용됩니다. L1 규제로 피처 개수를 줄이면서 L2 규제로 계수 값 크기를 조정합니다.\n",
        "\n",
        "- **로지스틱 회귀(Logistic Regression)**: 회귀 모델 아닌 분류용 선형 모델입니다. 이진 분류뿐만 아니라 희소 영역의 분류(텍스트 분류 등) 같은 영역에서 좋은 예측 성능을 발휘합니다."
      ],
      "metadata": {
        "id": "QEvUqOgIn8OL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. 단순 선형 회귀를 통한 회귀 이해**"
      ],
      "metadata": {
        "id": "nzrGVn7weslW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실젯값과 회귀 모델의 차이에 따른 오류 값을 남은 오류 또는 **잔차**라고 부릅니다. 최적의 회귀 모델은 잔차 합이 최소가 되는 모델이며 잔차 합이 최소가 될 수 있는 최적의 회귀 계수를 가진 것을 의미합니다.\n",
        "\n",
        "잔차는 양수나 음수 모두 될 수 있습니다. 보통 잔차를 계산할 때는 절댓값을 취해서 더하는 방식(**MAE, Mean Absolute Error**), 오류 값의 제곱을 구해서 더하는 방식(**RSS, Residual Sum of Squares**)을 사용합니다. 일반적으로는 RSS 방식을 취합니다. 회귀에서 RSS는 비용(Cost)이고 회귀 계수로 구성되는 RSS를 **비용 함수(손실 함수, Loss Function)**라고 부릅니다. 머신러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값인 잔차(오류 값)을 지속해서 감소시키고 더 감소하지 않는 최소 잔차를 구하는 것입니다.\n",
        "\n",
        "수학 수식은 생략하겠습니다."
      ],
      "metadata": {
        "id": "K5_v2fHXBGdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. 비용 최소화하기 - 경사 하강법(Gradient Descent) 소개**"
      ],
      "metadata": {
        "id": "CMYcQ9flesi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**경사 하강법**은 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 방식입니다. 점진적이고 반복적으로 계산하면서 회귀 계수 값을 업데이트하고 궁극적으로는 오류 값이 최소가 되는 회귀 계수를 도출하는 방식입니다. 딥러닝의 기반인 신경망에서도 경사 하강법을 통해 학습합니다.\n",
        "\n",
        "경사 하강법은 반복적으로 비용 함수의 반환 값(예측값과 실젯값의 차이)이 작아지는 방향성을 가지고 W 파라미터(회귀 계수)를 계속 조정합니다. 최초 오류 값을 100으로 가정한다면 두 번째 오류 값은 90, 세 번째 오류 값은 80처럼 지속해서 오류를 감소시키는 방향으로 계속 업데이트합니다. 오류 값이 더 감소할 수 없으면 최소 비용으로 판단하고 최적 파라미터로 반환합니다.\n",
        "\n",
        "참고로 실전에서는 대부분 대용량 데이터를 다루므로 속도가 빠른 **확률적 경사 하강법(Stochastic Gradient Descent)** 또는 **미니 배치 확률적 경사 하강법(Mini-Batch Stochastic Gradient Descent)**을 사용합니다. 전체 입력 데이터로 w가 업데이트되는 값을 계산하는 것이 아니라 일부 데이터만 이용하여 w가 업데이트되는 값을 계산하므로 일반 경사 하강법보다 속도가 빠릅니다. \n",
        "\n",
        "수학적 설명과 수식, 코드 구현은 생략하겠습니다. 자세한 내용은 서적을 참고하시기 바랍니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "QBS3S-uTHKfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. 사이킷런 LinearRegression을 이용한 보스턴 주택 가격 예측**"
      ],
      "metadata": {
        "id": "FfM3JS5Cesgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "선형 모델 중 규제가 적용되지 않은 `LinearRegression` 클래스를 사용하여 보스턴 주택 가격 예측 회귀를 구현해 보겠습니다."
      ],
      "metadata": {
        "id": "0vE7qUhcQ1eM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1. LinearRegression 클래스 - Ordinary Least Squares**"
      ],
      "metadata": {
        "id": "64D8OztaeseY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LinearRegression` 클래스는 예측값과 실젯값의 RSS를 최소화하여 **OLS(Ordinary Least Squares)** 추정 방식으로 구현한 클래스입니다. OLS 기반의 회귀 계수 계산은 입력 피처의 독립성에 많은 영향을 받습니다. 피처 간 상관관계가 매우 높은 경우 분산이 매우 커져서 오류에 매우 민감해집니다. 이 현상을 **다중 공선성(multi-collinearity)** 문제라고 합니다. 일반적으로 상관관계가 높은 피처가 많은 경우 독립적인 중요한 피처만 남기고 제거하거나 규제를 적용합니다. 또한 매우 많은 피처가 다중 공선성 문제를 가지고 있다면 PCA를 통해 차원 축소를 수행할 수도 있습니다."
      ],
      "metadata": {
        "id": "TZRb0crmWkJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2. 회귀 평가 지표**"
      ],
      "metadata": {
        "id": "UmnPfvB5esb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "회귀 평가 지표는 실젯값과 회귀 예측값의 차이 값을 기반으로 한 지표가 중심입니다. 실젯값과 예측값 차이를 더하면 양수와 음수가 섞여서 오류가 상쇄되므로(+2와 -2를 더하면 0) 정확한 지표가 될 수가 없기 때문에 아래 지표들을 활용합니다. 수식은 생략하겠습니다.\n",
        "\n",
        "- **MAE(Mean Absolute Error)**: 실젯값과 예측값의 차이를 절댓값으로 변환하여 평균합니다. `metrics` 모듈의 `mean_absolute_error()` 메서드이며, Scoring 함수(`cross_val_score` 메서드와 `GridSearchCV` 클래스)의 `scoring` 파라미터 인자는 `neg_mean_absolute_error`입니다. \n",
        "\n",
        "- **MSE(Mean Squared Error)**: 실젯값과 예측값의 차이를 제곱하여 평균합니다. `metrics` 모듈의 `mean_squared_error()` 메서드이며, `scoring` 파라미터 인자는 `neg_mean_squared_error`입니다. \n",
        "\n",
        "- **MSLE(Mean Squared Log Error)**: MSE에 로그를 적용합니다.\n",
        "\n",
        "- **RMSE(Root Mean Squared Error)**: MSE에 루트를 씌웁니다. MSE는 오류의 제곱을 구하므로 실제 오류 평균보다 더 커지는 특성이 있기 때문입니다. 사이킷런이 제공하지 않는 평가 지표이므로 MSE에 루트를 씌워서 계산하는 함수를 정의해야 합니다.\n",
        "\n",
        "- **RMSLE(Root Mean Squared Log Error)**: RMSL에 로그를 적용합니다.\n",
        "\n",
        "- **R$^2$**: 분산 기반으로 예측 성능을 평가합니다. 실젯값의 분산 대비 예측값의 분산 비율을 지표로 합니다. 1에 가까울수록 예측 정확도가 높습니다. `metrics` 모듈의 `r2_score()` 메서드이며, `scoring` 파라미터 인자는 `r2`입니다.\n",
        "\n",
        "참고로 `scoring` 파라미터 인자의 `neg_`는 Negative(음수)를 가진다는 것을 의미합니다. MAE는 절댓값이므로 음수일 수 없습니다. `neg_mean_absolute_error`를 적용하여 음수값을 변환하는 이유는 Scoring 함수가 `score` 값이 클수록 좋은 평가 결과로 자동적으로 평가하기 때문입니다. 특히 `GridSearchCV`는 가장 좋은 평가 값을 가지는 하이퍼 파라미터로 분류기를 학습까지 자동으로 시킬 수 있습니다. 그러나 실젯값과 예측값의 오류 차이를 기반으로 하는 회귀 평가 지표는 값이 커지면 오히려 나쁜 모델이라는 의미이므로 Scoring 함수에 반영하려면 보정 작업이 필요합니다. 따라서 -1을 원래 평가 지표 값에 곱해서 음수를 만들어 작은 오류 값이 더 큰 숫자로 인식하게 합니다. 즉, `neg_mean_absolute_error`는 -1 * `mean_absolute_error()`를 의미합니다.\n"
      ],
      "metadata": {
        "id": "0be5Bpn3Ypwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3. LinearRegression을 이용해 보스턴 주택 가격 회귀 구현**"
      ],
      "metadata": {
        "id": "0VBRrrelesZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "휴식 중\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VOG2Yl1NefdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. 다항 회귀와 과(대)적합/과소적합 이해**"
      ],
      "metadata": {
        "id": "UNeQDiQhesXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.1. 다항 회귀 이해**"
      ],
      "metadata": {
        "id": "lp48crYYesVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.2. 다항 회귀를 이용한 과소적합 및 과적합 이해**"
      ],
      "metadata": {
        "id": "xDSqLqINesS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.3. 편향-분산 트레이드오프(bias-variance trade off)**"
      ],
      "metadata": {
        "id": "XUrBk9zResQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. 규제 선형 모델 - 릿지, 라쏘, 엘라스틱넷**"
      ],
      "metadata": {
        "id": "uEF8_rlpesOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.1. 규제 선형 모델의 개요**"
      ],
      "metadata": {
        "id": "1aljAnKLesL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.2. 릿지 회귀**"
      ],
      "metadata": {
        "id": "p59_KnfCesJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.3. 라쏘 회귀**"
      ],
      "metadata": {
        "id": "Vu0qetCKesHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.4. 엘라스틱넷 회귀**"
      ],
      "metadata": {
        "id": "n0kywtH7esFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.5. 선형 회귀 모델을 위한 데이터 변환**"
      ],
      "metadata": {
        "id": "SQrLFVcVesDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. 로지스틱 회귀**"
      ],
      "metadata": {
        "id": "TLVjhMijesAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. 회귀 트리**"
      ],
      "metadata": {
        "id": "wSpFITeyer-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. 회귀 실습 - 자전거 대여 수요 예측**"
      ],
      "metadata": {
        "id": "tgRYmJ8ner8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1. 데이터 클렌징 및 가공**"
      ],
      "metadata": {
        "id": "Y4LULj6Igkpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.2. 로그 변환, 피처 인코딩과 모델 학습/예측/평가**"
      ],
      "metadata": {
        "id": "Sd7tonIXgknR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. 회귀 실습 - 캐글 주택 가격: 고급 회귀 기법**"
      ],
      "metadata": {
        "id": "vf9rluYNgkk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.1. 데이터 사전 처리(preprocessing)**"
      ],
      "metadata": {
        "id": "ONUIJa31gki0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.2. 선형 회귀 모델 학습/예측/평가**"
      ],
      "metadata": {
        "id": "--YRaGuygkgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.3. 회귀 트리 모델 학습/예측/평가**"
      ],
      "metadata": {
        "id": "QKWAOO4AgkeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.4. 회귀 모델의 예측 결과 혼합을 통한 최종 예측**"
      ],
      "metadata": {
        "id": "BH8v0O8Cgkbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.5. 스태킹 앙상블 모델을 통한 회귀 예측**"
      ],
      "metadata": {
        "id": "mRlAUFCGgkZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. 정리**"
      ],
      "metadata": {
        "id": "_p9I4jiggkCf"
      }
    }
  ]
}