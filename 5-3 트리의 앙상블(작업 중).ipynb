{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-3 트리의 앙상블(작업 중).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOyvVF0k174OC+Ah1QOkE7l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeSeungwon89/Lecture-and-self-study/blob/master/5-3%20%ED%8A%B8%EB%A6%AC%EC%9D%98%20%EC%95%99%EC%83%81%EB%B8%94(%EC%9E%91%EC%97%85%20%EC%A4%91).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52iVN67BFPWl"
      },
      "source": [
        "# 트리의 앙상블\n",
        "\n",
        "- 분류 클래스를 위주로 학습함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvnXEPK7EJMw"
      },
      "source": [
        "## 앙상블 학습\n",
        "\n",
        "- 앙상블 학습(Ensemble learning): 'Ensemble' 은 '전체적인 어울림이나 통일', '합주단' 등을 의미하며, 앙상블 학습은 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 결정 트리 기반 머신러닝 알고리즘을 의미함. 아래에서 설명하는 '정형 데이터' 를 다루는 데 가장 뛰어난 성과를 냄.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cSetDMTG7fd"
      },
      "source": [
        "## 정형 데이터와 비정형 데이터\n",
        "\n",
        "- 정형 데이터: 구조화 된 데이터를 의미하며 CSV, Database, Excel에 저장하기 쉬움.\n",
        "\n",
        "- 비정형 데이터: 비구조화 된 데이터로 텍스트 데이터, 사진, 디지털 음악 등을 의미함. 규칙성을 찾기 어려워 전통적인 머신러닝 방법으로 모델을 만들기 어려우므로 '신경망 알고리즘' 을 활용하여 모델을 만듦."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhy6_HGYUySn"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfHyka9QU0B4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "\n",
        "target = wine['class'].to_numpy()\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8VxCiG9G9wl"
      },
      "source": [
        "## 랜덤 포레스트\n",
        "\n",
        "- '랜덤 포레스트(Random Forest)' 는 앙상블 학습의 대표 주자로 안정적인 성능을 내며, 앙상블 학습을 적용할 때 가장 먼저 시도하길 권장하는 모델임. 결정 트리를 랜덤하게 만들어 결정 트리의 숲을 만들고, 각 결정 트리의 예측을 사용하여 최종 예측을 만듦. \n",
        "\n",
        "- 각 트리를 훈련하기 위한 데이터를 랜덤하게 만드는데, 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만듦. 이때 한 샘플이 중복되어 추출될 수 있음.\n",
        "\n",
        " - e.g. 가방 1,000개에서 샘플을 100개씩 뽑는다면 먼저 1개를 뽑고, 뽑았던 1개를 다시 가방에 넣음. 이렇게 계속 100개를 가방에서 뽑으면 중복된 샘플을 뽑을 수 있는데 이 샘플을 '부트스트랩 샘플(Bootstrap sample)' 이라고 부름. 기본적으로 훈련 세트의 크기와 같게 만듦. 가방 1,000개에서 중복하여 샘플 1,000개를 뽑기 때문에 부트스트랩 샘플은 훈련 세트와 크기가 같음.\n",
        "\n",
        " - 부트스트랩(Bootstrap): 보통 '부트스트랩 방식' 이라고 부르며, 위 예시처럼 데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식을 의미함.\n",
        "\n",
        "- 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾음.\n",
        "\n",
        " - RandomForestClassifier: 분류 클래스이며, 전체 특성 개수의 제곱근만큼 특성을 사용함. 즉 특성 4개가 있다면 노드마다 2개를 랜덤하게 선택하여 사용함. 결정 트리의 앙상블이므로 'DecisionTreeClassifier' 가 제공하는 중요한 매개변수인 'criterion', 'max_depth', 'max_feature', 'min_samples_split', 'min_impurity_decrease', 'min_samples_leaf' 등을 모두 제공함.\n",
        "\n",
        "   - n_estimators: 앙상블을 구성할 트리 개수를 지정하는 매개변수. 기본값은 100.\n",
        "\n",
        "   - criterion: 불순도를 지정하는 매개변수. 기본값은 지니 불순도를 의미하는 'gini' 이며, 엔트로피 불순도를 사용하려면 'entrophy' 로 지정함.\n",
        "\n",
        "   - max_depth: 트리가 성장할 최대 깊이를 지정하는 매개변수. 기본값은 None으로 리프 노드가 순수하거나 'min_samples_split' 매개변수에 지정한 값보다 샘플 개수가 적을 때까지 성장함.\n",
        "\n",
        "   - min_samples_split: 노드를 나누기 위한 최소 샘플 개수를 지정하는 매개변수. 기본값은 2.\n",
        "\n",
        "   - max_features: 최적의 분할을 위해 탐색할 특성 개수를 지정하는 매개변수. 기본값은 auto로 특성 개수의 제곱근을 의미함.\n",
        "\n",
        "   - bootstrap: 부트스트랩 샘플 사용 여부를 지정하는 매개변수. 기본값은 True.\n",
        "\n",
        "   - oob_score: OOB 샘플을 사용하여 훈련한 모델을 평가할지 여부를 지정하는 매개변수. 기본값은 False. 아래에서 추가로 설명함.\n",
        "\n",
        "   - n_jobs: 병렬 실행에 사용할 CPU 코어 수를 지정하는 매개변수. 기본값은 1로 하나의 코어를 사용하며, -1로 지정하면 시스템에 있는 모든 코어를 사용함.\n",
        "\n",
        " - RandomForestRegressor: 회귀 클래스이며, 전체 특성을 사용함.\n",
        "\n",
        "- 기본적으로 결정 트리 100개를 이 방식으로 훈련함.\n",
        "\n",
        " - 분류일 경우에는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼음. \n",
        " \n",
        " - 회귀일 경우에는 각 트리의 예측을 평균한 값을 예측으로 삼음.\n",
        "\n",
        "- 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 방지하고, 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있음. 종종 기본 매개변수 설정만으로도 좋은 결과를 냄.\n",
        "\n",
        "- 결정 트리의 큰 장점 중 하나인 특성 중요도를 계산함. 랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것임."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OomQQ2kEz4tj"
      },
      "source": [
        "### 교차 검증 수행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD7qgN-8iLiS",
        "outputId": "436e6a80-ed78-4f8c-9d6b-824f1a76bd88"
      },
      "source": [
        "# 교차 검증을 수행함.\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs = -1, random_state = 42)\n",
        "\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score = True, n_jobs = -1)\n",
        "print(scores)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "# 과대적합 상태임.\n",
        "# 알고리즘을 살펴보는 것이 목적이므로 매개변수를 더 조정하지는 않음.\n",
        "# 아울러 이 예제는 매우 간단하고 특성이 많지 않아서\n",
        "# 그리드 서치를 사용하더라도 하이퍼파라미터 튜닝 결과가 크게 나아지지 않음."
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'fit_time': array([0.58129835, 0.58413625, 0.57594681, 0.57646108, 0.36021495]), 'score_time': array([0.10230541, 0.10272074, 0.10223579, 0.10305619, 0.10219908]), 'test_score': array([0.88461538, 0.88942308, 0.90279115, 0.88931665, 0.88642926]), 'train_score': array([0.9971133 , 0.99663219, 0.9978355 , 0.9973545 , 0.9978355 ])}\n",
            "0.9973541965122431 0.8905151032797809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m854VuQfz9HN"
      },
      "source": [
        "### 모델 훈련 후 특성 중요도 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_V7LcfMxhoM",
        "outputId": "3ee8568f-859b-4c65-93e6-3425b42b8de5"
      },
      "source": [
        "# 모델 훈련 후 특성 중요도를 출력함. 특성 중요도는 'feature_importances_' 속성에 저장됨.\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.feature_importances_)\n",
        "# 앞의 1절에서 결정 트리 모델의 특성 중요도는 [0.12345626, 0.86862934, 0.0079144] 이고, 각각 alcohol, sugar, pH를 나타냄. \n",
        "# 랜덤 포레스트 점수와 비교하면 alcohol과 pH의 중요도가 상승하고 sugar의 중요도가 감소함. \n",
        "# 이런 이유는 랜덤 포레스트가 특성의 일부를 랜덤하게 선택하여 결정 트리를 훈련하기 때문임.\n",
        "# 즉 하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 부여하는 것임.\n",
        "# 이렇게 하여 과대적합을 줄이고 일반화 성능을 높이는 데 도움이 됨."
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifOucmQO1K-5"
      },
      "source": [
        "### OOB 샘플로 모델 평가하기\n",
        "\n",
        "- 'RandomForestClassifier' 의 기능 중에 자체적으로 모델을 평가하는 점수를 얻을 수 있는 기능이 있음. 부트스트랩 샘플에 포함되지 않고 남는 샘플을 'OOB(out of bag)' 이라고 하는데, 이 남는 샘플을 사용하여 부트스트랩 샘플로 훈련한 결정 트리를 평가함. '검증 세트' 역할을 한다고 볼 수 있으며, 교차 검증을 대신하므로 훈련 세트에 더 많은 샘플을 사용할 수 있음.\n",
        "\n",
        "- 이 OOB 점수를 평균하여 출력하기 위해 'RandomForestClassifier' 클래스의 'oob_score' 매개변수를 Ture로 지정해야 함. 기본값은 False. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Auw_yTnQtyG",
        "outputId": "9a3aa32e-9574-4627-851e-c86c0f338366"
      },
      "source": [
        "rf = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state = 42)\n",
        "\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.oob_score_)\n",
        "# 교차 검증에서 얻은 점수와 비슷한 결과를 얻음."
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8934000384837406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkXLggQ5HJI5"
      },
      "source": [
        "## 엑스트라 트리\n",
        "\n",
        "- '엑스트라 트리(Extra Trees)' 는 랜덤 포레스트와 매우 비슷하게 작동하는 모델임. 기본적으로 100개의 결정 트리를 훈련하며, 랜덤 포레스트와 동일하게 결정 트리가 제공하는 매개변수 대부분을 지원함. \n",
        "\n",
        "- 일부 특성을 랜덤하게 선택하여 노드를 분할하는 데 사용함.\n",
        "\n",
        "- 결정 트리를 만들 때 전체 훈련 세트를 사용하므로, 부트스트랩 샘플을 사용하지 않음.\n",
        "\n",
        "- 노드를 분할할 때 가장 좋은 분할을 찾지 않고 무작위로 분할함. 이렇게 무작위로 분할하면 모델 성능이 낮아지겠지만, 많은 트리를 앙상블 하기 때문에 과대적합을 막고 검증 세트 점수를 높이는 효과가 있음. 아래에서 추가로 설명함.\n",
        "\n",
        "- 'DecisionTreeClassifier' 클래스의 'splitter' 매개변수를 'random' 으로 지정하면 엑스트라 트리와 동일함.\n",
        "\n",
        "- ExtraTreesClassifier: 분류 클래스.\n",
        "\n",
        "   - n_estimators: 앙상블을 구성할 트리 개수를 지정하는 매개변수. 기본값은 100.\n",
        "\n",
        "   - criterion: 불순도를 지정하는 매개변수. 기본값은 지니 불순도를 의미하는 'gini' 이며, 엔트로피 불순도를 사용하려면 'entrophy' 로 지정함.\n",
        "\n",
        "   - max_depth: 트리가 성장할 최대 깊이를 지정하는 매개변수. 기본값은 None으로 리프 노드가 순수하거나 'min_samples_split' 매개변수에 지정한 값보다 샘플 개수가 적을 때까지 성장함.\n",
        "\n",
        "   - min_samples_split: 노드를 나누기 위한 최소 샘플 개수를 지정하는 매개변수. 기본값은 2.\n",
        "\n",
        "   - max_features: 최적의 분할을 위해 탐색할 특성 개수를 지정하는 매개변수. 기본값은 auto로 특성 개수의 제곱근을 의미함.\n",
        "\n",
        "   - bootstrap: 부트스트랩 샘플 사용 여부를 지정하는 매개변수. 기본값은 False로 랜덤 포레스트와 다름.\n",
        "\n",
        "   - oob_score: OOB 샘플을 사용하여 훈련한 모델을 평가할지 여부를 지정하는 매개변수. 기본값은 False.\n",
        "\n",
        "   - n_jobs: 병렬 실행에 사용할 CPU 코어 수를 지정하는 매개변수. 기본값은 1로 하나의 코어를 사용하며, -1로 지정하면 시스템에 있는 모든 코어를 사용함.\n",
        "\n",
        "- ExtraTreesRegressor: 회귀 클래스."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR8suVNpXSdD"
      },
      "source": [
        "### 교차 검증 수행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNIbEPxtWtEc",
        "outputId": "2fc0ea87-13b1-4f39-bb35-1d282a9a3cc9"
      },
      "source": [
        "# 교차 검증을 수행함.\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "et = ExtraTreesClassifier(n_jobs = -1, random_state = 42)\n",
        "\n",
        "scores = cross_validate(et, train_input, train_target, return_train_score = True, n_jobs = -1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "# 랜덤 포레스트와 비슷한 결과를 출력함. 이 예제는 특성이 많지 않아서 두 모델의 차이가 크지 않음.\n",
        "# 보통 엑스트라 트리가 무작위성이 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리를 훈련해야 함.\n",
        "# 더 많은 결정 트리를 훈련하지만 노드를 랜덤하게 분할하므로 계산 속도가 빠름."
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr1JCMThYkK0"
      },
      "source": [
        "### 모델 훈련 후 특성 중요도 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWo1LCzIYmnz",
        "outputId": "d232c5f0-7493-4877-90e8-cd503dd9e2c3"
      },
      "source": [
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)\n",
        "# 앞의 1절에서 결정 트리 모델의 특성 중요도는 [0.12345626, 0.86862934, 0.0079144] 이고, 각각 alcohol, sugar, pH를 나타냄.\n",
        "# 엑스트라 트리의 점수와 비교하면 alcohol과 pH의 중요도가 상승하고 sugar의 중요도가 감소함. "
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMrl_AsHKss"
      },
      "source": [
        "## 그레이디언트 부스팅\n",
        "\n",
        "- '그레이디언트 부스팅(Gradient boosting)' 은 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이며, 그레이디언트(Gradient)의 의미처럼 '경사 하강법' 을 사용하여 트리를 앙상블에 추가함. 이전에 설명했듯이 '경사 하강법' 은 손실 함수를 산으로 정의하고 가장 낮은 곳을 찾아 내려오는 과정임.\n",
        "\n",
        "- 모델의 가중치와 절편을 조금씩 바꾸면서 가장 낮은 곳으로 내려옴. 손실 함수의 낮은 곳으로 천천히 조금씩 이동하기 위해 깊이가 얕은 결정 트리를 계속 추가하면서 이동하는 방법을 취함.\n",
        "\n",
        "- 분류에서는 '로지스틱 손실 함수' 를 사용하고, 회귀에서는 '평균 제곱 오차 함수' 를 사용함.\n",
        "\n",
        "- GradientBoostingClassifier: 분류 클래스이며, 기본적으로 깊이가 3인 결정 트리를 100개 사용하는  깊이가 얕은 결정 트리를 사용하므로 과대적합에 강하고 높은 일반화 성능을 낼 수 있음.\n",
        "\n",
        " - loss: 손실 함수를 지정하는 매개변수. 기본값은 'deviance' 로 로지스틱 손실 함수를 의미함.\n",
        "\n",
        " - n_estimators: 부스팅 단계를 수행하는 트리 개수를 지정하는 매개변수. 기본값은 100.\n",
        "\n",
        " - learning_rate: 트리가 앙상블에 기여하는 정도인 학습률을 지정하는 매개변수. 속도를 조절할 수 있으며 기본값은 0.1.\n",
        "\n",
        " - subsample: 트리 훈련에 사용하는 훈련 세트의 비율을 정하는 매개변수. 기본값은 1.0으로 전체 훈련 세트를 사용함. 1.0보다 작으면 훈련 세트 일부를 사용하며, 경사 하강법 단계마다 일부 샘플을 랜덤하게 선택하여 진행하는 '확률적 경사 하강법'이나 '미니배치 경사 하강법' 과 비슷함.\n",
        "\n",
        " - max_depth: 개별 회귀 트리의 최대 깊이를 지정하는 매개변수. 기본값은 3.\n",
        "\n",
        "- GradientBoostingRegressor: 회귀 클래스.\n",
        "\n",
        "- 일반적으로 그레이디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능을 내지만, 순서대로 트리를 추가하기 때문에 훈련 속도가 느림. 즉 'GradientBoostingClassifier' 클래스에는 'n_jobs' 매개변수가 없음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqEVC36895d2"
      },
      "source": [
        "### 교차 검증 수행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDwMo4E1bPgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586e1e68-bf99-4382-99c8-60f098588b3e"
      },
      "source": [
        "# 'GradientBoostingClassifier' 클래스를 사용하여 교차 검증 점수를 확인함.\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state = 42)\n",
        "\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score = True, n_jobs = -1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "# 과대적합이 거의 되지 않음.\n",
        "# 그레이디언트 부스팅은 결정 트리 개수를 늘려도 과대적합에 매우 강함.\n",
        "# 학습률을 증가시키고 트리 개수를 늘리면 성능이 더 향상될 수 있음."
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL2YcS5U98bS"
      },
      "source": [
        "### 매개변수를 조정하여 점수 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dIIPUtX9zBX",
        "outputId": "811650a3-5e5a-4cb9-f984-23759c7b63f1"
      },
      "source": [
        "# 'n_estimators' 매개변수 값을 변경하여 결정 트리 개수를 500개로 늘리고\n",
        "# 'learning_rate' 매개변수 값을 변경하여 학습률을 0.2로 지정함.\n",
        "gb = GradientBoostingClassifier(n_estimators = 500, learning_rate = 0.2, random_state = 42)\n",
        "\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score = True, n_jobs = -1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4odBtT5_-jS"
      },
      "source": [
        "### 모델 훈련 후 특성 중요도 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHJ7YdqfAGIZ",
        "outputId": "bd92b7fc-b783-4d9e-9a9b-8d271a3db1eb"
      },
      "source": [
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)\n",
        "# 그레이디언트 부스팅은 sugar에 0.68010884만큼 집중하며,\n",
        "# 랜덤 포레스트의 sugar에 대한 집중도(0.50039841)보다 더 비중을 두고 집중함."
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.15872278 0.68010884 0.16116839]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MLRAKvKHNU5"
      },
      "source": [
        "## 히스토그램 기반 그레이디언트 부스팅\n",
        "\n",
        "- '히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)' 은 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘임.\n",
        "\n",
        "- 입력 특성을 256개 구간으로 나눠서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾음. 256개 구간 중에 하나를 떼어 놓고 누락된 값을 위해 사용하며, 입력에 누락된 특성이 있더라도 따로 전처리할 필요가 없음.\n",
        "\n",
        "- HistGradientBoostingClassifier: 분류 클래스이며, 일반적으로 기본 매개변수로 안정적인 성능을 얻을 수 있음.\n",
        "\n",
        " - learning_rate: 학습률(감쇠율)을 지정하는 매개변수. 기본값은 0.1이며, 1.0이면 감쇠가 전혀 없음. 여기서 '감쇠' 는 '힘이 줄어서 약하여짐' 을 의미함.\n",
        "\n",
        " - max_iter: 부스팅 단계를 수행하는 트리 개수(부스팅 반복 횟수)를 지정하는 매개변수. 기본값은 100. 모델 성능을 높이려면 이 매개변수를 테스트 함. 트리 개수를 지정하는 'n_estimators' 매개변수 대신 사용함. \n",
        "\n",
        " - max_bins: 입력 데이터를 나눌 구간 개수를 지정하는 매개변수. 기본값은 255이며 더 크게 지정할 수 없음. 위에서 설명한 256개 구간처럼, 누락된 값을 위해 구간 1개가 추가됨.\n",
        "\n",
        "- HistGradientBoostingRegressor: 회귀 클래스."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3IacLQzLX50"
      },
      "source": [
        "### 교차 검증 수행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnJA_djXFJit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72862673-5619-44fa-9746-106c4f4c9baa"
      },
      "source": [
        "# 히스토그램 기반 그레이디언트 부스팅은 아직 테스트 과정에 속해 있으며,\n",
        "# 사용하려면 'sklearn.experimental' 패키지에 있는 'enable_hist_gradient_boosting' 모듈을 임포트 해야 함.\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "hgb = HistGradientBoostingClassifier(random_state = 42)\n",
        "\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score = True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "# 과대적합을 제어하면서 그레이디언트 부스팅보다 좀 더 높은 성능을 냄."
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxhqg7imMulY"
      },
      "source": [
        "### 모델 훈련 후 특성 중요도 출력하기\n",
        "\n",
        "- 특성 중요도를 계산하기 위해 'permutation_importance()' 함수를 사용함. 'permutation' 은 '순열, 치환' 을 의미함.\n",
        "\n",
        "- 특성을 하나씩 랜덤하게 섞어서 모델 성능이 변화하는지 관찰하여 어떤 특성이 중요한지 계산함.\n",
        "\n",
        " - n_repeats: 랜덤하게 섞을 횟수를 지정하는 매개변수.\n",
        "\n",
        "- 객체 3가지를 가짐.\n",
        "\n",
        " - improtances: 반복하여 얻은 특성 중요도.\n",
        "\n",
        " - importances_mean: 반복하여 얻은 특성 중요도의 평균.\n",
        "\n",
        " - importances_std: 반복하여 얻은 특성 중요도의 표준편차.\n",
        "\n",
        "- 훈련 세트뿐만 아니라 테스트 세트에서 적용할 수 있고, 사이킷런에서 제공하는 '추정기' 모델에도 적용 가능함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyZSaZ9mMt0l",
        "outputId": "cf6ca45c-3b6e-4a41-cd5b-fa1f5d94a19b"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "hgb.fit(train_input, train_target)\n",
        "\n",
        "result = permutation_importance(hgb, train_input, train_target,\n",
        "                                n_repeats = 10, random_state = 42, n_jobs = -1)\n",
        "\n",
        "print(result.importances)\n",
        "print(result.importances_mean)\n",
        "print(result.importances_std)\n",
        "# 이중에 평균은 랜덤 포레스트와 비슷한 비율을 나타냄."
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.08793535 0.08350972 0.08908986 0.08312488 0.09274581 0.08755051\n",
            "  0.08601116 0.09601693 0.09082163 0.09082163]\n",
            " [0.22782374 0.23590533 0.23936887 0.23436598 0.23725226 0.23436598\n",
            "  0.23359631 0.23398114 0.23994612 0.22724649]\n",
            " [0.08581874 0.08601116 0.08062344 0.07504329 0.08427939 0.07792957\n",
            "  0.07234943 0.07465846 0.08139311 0.08466423]]\n",
            "[0.08876275 0.23438522 0.08027708]\n",
            "[0.00382333 0.00401363 0.00477012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVVojPdxPysB",
        "outputId": "7c6e6164-998f-4edc-b1b8-7b86ac7fb9e6"
      },
      "source": [
        "# 테스트 세트로 특성 중요도를 계산함.\n",
        "result = permutation_importance(hgb, test_input, test_target,\n",
        "                                n_repeats = 10, random_state = 42, n_jobs = -1)\n",
        "\n",
        "print(result.importances)\n",
        "print(result.importances_mean)\n",
        "print(result.importances_std)\n",
        "# 그레이디언트 부스팅과 비슷하게 좀 더 sugar에 집중함.\n",
        "# 이런 분석 과정을 통해 실전에 투입했을 때 어떤 특성에 관심을 가질지 예상할 수 있음."
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.06230769 0.05769231 0.05538462 0.05538462 0.06076923 0.06076923\n",
            "  0.06846154 0.06230769 0.05461538 0.05923077]\n",
            " [0.20076923 0.2        0.21153846 0.20076923 0.20307692 0.18923077\n",
            "  0.19615385 0.19461538 0.21384615 0.21384615]\n",
            " [0.05692308 0.04692308 0.05076923 0.04769231 0.04692308 0.05\n",
            "  0.04384615 0.04692308 0.04307692 0.05692308]]\n",
            "[0.05969231 0.20238462 0.049     ]\n",
            "[0.004      0.007938   0.00453846]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIh5kVDhSyEC"
      },
      "source": [
        "### 테스트 세트로 성능 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPKvDthmSxZj",
        "outputId": "c6dbf3ae-01cd-43cd-a481-692713e9e1db"
      },
      "source": [
        "hgb.score(test_input, test_target)\n",
        "# 실전에 투입하면 이보다 더 낮은 성능을 가질 가능성이 높음.\n",
        "# 결론적으로 앙상블 모델은 단일 결정 트리 모델보다 좋은 결과를 얻음.\n",
        "# 2절 랜덤 서치에서 테스트 정확도는 86%였음."
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaerY3SEUa7b"
      },
      "source": [
        "### XGBoost\n",
        "\n",
        "- 히스토그램 기반 그레이디언트 부스팅을 구현한 대표적인 라이브러리.\n",
        "\n",
        "- 'tree_method' 매개변수를 'hist' 로 지정하여 히스토그램 기반 그레이디언트 부스팅을 사용함.\n",
        "\n",
        "- 'cross_validate()' 함수와 함께 사용할 수 있음.\n",
        "\n",
        "- 더 자세한 설명은 [[1]](https://xgboost.ai/), [[2]](https://xgboost.readthedocs.io/en/latest/) 를 참고."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igkpxjStW6ZI"
      },
      "source": [
        "#### 교차 검증 수행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkr2q4YEXG0X",
        "outputId": "fdb24337-b270-44d3-9eca-90171a8ec3fa"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(tree_method = 'hist', random_state = 42)\n",
        "\n",
        "scores = cross_validate(xgb, train_input, train_target, return_train_score = True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8824322471423747 0.8726214185237284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjJibiOPWyr_"
      },
      "source": [
        "#### 모델 훈련 후 특성 중요도 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM3pJXItUZg2",
        "outputId": "0d30b2f5-48ff-49ce-a810-9fb5786c9484"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "xgb.fit(train_input, train_target)\n",
        "\n",
        "result = permutation_importance(xgb, train_input, train_target,\n",
        "                                n_repeats = 10, random_state = 42, n_jobs = -1)\n",
        "\n",
        "print(result.importances)\n",
        "print(result.importances_mean)\n",
        "print(result.importances_std)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.03983067 0.03521262 0.04329421 0.039061   0.0421397  0.03925342\n",
            "  0.03713681 0.04348663 0.04252453 0.04310179]\n",
            " [0.19607466 0.20069271 0.19973061 0.20030787 0.20319415 0.20319415\n",
            "  0.19665191 0.19395805 0.20261689 0.19415047]\n",
            " [0.03675197 0.03636713 0.0359823  0.03232634 0.03790648 0.03617472\n",
            "  0.03386569 0.02944006 0.03867616 0.03790648]]\n",
            "[0.04050414 0.19905715 0.03553973]\n",
            "[0.00271338 0.00341103 0.00272264]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPDLAl1BXLe3"
      },
      "source": [
        "#### 테스트 세트로 성능 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMwyMFI2XN65",
        "outputId": "5598fc82-8b2b-4a32-844f-0a9b81278f7f"
      },
      "source": [
        "xgb.score(test_input, test_target)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8669230769230769"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb-NXRxtVJQa"
      },
      "source": [
        "### LightBGM\n",
        "\n",
        "- 히스토그램 기반 그레이디언트 부스팅을 구현한 대표적인 라이브러리.\n",
        "\n",
        "- 'tree_method' 매개변수를 'hist' 로 지정하여 히스토그램 기반 그레이디언트 부스팅을 사용함.\n",
        "\n",
        "- 'cross_validate()' 함수와 함께 사용할 수 있음.\n",
        "\n",
        "- 더 자세한 설명은 [[1]](https://github.com/microsoft/LightGBM), [[2]](https://lightgbm.readthedocs.io/en/latest) 를 참고."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJUsf9kpaBqy"
      },
      "source": [
        "#### 교차 검증 수행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd5tIaAJaK2K",
        "outputId": "be694b89-6572-40a5-d2eb-3ea7df28c05f"
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgb = LGBMClassifier(tree_method = 'hist', random_state = 42)\n",
        "\n",
        "scores = cross_validate(lgb, train_input, train_target, return_train_score = True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9338079582727165 0.8789710890649293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaS6CzDqaDa2"
      },
      "source": [
        "#### 모델 훈련 후 특성 중요도 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFD4q7YbaNg1",
        "outputId": "03dde126-b061-498d-90c2-956aa7de1595"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "lgb.fit(train_input, train_target)\n",
        "\n",
        "result = permutation_importance(lgb, train_input, train_target,\n",
        "                                n_repeats = 10, random_state = 42, n_jobs = -1)\n",
        "\n",
        "print(result.importances)\n",
        "print(result.importances_mean)\n",
        "print(result.importances_std)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.08908986 0.08389455 0.09024437 0.08524149 0.09582451 0.09101405\n",
            "  0.09005195 0.09390033 0.09274581 0.09332307]\n",
            " [0.23051761 0.23513566 0.24110063 0.23224937 0.2376371  0.23609775\n",
            "  0.23571291 0.2345584  0.24013854 0.22705407]\n",
            " [0.08716567 0.08697325 0.08774293 0.0802386  0.0894747  0.07869925\n",
            "  0.07562055 0.07869925 0.08735809 0.08601116]]\n",
            "[0.090533   0.2350202  0.08379835]\n",
            "[0.00355975 0.00404098 0.00467245]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXw92XqhaFgF"
      },
      "source": [
        "#### 테스트 세트로 성능 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHrpMAFtVi39",
        "outputId": "13fec9ab-6723-48a5-b1f4-65945560e72f"
      },
      "source": [
        "lgb.score(test_input, test_target)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8638461538461538"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    }
  ]
}