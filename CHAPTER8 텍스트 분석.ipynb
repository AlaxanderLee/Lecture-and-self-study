{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CHAPTER8 텍스트 분석.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOH8kF9DdjsiTHx9ehjtaqW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeSeungwon89/Machine-learning_Theory/blob/master/CHAPTER8%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP이냐 텍스트 분석이냐**"
      ],
      "metadata": {
        "id": "TxMLRcI2lI8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "기실 NLP와 텍스트 분석을 구분하는 것은 큰 의미가 없지만 굳이 나눈다면 아래와 같습니다.\n",
        "\n",
        "**NLP(National Language Processing, 자연어 처리)**는 머신이 인간의 언어를 이해하고 해석하는 데 더 중점을 두고 발전하고 있습니다. NLP의 영역은 언어를 해석하기 위한 기계 번역, 자동으로 질문을 해석하고 답을 해주는 질의응답 시스템 등입니다.\n",
        "\n",
        "**TA(Text Analytics, Text Mining, 텍스트 분석)**은 비정형 텍스트에서 의미 있는 정보를 추출하는 데 중점을 두고 발전하고 있습니다. NLP는 TA의 기반 기술이며, NLP가 발전하면서 TA도 더 정교하게 발전했습니다. \n",
        "\n",
        "머신러닝 기술은 NLP와 TA의 발전에 크게 기여했습니다. 예전의 텍스트를 구성하는 언어적인 룰이나 업무의 룰에 따라 텍스트를 분석하는 룰 기반 시스템에서 머신러닝의 텍스트 데이터를 기반으로 모델을 학습하고 예측하는 기반으로 변경되면서 기술이 비약적으로 발전한 것입니다.\n",
        "\n",
        "TA는 머신러닝, 언어 이해, 통계 등을 확용하여 모델을 수립하고 정보를 추출하여 비즈니스 인텔리전스나 예측 분석 등 분석 작업을 주로 수행합니다. 주로 아래와 같은 기술 영역에 집중합니다.\n",
        "\n",
        "- **텍스트 분류(Text Classification, Text Categorization)**: 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법을 통칭합니다. 지도학습을 적용하며, 신문 기사가 특정 카테고리에 속하는지 자동으로 분류하거나 스팸 메일을 검출하는 기능을 예로 들 수 있습니다. \n",
        "\n",
        "- **감성 분석(Sentiment Analysis)**: 텍스트에서 나타나는 주관적인 요소를 분석하는 기법을 총칭합니다. 지도학습과 비지도학습을 모두 적용할 수 있으며, 소셜 미디어 감정 분석, 영화나 제품에 대한 리뷰, 여론조사 의견 분석 등을 예로 들 수 있습니다. \n",
        "\n",
        "- **텍스트 요약(Summarization)**: 텍스트 내에서 중요한 주제나 중심 사상을 추출하는 기법을 의미합니다. 토픽 모델링(Topic Modeling)을 예로 들 수 있습니다.\n",
        "\n",
        "- **텍스트 군집화**와 **유사도 측정**: 비슷한 유형의 문서를 군집화하는 기법을 의미합니다. 텍스트 분류를 비지도학습으로 수행하는 방법의 일환으로 사용될 수 있습니다. 유사도 측정 역시 문서들 간 유사도를 측정하여 비슷한 문서끼리 모을 수 있는 기법입니다."
      ],
      "metadata": {
        "id": "EdvCmmfmQe6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 텍스트 분석 이해**"
      ],
      "metadata": {
        "id": "7SFqUTDUlI5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지 ML 모델은 정형 데이터 기반에서 모델을 수립하고 예측을 수행했습니다. 그리고 머신러닝 알고리즘은 숫자형 피처 기반 데이터만 입력받을 수 있으므로 텍스트를 머신러닝에 적용하려면 비정형 텍스트 데이터를 피처 형태로 추출하고 추출된 피처에 의미 있는 값을 부여하는 것이 중요합니다. 텍스트를 단어 기반(또는 단어 일부분)의 다수 피처로 추출하고 이 피처에 단어 빈도수 같은 숫자 값을 부여하면 텍스트는 단어 조합인 벡터값으로 표현될 수 있습니다. 이렇게 텍스트를 변환하는 것을 피처 벡터화(Feature Vectorization) 또는 피처 추출(Feature Extraction)이라고 합니다. 텍스트를 피처 벡터화 하여 변환하는 방법은 **BOW(Bag of Words)**와 **Word2Vec**가 대표적입니다.\n"
      ],
      "metadata": {
        "id": "gttzXbdz1W-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1. 텍스트 분석 수행 프로세스**"
      ],
      "metadata": {
        "id": "Zra4U2lflI3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TA는 아래와 같은 프로세스 순으로 수행합니다.\n",
        "\n",
        "1. 텍스트 사전 준비(텍스트 전처리): 클렌징, 대/소문자 변경, 특수문자 삭제, 단어 등의 토큰화, 무의미한 단어(Stop word) 제거, 어근 추출(Stemming/Lemmatization) 등 텍스트 정규화 작업을 수행합니다.\n",
        "\n",
        "1. 피처 벡터화 및 추출: 전처리로 가공된 텍스트에서 피처를 추출하고 벡터 값을 할당합니다. BOW와 Word2Vec가 대표적인 방법입니다. BOW는 Count 기반과 TF-IDF 기반 벡터화를 수행합니다. \n",
        "\n",
        "1. ML 모델 수립 및 학습, 예측, 평가: 피처 벡터화가 수행된 데이터 세트에 ML 모델을 적용하여 학습, 예측, 평가를 수행합니다.\n"
      ],
      "metadata": {
        "id": "C2MQRdd4YomJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. 파이썬 기반의 NLP, 텍스트 분석 패키지**"
      ],
      "metadata": {
        "id": "Ss_Da4BXlI1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "대표적으로 사용되는 패키지는 아래와 같습니다.\n",
        "\n",
        "- **NLTK(Natural Language Toolkit for Python)**: 파이썬의 가장 대표적인 NLP 패키지입니다. 방대한 데이터 세트와 서브 모듈을 가졌으며 NLP의 대부분 영역을 커버합니다. 수많은 NLP 패키지가 NLTK의 영향을 받아 작성되고 있습니다. 느린 수행 속도로 인해 대량 데이터 기반에서는 활용되지 않습니다. 아울러 수행 성능, 정확도, 신기술, 엔터프라이즈 기능 지원 등 전반적인 측면이 부족하기 때문에 실무에서 잘 활용되지 않습니다.\n",
        "\n",
        "- **Genism**: 토픽 모델링 분야에서 가장 두각을 보이며 많이 사용되는 패키지입니다. 토픽 모델링을 쉽게 구현하는 기능을 지원하며, Word2Vec 구현 등 다양한 신기능도 지원합니다.\n",
        "\n",
        "- **SpaCy**: 수행 성능이 매우 뛰어나 최근들어 주목 받는 패키지입니다. NLP 애플리케이션에서 사용하는 사례가 느는 추세입니다.\n",
        "\n",
        "참고로 사이킷런은 머신러닝 위주의 라이브러리입니다. 따라서 NLP를 위한 라이브러리, 예컨대 '어근 처리'를 수행하는 것처럼 NLP 패키지에 특화된 라이브러리는 없습니다. 다만 텍스트를 일정 수준으로 가공하고 머신러닝 알고리즘에 텍스트 데이터를 피처로 처리하기 위한 편리한 기능을 제공하므로 충분히 TA를 수행할 수 있습니다. 더 다양한 TA가 적용되어야 할 경우에는 위에서 설명한 패키지를 결합하여 애플리케이션을 작성합니다."
      ],
      "metadata": {
        "id": "_zTCuhLuaNko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화**"
      ],
      "metadata": {
        "id": "9OznRAtklIzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트를 바로 피처화할 수 없으므로 데이터 전처리 과정을 거쳐야 합니다. 텍스트 정규화는 텍스트를 머신러닝 알고리즘이나 NLP 애플리케이션에 입력 데이터로 사용할 목적으로 클렌징, 정제, 토큰화, 어근화 등 사전 작업을 수행하는 작업입니다. \n"
      ],
      "metadata": {
        "id": "UKxWRQzpkqoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1. 클렌징(Cleansing)**"
      ],
      "metadata": {
        "id": "e6bTR_EVlpht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TA를 수행하면서 불필요하거나 방해 요소로 작용할 만한 문자와 기호(예컨대 HTML, XML 태그나 특정 기호) 등을 사전에 제거하는 작업입니다. "
      ],
      "metadata": {
        "id": "JpwKLvkdlmIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2. 텍스트 토큰화(Tokenization)**"
      ],
      "metadata": {
        "id": "60OE02wklIxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 토큰화 유형은 문서에서 문장을 분리하는 문장 토큰화, 문장에서 단어를 토큰으로 분리하는 단어 토큰화로 나뉩니다. "
      ],
      "metadata": {
        "id": "HDui4Tosl8jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.1. 문장 토큰화** "
      ],
      "metadata": {
        "id": "ycsjIa34lIus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 토큰화는 문장의 마침표나 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적입니다. 아울러 정규 표현식에 따른 문장 토큰화도 가능합니다. NTLK에서 일반적으로 많이 쓰이는 `sent_tokenize` 클래스로 토큰화해 보겠습니다. 샘플 텍스트 문서를 문장 3개로 분리하는 작업입니다."
      ],
      "metadata": {
        "id": "PsoGPM1gmJNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "# 마침표, 개행 문자 등의 데이터 세트를 다운로드합니다.\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "You can see it out your window or on your television. \\\n",
        "You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences), len(sentences))\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcSnOsoJKELE",
        "outputId": "e36ec97a-dd69-4340-f868-e1aecfa748fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "리스트 객체에 문장 3개가 담겨 있습니다."
      ],
      "metadata": {
        "id": "YfZeTfK-LnKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.2. 단어 토큰화**"
      ],
      "metadata": {
        "id": "YBzBF5QTlIsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 토큰화는 문장을 단어로 토큰화하는 작업입니다. 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리하며, 정규 표현식을 이용하여 다양한 유형으로도 토큰화할 수 있습니다. 마침표나 개행문자처럼 문장을 분리하는 구분자를 이용하여 단어를 토큰화할 수 있으므로 Bag of Word처럼 단어 순서가 중요하지 않으면 문장 토큰화를 사용하지 않고 단어 토큰화만 사용해도 충분합니다. 일반적으로 문장 토큰화는 각 문장이 가지는 의미가 중요한 요소로 사용될 때 사용합니다. NLTK의 `word_tokenize()` 클래스를 활용해 보겠습니다."
      ],
      "metadata": {
        "id": "5xy1fqyeL4lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0bo_zSBMoER",
        "outputId": "8cf0fadf-c72a-4395-8d9f-7f8d8182a53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 토큰화와 단어 토큰화를 동시에 수행해 보겠습니다. 먼저 여러 문장을 문장별로 단어 토큰화 하는 함수를 선언하겠습니다."
      ],
      "metadata": {
        "id": "weP3lTu-NQcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return word_tokens"
      ],
      "metadata": {
        "id": "RjkrtdV8NYm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수를 사용하여 두 작업을 한 번에 수행해 보겠습니다."
      ],
      "metadata": {
        "id": "2JIKFdhiOxKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "You can see it out your window or on your television. \\\n",
        "You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens), len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBmgvq5YOUOl",
        "outputId": "053ef058-ec74-44cf-d931-11f32f4d1abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장을 단어별로 토큰화하면 문맥 의미를 잃게 됩니다. 이 문제를 다소 해결하는 방안으로 [n-gram](https://wikidocs.net/21692)을 사용합니다. n-gram은 연속된 n개의 단어를 하나의 토큰화 단위로 분리합니다. n개 단어 크기 윈도우를 만들어 문장 처음부터 오른쪽으로 움직이면서 토큰화합니다. 예컨대 'Agent Smith knocks the door'를 2-gram(bigram)으로 만들면 (Agent, Smith), (knocks, the), (the, door)와 같이 연속적으로 단어 2개들을 차례로 이동하면서 토큰화합니다."
      ],
      "metadata": {
        "id": "YCiOl9pQOu_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3. 스톱 워드(Stop Word) 제거**"
      ],
      "metadata": {
        "id": "sKnfqCTulIqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "스톱 워드는 문맥적으로 무의미하여 분석에 불필요한 단어를 말합니다. 'is', 'the', 'a', 'will' 등을 예로 들 수 있습니다. 이런 단어들을 제거해야 데이터 분석을 수행할 때 중요 단어로 인지되지 않습니다.\n",
        "\n",
        "스톱 워드는 언어별로 목록화되어 있습니다. 아래에서 NLTK의 스톱 워드 종류를 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "-an7PdvhP_Cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print('영어의 스톱 워드 개수: {}개'.format(len(nltk.corpus.stopwords.words('english'))))\n",
        "print(nltk.corpus.stopwords.words('english')[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXqg1XLwRGpB",
        "outputId": "526fd8f9-9139-42ba-8459-0949346501d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "영어의 스톱 워드 개수: 179개\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "유의미한 단어만 추출해 보겠습니다."
      ],
      "metadata": {
        "id": "-mQR289iUa14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "You can see it out your window or on your television. \\\n",
        "You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in word_tokens:\n",
        "    filtered_words = []\n",
        "    for word in sentence:\n",
        "        # 단어를 소문자로 변환합니다.\n",
        "        word = word.lower()\n",
        "        # 단어가 스톱 워드 리스트에 없으면 `filtered_words` 리스트에 추가합니다.\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    # 필터링한 단어를 모은 `filtered_words` 리스트를 `all_tokens` 리스트에 추가합니다.\n",
        "    all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0zjUboNSq6t",
        "outputId": "60f6ef14-4604-4449-d8c6-3ecb5655abeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4. Stemming과 Lemmatization**"
      ],
      "metadata": {
        "id": "FwfF0wNIlIoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**과 **Lemmatization**은 문법적 또는 의미적으로 변화하는 단어의 원형을 찾습니니다. Stemming은 원형 단어로 변환할 때 일반적이고 더 단순한 방법을 적용하여 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출합니다. 반면 Lemmatization은 문법적인 요소와 더 의미적인 부분을 감안하여 정확한 철자로 된 어근 단어를 찾습니다. 따라서 Lemmatization이 더 정교하며 의미론적인 기반에서 단어의 원형을 찾습니다. 다만 수행 시간이 오래 걸립니다.\n",
        "\n",
        "NLTK가 가진 Stemming을 위한 대표적 Stemmer는 `stem` 패키지의 `porter`, `lancaster`, `snowball` 모듈입니다."
      ],
      "metadata": {
        "id": "WYFkRuySVWf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.stem\n",
        "\n",
        "help(nltk.stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BheEnlJrHjwR",
        "outputId": "46094631-ce5a-4acd-efc8-b8856d4f5e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on package nltk.stem in nltk:\n",
            "\n",
            "NAME\n",
            "    nltk.stem - NLTK Stemmers\n",
            "\n",
            "DESCRIPTION\n",
            "    Interfaces used to remove morphological affixes from words, leaving\n",
            "    only the word stem.  Stemming algorithms aim to remove those affixes\n",
            "    required for eg. grammatical role, tense, derivational morphology\n",
            "    leaving only the stem of the word.  This is a difficult problem due to\n",
            "    irregular words (eg. common verbs in English), complicated\n",
            "    morphological rules, and part-of-speech and sense ambiguities\n",
            "    (eg. ``ceil-`` is not the stem of ``ceiling``).\n",
            "    \n",
            "    StemmerI defines a standard interface for stemmers.\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    api\n",
            "    arlstem\n",
            "    isri\n",
            "    lancaster\n",
            "    porter\n",
            "    regexp\n",
            "    rslp\n",
            "    snowball\n",
            "    util\n",
            "    wordnet\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.7/dist-packages/nltk/stem/__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래에서 사용할 `LancasterStemmer` 클래스는 `lancaster` 모듈에 포함됩니다."
      ],
      "metadata": {
        "id": "mBa1LktfI8PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(nltk.stem.lancaster)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpuHDaalHbri",
        "outputId": "2d8657a6-4ad2-48fb-e6c9-629ea1620b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on module nltk.stem.lancaster in nltk.stem:\n",
            "\n",
            "NAME\n",
            "    nltk.stem.lancaster\n",
            "\n",
            "DESCRIPTION\n",
            "    A word stemmer based on the Lancaster (Paice/Husk) stemming algorithm.\n",
            "    Paice, Chris D. \"Another Stemmer.\" ACM SIGIR Forum 24.3 (1990): 56-61.\n",
            "\n",
            "CLASSES\n",
            "    nltk.stem.api.StemmerI(builtins.object)\n",
            "        LancasterStemmer\n",
            "    \n",
            "    class LancasterStemmer(nltk.stem.api.StemmerI)\n",
            "     |  LancasterStemmer(rule_tuple=None, strip_prefix_flag=False)\n",
            "     |  \n",
            "     |  Lancaster Stemmer\n",
            "     |  \n",
            "     |      >>> from nltk.stem.lancaster import LancasterStemmer\n",
            "     |      >>> st = LancasterStemmer()\n",
            "     |      >>> st.stem('maximum')     # Remove \"-um\" when word is intact\n",
            "     |      'maxim'\n",
            "     |      >>> st.stem('presumably')  # Don't remove \"-um\" when word is not intact\n",
            "     |      'presum'\n",
            "     |      >>> st.stem('multiply')    # No action taken if word ends with \"-ply\"\n",
            "     |      'multiply'\n",
            "     |      >>> st.stem('provision')   # Replace \"-sion\" with \"-j\" to trigger \"j\" set of rules\n",
            "     |      'provid'\n",
            "     |      >>> st.stem('owed')        # Word starting with vowel must contain at least 2 letters\n",
            "     |      'ow'\n",
            "     |      >>> st.stem('ear')         # ditto\n",
            "     |      'ear'\n",
            "     |      >>> st.stem('saying')      # Words starting with consonant must contain at least 3\n",
            "     |      'say'\n",
            "     |      >>> st.stem('crying')      #     letters and one of those letters must be a vowel\n",
            "     |      'cry'\n",
            "     |      >>> st.stem('string')      # ditto\n",
            "     |      'string'\n",
            "     |      >>> st.stem('meant')       # ditto\n",
            "     |      'meant'\n",
            "     |      >>> st.stem('cement')      # ditto\n",
            "     |      'cem'\n",
            "     |      >>> st_pre = LancasterStemmer(strip_prefix_flag=True)\n",
            "     |      >>> st_pre.stem('kilometer') # Test Prefix\n",
            "     |      'met'\n",
            "     |      >>> st_custom = LancasterStemmer(rule_tuple=(\"ssen4>\", \"s1t.\"))\n",
            "     |      >>> st_custom.stem(\"ness\") # Change s to t\n",
            "     |      'nest'\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      LancasterStemmer\n",
            "     |      nltk.stem.api.StemmerI\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, rule_tuple=None, strip_prefix_flag=False)\n",
            "     |      Create an instance of the Lancaster stemmer.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __unicode__ = __str__(self, /)\n",
            "     |  \n",
            "     |  parseRules(self, rule_tuple=None)\n",
            "     |      Validate the set of rules used in this stemmer.\n",
            "     |      \n",
            "     |      If this function is called as an individual method, without using stem\n",
            "     |      method, rule_tuple argument will be compiled into self.rule_dictionary.\n",
            "     |      If this function is called within stem, self._rule_tuple will be used.\n",
            "     |  \n",
            "     |  stem(self, word)\n",
            "     |      Stem a word using the Lancaster stemmer.\n",
            "     |  \n",
            "     |  unicode_repr = __repr__(self)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  default_rule_tuple = ('ai*2.', 'a*1.', 'bb1.', 'city3s.', 'ci2>', 'cn1...\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from nltk.stem.api.StemmerI:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "\n",
            "DATA\n",
            "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.7/dist-packages/nltk/stem/lancaster.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization을 위해서는 `WordNetLemmatizer` 클래스를 사용합니다. 이 클래스는 `wordnet` 모듈에 포함됩니다."
      ],
      "metadata": {
        "id": "FgNnubu0JptS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(nltk.stem.wordnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXRNN0HbJt3-",
        "outputId": "fd703a36-06e8-483e-9197-3f5ea04b4a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on module nltk.stem.wordnet in nltk.stem:\n",
            "\n",
            "NAME\n",
            "    nltk.stem.wordnet\n",
            "\n",
            "DESCRIPTION\n",
            "    # Natural Language Toolkit: WordNet stemmer interface\n",
            "    #\n",
            "    # Copyright (C) 2001-2017 NLTK Project\n",
            "    # Author: Steven Bird <stevenbird1@gmail.com>\n",
            "    #         Edward Loper <edloper@gmail.com>\n",
            "    # URL: <http://nltk.org/>\n",
            "    # For license information, see LICENSE.TXT\n",
            "\n",
            "CLASSES\n",
            "    builtins.object\n",
            "        WordNetLemmatizer\n",
            "    \n",
            "    class WordNetLemmatizer(builtins.object)\n",
            "     |  WordNet Lemmatizer\n",
            "     |  \n",
            "     |  Lemmatize using WordNet's built-in morphy function.\n",
            "     |  Returns the input word unchanged if it cannot be found in WordNet.\n",
            "     |  \n",
            "     |      >>> from nltk.stem import WordNetLemmatizer\n",
            "     |      >>> wnl = WordNetLemmatizer()\n",
            "     |      >>> print(wnl.lemmatize('dogs'))\n",
            "     |      dog\n",
            "     |      >>> print(wnl.lemmatize('churches'))\n",
            "     |      church\n",
            "     |      >>> print(wnl.lemmatize('aardwolves'))\n",
            "     |      aardwolf\n",
            "     |      >>> print(wnl.lemmatize('abaci'))\n",
            "     |      abacus\n",
            "     |      >>> print(wnl.lemmatize('hardrock'))\n",
            "     |      hardrock\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __unicode__ = __str__(self, /)\n",
            "     |  \n",
            "     |  lemmatize(self, word, pos='n')\n",
            "     |  \n",
            "     |  unicode_repr = __repr__(self)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "\n",
            "FUNCTIONS\n",
            "    teardown_module(module=None)\n",
            "        # unload wordnet\n",
            "\n",
            "DATA\n",
            "    NOUN = 'n'\n",
            "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
            "    wordnet = <WordNetCorpusReader in '.../corpora/wordnet' (not loaded ye...\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.7/dist-packages/nltk/stem/wordnet.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 Stemming을 수행해 보겠습니다. 클래스 인스턴스를 생성하여 `stem(단어)` 메서드를 사용하는 방법입니다."
      ],
      "metadata": {
        "id": "baH7sbK8IiHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0R_A-AsHUs_",
        "outputId": "d2c3dd17-2307-4f2e-c906-878c91ef6c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 'work': 기본 단어인 'work'에 'ing', 's', 'ed'가 붙는 단순한 변화이므로 'work'를 출력했습니다.\n",
        "\n",
        "- 'amuse': 기본 단어인 'amuse'에서 'e'가 탈락하고 'ing', 'es', 'ed'가 붙는 복잡한 변화이므로 'amus'를 기본 단어로 인식하고 출력했습니다.\n",
        "\n",
        "- 'happy', 'fancy': 원형을 찾지 못하고 원형에서 철자가 다른 어근 단어로 인식하고 출력했습니다.\n",
        "\n",
        "이번에는 Lemmatization을 수행해 보겠습니다. 클래스 인스턴스를 생성하여 `lemmatize(단어, 품사)` 메서드를 사용하는 방법입니다. 품사로 전달할 인자는 동사는 `v`, 형용사는 `a`입니다."
      ],
      "metadata": {
        "id": "3FUZ6dFaL95L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
        "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
        "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byhd8V1sS9ow",
        "outputId": "9454d223-2339-4b37-8123-b3a35a4ba9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정확하게 출력했습니다. "
      ],
      "metadata": {
        "id": "hYh5rwXcUdls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Bag of Words - BOW**"
      ],
      "metadata": {
        "id": "WBs3j00flIl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BOW** 모델은 문서가 가진 모든 단어를 문맥과 순서를 무시하고 일괄적으로 단어가 출현하는 만큼 빈도 값을 부여하여 피처 값을 추출합니다. 문서에 포함된 모든 단어를 봉투(Bag)에 넣고 섞는다는 뜻에서 Bag of Words라는 표현을 채택한 것입니다.\n",
        "\n",
        "BOW 모델의 핵심 원리를 살펴보겠습니다. 중복된 단어를 가진 두 문장이 있다고 가정하겠습니다. 더 자세한 내용은 본서를 참고하시기 바랍니다.\n",
        "\n",
        "1. 중복된 단어를 하나만 남기고 모두 제거합니다. 각 단어를 피처(칼럼) 형태로 나열하고 고유 인덱스를 부여합니다.\n",
        "\n",
        "1. 각 단어가 나타난 횟수를 인덱스에 부여합니다. 예컨대 문장1과 문장2가 있다고 가정할 때, 문장1만 특정 단어가 있고 나타난 횟수가 1번이면 1을 부여하고, 문장1과 문장2 모두 특정 단어가 있고 나타난 횟수가 각각 1번과 2번이면 각각 1과 2를 부여합니다.\n",
        "\n",
        "BOW 모델의 장점은 아래와 같습니다.\n",
        "\n",
        "- 쉽고 빠르게 구축할 수 있습니다.\n",
        "\n",
        "- 문서 특징을 잘 나타내며 여러 분야에서 활용합니다.\n",
        "\n",
        "단점은 아래와 같습니다.\n",
        "\n",
        "- 문맥 의미(Semantic Context)를 반영하기에 부족합니다. 단어 순서를 고려하지 않으므로 문맥 의미를 무시합니다. 보완책으로 n_gram 기법을 활용할 수 있지만 제한적이기 때문에 문맥 해석을 처리하기 어렵습니다.\n",
        "\n",
        "- **희소 행렬(Sparse Matrix)** 문제가 있습니다. BOW로 피처 벡터화를 수행하면 희소 행렬 형태를 가진 데이터 세트가 만들어지기 쉽습니다. 많은 문서에서 단어를 추출하면 상당한 수의 칼럼이 생성되는데, 문서마다 서로 다른 단어로 칼럼이 구성되므로 단어가 문서마다 나타나지 않는 경우가 훨씬 많습니다. 예컨대 문서1에 단어가 수십만 개일 경우 문서2에 있는 단어는 문서1과 공통된 단어 수가 적을 가능성이 높기 때문에, 공통되지 않은 단어는 모두 0 값으로 채워집니다.\n",
        "\n",
        "희소 행렬은 머신러닝 알고리즘의 수행 시간과 예측 성능을 떨어뜨리므로 이를 극복하기 위한 기법이 존재합니다. 아래에서 다시 다루겠습니다. 참고로 0 값이 아니라 유의미한 값으로 채워진 행렬은 **밀집 행렬(Dense Matrix)**입니다. "
      ],
      "metadata": {
        "id": "WI34lI9LUh-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1. BOW 피처 벡터화**"
      ],
      "metadata": {
        "id": "ceqQYcX0lIj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "머신러닝 알고리즘은 숫자형 피처를 데이터로 받아서 동작하므로 텍스트 같은 데이터를 알고리즘에 바로 입력할 수 없습니다. 텍스트를 특정 의미를 가진 숫자형 값인 벡터 값으로 변환하는 **피처 벡터화** 작업을 거쳐야 합니다. 자세한 내용은 본서를 참고하시기 바랍니다. \n",
        "\n",
        "피처 벡터화의 원리는 아래와 같습니다. \n",
        "\n",
        "1. 각 문서가 가진 텍스트를 단어로 추출하여 피처로 할당합니다.\n",
        "\n",
        "1. 각 단어의 발생 빈도와 같은 값을 피처에 값으로 부여합니다.\n",
        "\n",
        "1. 단어 피처의 발생 빈도 값으로 구성된 벡터로 만듭니다.\n",
        "\n",
        "즉, BOW 모델에서 피처 벡터화를 수행하는 것은 모든 문서에서 모든 단어를 칼럼 형태로 나열하고 각 문서에서 해당 단어 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 작업을 의미합니다. 예컨대 텍스트 문서가 M개만큼 존재하고 이 텍스트 문서들에서 모든 단어 N개를 추출하여 나열한다면 단어 N개의 값이 할당된 피처의 벡터 세트가 생성되며, 단어 피처가 $M \\times N$개로 구성된 행렬이 만들어집니다.\n",
        "\n",
        "BOW의 피처 벡터화는 아래 방식이 존재합니다.\n",
        "\n",
        "- 카운트(Count) 기반 벡터화: 단어 피처에 값을 부여할 때 단어가 나타난 횟수(카운트)를 부여합니다. 카운트 값이 높을수록 중요한 단어입니다. 이 방식의 약점이라면 별다른 의미 없이 자주 사용될 수밖에 없는 단어까지 높은 값을 부여하기 때문에 문서 특징을 나타내는 중요한 단어에 집중하지 못합니다. 문서 개수가 적거나 텍스트 길이가 적은 경우에 적합한 방법입니다.\n",
        "\n",
        "- **TF-IDF(Term Frequency - Inverse Document Frequency)** 기반 벡터화: 카운트 기반 벡터화의 약점을 보완하기 위한 방법입니다. 특성 문서에서 자주 나타난 단어에 높은 가중치를 부여하지만 모든 문서에서 전반적으로 자주 나타나는 단어에는 페널티(Penalty)를 부여합니다. 예컨대 언어 특성상 모든 문서에서 보편적이고 반복적으로 사용될 수 있는 부사('많이', '상당하게', 당연하게' 등)나 명사('이름', '업무', '조직' 등)와 같은 단어들은 중요하지 않으므로(물론 형용사도 포함될 수 있습니다) 페널티를 부여하여 가중치 균형을 잡습니다. 문서 개수가 많거나 텍스트 길이가 많은 경우에 적합한 방법입니다. "
      ],
      "metadata": {
        "id": "slttFhByv2vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2. 사이킷런의 Count 및 TF-IDF 벡터화 구현: CountVectorizer, TfidfVectorizer**"
      ],
      "metadata": {
        "id": "brJCR7ozlIho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CounterVectorizer` 클래스는 카운트 기반 벡터화를 구현한 클래스이며 소문자 일괄 변환, 토큰화, 스톱 워드 필터링 등 전처리도 수행합니다. 파라미터는 아래와 같습니다. 참고로 TF-IDF를 구현한 `TfidfVectorizer` 클래스는 `CountVectorizer` 클래스의 파라미터와 변환 방법이 동일합니다.\n"
      ],
      "metadata": {
        "id": "lbesuOOI3Bs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "help(CountVectorizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwQC9mbMVkH1",
        "outputId": "bb7e8c09-2f53-4448-9952-6f0794717cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
            "\n",
            "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
            " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
            " |  \n",
            " |  Convert a collection of text documents to a matrix of token counts.\n",
            " |  \n",
            " |  This implementation produces a sparse representation of the counts using\n",
            " |  scipy.sparse.csr_matrix.\n",
            " |  \n",
            " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
            " |  that does some kind of feature selection then the number of features will\n",
            " |  be equal to the vocabulary size found by analyzing the data.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  input : {'filename', 'file', 'content'}, default='content'\n",
            " |      - If `'filename'`, the sequence passed as an argument to fit is\n",
            " |        expected to be a list of filenames that need reading to fetch\n",
            " |        the raw content to analyze.\n",
            " |  \n",
            " |      - If `'file'`, the sequence items must have a 'read' method (file-like\n",
            " |        object) that is called to fetch the bytes in memory.\n",
            " |  \n",
            " |      - If `'content'`, the input is expected to be a sequence of items that\n",
            " |        can be of type string or byte.\n",
            " |  \n",
            " |  encoding : str, default='utf-8'\n",
            " |      If bytes or files are given to analyze, this encoding is used to\n",
            " |      decode.\n",
            " |  \n",
            " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
            " |      Instruction on what to do if a byte sequence is given to analyze that\n",
            " |      contains characters not of the given `encoding`. By default, it is\n",
            " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
            " |      values are 'ignore' and 'replace'.\n",
            " |  \n",
            " |  strip_accents : {'ascii', 'unicode'}, default=None\n",
            " |      Remove accents and perform other character normalization\n",
            " |      during the preprocessing step.\n",
            " |      'ascii' is a fast method that only works on characters that have\n",
            " |      an direct ASCII mapping.\n",
            " |      'unicode' is a slightly slower method that works on any characters.\n",
            " |      None (default) does nothing.\n",
            " |  \n",
            " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
            " |      :func:`unicodedata.normalize`.\n",
            " |  \n",
            " |  lowercase : bool, default=True\n",
            " |      Convert all characters to lowercase before tokenizing.\n",
            " |  \n",
            " |  preprocessor : callable, default=None\n",
            " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
            " |      preserving the tokenizing and n-grams generation steps.\n",
            " |      Only applies if ``analyzer`` is not callable.\n",
            " |  \n",
            " |  tokenizer : callable, default=None\n",
            " |      Override the string tokenization step while preserving the\n",
            " |      preprocessing and n-grams generation steps.\n",
            " |      Only applies if ``analyzer == 'word'``.\n",
            " |  \n",
            " |  stop_words : {'english'}, list, default=None\n",
            " |      If 'english', a built-in stop word list for English is used.\n",
            " |      There are several known issues with 'english' and you should\n",
            " |      consider an alternative (see :ref:`stop_words`).\n",
            " |  \n",
            " |      If a list, that list is assumed to contain stop words, all of which\n",
            " |      will be removed from the resulting tokens.\n",
            " |      Only applies if ``analyzer == 'word'``.\n",
            " |  \n",
            " |      If None, no stop words will be used. max_df can be set to a value\n",
            " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
            " |      words based on intra corpus document frequency of terms.\n",
            " |  \n",
            " |  token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
            " |      Regular expression denoting what constitutes a \"token\", only used\n",
            " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
            " |      or more alphanumeric characters (punctuation is completely ignored\n",
            " |      and always treated as a token separator).\n",
            " |  \n",
            " |      If there is a capturing group in token_pattern then the\n",
            " |      captured group content, not the entire match, becomes the token.\n",
            " |      At most one capturing group is permitted.\n",
            " |  \n",
            " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
            " |      The lower and upper boundary of the range of n-values for different\n",
            " |      word n-grams or char n-grams to be extracted. All values of n such\n",
            " |      such that min_n <= n <= max_n will be used. For example an\n",
            " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
            " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
            " |      Only applies if ``analyzer`` is not callable.\n",
            " |  \n",
            " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
            " |      Whether the feature should be made of word n-gram or character\n",
            " |      n-grams.\n",
            " |      Option 'char_wb' creates character n-grams only from text inside\n",
            " |      word boundaries; n-grams at the edges of words are padded with space.\n",
            " |  \n",
            " |      If a callable is passed it is used to extract the sequence of features\n",
            " |      out of the raw, unprocessed input.\n",
            " |  \n",
            " |      .. versionchanged:: 0.21\n",
            " |  \n",
            " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
            " |      first read from the file and then passed to the given callable\n",
            " |      analyzer.\n",
            " |  \n",
            " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
            " |      When building the vocabulary ignore terms that have a document\n",
            " |      frequency strictly higher than the given threshold (corpus-specific\n",
            " |      stop words).\n",
            " |      If float, the parameter represents a proportion of documents, integer\n",
            " |      absolute counts.\n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
            " |      When building the vocabulary ignore terms that have a document\n",
            " |      frequency strictly lower than the given threshold. This value is also\n",
            " |      called cut-off in the literature.\n",
            " |      If float, the parameter represents a proportion of documents, integer\n",
            " |      absolute counts.\n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  max_features : int, default=None\n",
            " |      If not None, build a vocabulary that only consider the top\n",
            " |      max_features ordered by term frequency across the corpus.\n",
            " |  \n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  vocabulary : Mapping or iterable, default=None\n",
            " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
            " |      indices in the feature matrix, or an iterable over terms. If not\n",
            " |      given, a vocabulary is determined from the input documents. Indices\n",
            " |      in the mapping should not be repeated and should not have any gap\n",
            " |      between 0 and the largest index.\n",
            " |  \n",
            " |  binary : bool, default=False\n",
            " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
            " |      probabilistic models that model binary events rather than integer\n",
            " |      counts.\n",
            " |  \n",
            " |  dtype : type, default=np.int64\n",
            " |      Type of the matrix returned by fit_transform() or transform().\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  vocabulary_ : dict\n",
            " |      A mapping of terms to feature indices.\n",
            " |  \n",
            " |  fixed_vocabulary_ : bool\n",
            " |      True if a fixed vocabulary of term to indices mapping\n",
            " |      is provided by the user.\n",
            " |  \n",
            " |  stop_words_ : set\n",
            " |      Terms that were ignored because they either:\n",
            " |  \n",
            " |        - occurred in too many documents (`max_df`)\n",
            " |        - occurred in too few documents (`min_df`)\n",
            " |        - were cut off by feature selection (`max_features`).\n",
            " |  \n",
            " |      This is only available if no vocabulary was given.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  HashingVectorizer : Convert a collection of text documents to a\n",
            " |      matrix of token counts.\n",
            " |  \n",
            " |  TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
            " |      of TF-IDF features.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The ``stop_words_`` attribute can get large and increase the model size\n",
            " |  when pickling. This attribute is provided only for introspection and can\n",
            " |  be safely removed using delattr or set to None before pickling.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
            " |  >>> corpus = [\n",
            " |  ...     'This is the first document.',\n",
            " |  ...     'This document is the second document.',\n",
            " |  ...     'And this is the third one.',\n",
            " |  ...     'Is this the first document?',\n",
            " |  ... ]\n",
            " |  >>> vectorizer = CountVectorizer()\n",
            " |  >>> X = vectorizer.fit_transform(corpus)\n",
            " |  >>> vectorizer.get_feature_names_out()\n",
            " |  array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
            " |         'this'], ...)\n",
            " |  >>> print(X.toarray())\n",
            " |  [[0 1 1 1 0 0 1 0 1]\n",
            " |   [0 2 0 1 0 1 1 0 1]\n",
            " |   [1 0 0 1 1 0 1 1 1]\n",
            " |   [0 1 1 1 0 0 1 0 1]]\n",
            " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
            " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
            " |  >>> vectorizer2.get_feature_names_out()\n",
            " |  array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
            " |         'second document', 'the first', 'the second', 'the third', 'third one',\n",
            " |         'this document', 'this is', 'this the'], ...)\n",
            " |   >>> print(X2.toarray())\n",
            " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
            " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
            " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      CountVectorizer\n",
            " |      _VectorizerMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, raw_documents, y=None)\n",
            " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      y : None\n",
            " |          This parameter is ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted vectorizer.\n",
            " |  \n",
            " |  fit_transform(self, raw_documents, y=None)\n",
            " |      Learn the vocabulary dictionary and return document-term matrix.\n",
            " |      \n",
            " |      This is equivalent to fit followed by transform, but more efficiently\n",
            " |      implemented.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      y : None\n",
            " |          This parameter is ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : array of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |  \n",
            " |  get_feature_names(self)\n",
            " |      DEPRECATED: get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            " |      \n",
            " |      Array mapping from feature integer indices to feature name.\n",
            " |      \n",
            " |          Returns\n",
            " |          -------\n",
            " |          feature_names : list\n",
            " |              A list of feature names.\n",
            " |  \n",
            " |  get_feature_names_out(self, input_features=None)\n",
            " |      Get output feature names for transformation.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      input_features : array-like of str or None, default=None\n",
            " |          Not used, present here for API consistency by convention.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_names_out : ndarray of str objects\n",
            " |          Transformed feature names.\n",
            " |  \n",
            " |  inverse_transform(self, X)\n",
            " |      Return terms per document with nonzero entries in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_inv : list of arrays of shape (n_samples,)\n",
            " |          List of arrays of terms.\n",
            " |  \n",
            " |  transform(self, raw_documents)\n",
            " |      Transform documents to document-term matrix.\n",
            " |      \n",
            " |      Extract token counts out of raw text documents using the vocabulary\n",
            " |      fitted with fit or the one provided to the constructor.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : sparse matrix of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from _VectorizerMixin:\n",
            " |  \n",
            " |  build_analyzer(self)\n",
            " |      Return a callable to process input data.\n",
            " |      \n",
            " |      The callable handles that handles preprocessing, tokenization, and\n",
            " |      n-grams generation.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      analyzer: callable\n",
            " |          A function to handle preprocessing, tokenization\n",
            " |          and n-grams generation.\n",
            " |  \n",
            " |  build_preprocessor(self)\n",
            " |      Return a function to preprocess the text before tokenization.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      preprocessor: callable\n",
            " |            A function to preprocess the text before tokenization.\n",
            " |  \n",
            " |  build_tokenizer(self)\n",
            " |      Return a function that splits a string into a sequence of tokens.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      tokenizer: callable\n",
            " |            A function to split a string into a sequence of tokens.\n",
            " |  \n",
            " |  decode(self, doc)\n",
            " |      Decode the input into a string of unicode symbols.\n",
            " |      \n",
            " |      The decoding strategy depends on the vectorizer parameters.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      doc : bytes or str\n",
            " |          The string to decode.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      doc: str\n",
            " |          A string of unicode symbols.\n",
            " |  \n",
            " |  get_stop_words(self)\n",
            " |      Build or fetch the effective stop words list.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      stop_words: list or None\n",
            " |              A list of stop words.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from _VectorizerMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CountVectorizer` 클래스를 활용하여 카운트 기반 피처 벡터화를 수행하는 방법은 아래와 같습니다.\n",
        "\n",
        "1. 데이터 전처리: 영어의 경우 단어를 소문자로 변경하는 등 전처리 작업을 수행합니다. 디폴트는 `lowercase=Ture`입니다.\n",
        "\n",
        "1. 디폴트 `analyzer=word`인 단어 기준으로 `n_gram_range` 파라미터를 반영하여 단어를 토큰화합니다. `n_gram_range` 파라미터는 단어 순서를 보강하기 위한 범위를 지정하는 것이며 `(1, 1)`로 지정하면 토큰화된 단어를 1개씩 피처로 추출하고 `(1, 2)`로 지정하면 토큰화된 단어를 최소 1개씩 순서대로 최대 2개씩(최대 2개) 묶어서 추출합니다. \n",
        "\n",
        "1. 텍스트 정규화를 수행합니다. 단, `stop_words='english'`로 파라미터를 지정하면 스톱 워드만 필터링 합니다. Stemming이나 Lemmatization 같은 어근 변환은 `CountVectorizer` 클래스에서 지원하진 않지만 `tokenizer` 파라미터에 어근 변환 함수를 따로 선언하여 적용하거나 외부 패키지를 적용하면 어근 변환을 수행할 수 있습니다.\n",
        "\n",
        "1. `max_df`, `min_df`, `max_features` 등 파라미터를 이용하여 토큰화된 단어를 피처로 추출하고 단어 빈도수 벡터 값을 적용합니다. \n",
        "\n",
        " - `max_df`: 높은 빈도수를 가진 단어 피처를 제외합니다. 자주 나타나는 단어는 스톱 워드와 비슷한 반복적인 단어일 가능성이 높습니다.\n",
        "\n",
        " - `min_df`: 낮은 빈도수를 가진 단어 피처를 제외합니다. 적게 나타나는 단어는 중요하지 않은 단어일 가능성이 높습니다.\n",
        "\n",
        " - `max_features`: 추출할 피처 개수를 제한합니다."
      ],
      "metadata": {
        "id": "S4xq2GSWWxpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3. BOW 벡터화를 위한 희소 행렬**"
      ],
      "metadata": {
        "id": "qe1Qyw3_lIfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "대규모의 행렬이 생성될 때 레코드의 각 문서가 가지는 단어 수는 제한적이므로 행렬 값은 대부분 0입니다. 이렇게 대규모 행렬의 값이 대부분 0인 행렬을 희소 행렬이라고 합니다. BOW 형태를 가진 언어 모델의 피처 벡터화는 대부분 희소 행렬입니다.\n",
        "\n",
        "다만 희소 행렬은 불필요한 0 값이 메모리 공간에 지나치게 많이 할당되어 많은 메모리 공간이 필요하고 행렬 크기가 커서 연산할 때 데이터 엑세스를 위한 오랜 시간이 소요됩니다. 물리적으로 적은 메모리 공간을 사용하도록 변환하는 방법은 **COO** 형식과 **CSR** 형식이 있습니다. 이 중에 CSR이 더 뛰어나고 많이 사용됩니다."
      ],
      "metadata": {
        "id": "RKsNwnWadUHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4. 희소 행렬 - COO 형식**"
      ],
      "metadata": {
        "id": "MM3suxkZlIdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COO(Coordinate: 좌표)** 형식은 0이 아닌 데이터만 별도 데이터 배열에 저장하고, 그 데이터가 가리키는 행과 열 위치를 별도 배열로 저장합니다. 예컨대 아래 2차원 데이터인 밀집 행렬이 있다고 가정하겠습니다.\n",
        "\n",
        "```\n",
        "[[3, 0, 1],\n",
        " [0, 2, 0]]\n",
        "```\n",
        "\n",
        "여기서 0이 아닌 데이터 `[3, 1, 2]`를 위치(row, col)로 표시하면 `(0, 0), (0, 2), (1, 1)`입니다. 로우(row)와 칼럼(col)을 따로 떼어 별도 배열로 저장하면 로우는 `[0, 0, 1]`이고 칼럼은 `[0, 2, 1]`입니다.\n",
        "\n",
        "사이파이의 `sparse` 패키지를 사용하면 희소 행렬 변환을 수행할 수 있습니다. 위 2차원 데이터를 COO 형식으로 희소 행렬로 변환해 보겠습니다. 사용할 클래스는 `coo_matrix`입니다. 참고로 `coo_matrix`의 명칭은 소문자로 시작하지만 메서드가 아닌 클래스입니다."
      ],
      "metadata": {
        "id": "xrFt_F9I5o-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "# 밀집 행렬을 생성합니다.\n",
        "dense = np.array([[3, 0, 1], [0, 2, 0]])\n",
        "\n",
        "# 0이 아닌 데이터를 추출합니다.\n",
        "data = np.array([3, 1, 2])\n",
        "\n",
        "# 행 위치와 열 위치를 배열로 생성합니다.\n",
        "row_pos = np.array([0, 0, 1])\n",
        "col_pos = np.array([0, 2, 1])\n",
        "\n",
        "# COO 형식으로 희소 행렬을 생성하고 인스턴스화합니다.\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
        "\n",
        "# `toarray()` 메서드를 사용하여 밀집 행렬로 만듭니다.\n",
        "sparse_coo.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEiZfCgN7KNn",
        "outputId": "d27a44a6-9710-4219-b53c-adb3056e75d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COO 형식의 문제점이라면 행과 열 위치를 나타내기 위해 반복적으로 위치 데이터를 사용해야 하는 것입니다. 예컨대 아래 2차원 배열이 있다고 가정하겠습니다.\n",
        "\n",
        "```\n",
        "[[0, 0, 1, 0, 0, 5],\n",
        " [1, 4, 0, 3, 2, 5],\n",
        " [0, 6, 0, 3, 0, 0],\n",
        " [2, 0, 0, 0, 0, 0],\n",
        " [0, 0, 0, 7, 0, 8],\n",
        " [1, 0, 0, 0, 0, 0]]\n",
        "```\n",
        "\n",
        "여기서 0이 아닌 데이터 배열은 아래와 같습니다.\n",
        "\n",
        "`[1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1]`\n",
        "\n",
        "행 위치 배열은 아래와 같습니다.\n",
        "\n",
        "`[0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5]`\n",
        "\n",
        "열 위치 배열은 아래와 같습니다.\n",
        "\n",
        "`[2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0]`\n",
        "\n",
        "이 중에 행 위치 배열은 같은 값이 반복적으로 나열되고 순차적으로 증가합니다. 0은 2번, 1은 5번, 2는 2번인 식입니다. 따라서 행 위치 배열의 고유한 값의 시작 위치만 표기한다면(위치의 위치를 표기한다면) 불필요한 반복을 제거할 수 있을 것입니다. 인덱스를 기준으로 삼아 0의 시작 위치는 0, 1의 시작 위치는 2, 2의 시작 위치는 7, 3의 시작 위치는 9, 4의 시작 위치는 10, 5의 시작 위치는 12이므로 `[0, 2, 7, 9, 10, 12]`가 됩니다. 최종적으로 이 배열의 맨 마지막에 데이터의 총 항목 개수를 추가하면 `[0, 2, 7, 9, 10, 12, 13]`이 됩니다. 이 일련의 방식이 바로 아래에서 기술할 **CSR(Compressed Sparse Row)** 형식입니다."
      ],
      "metadata": {
        "id": "4yaM6G0RAdhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.5. 희소 행렬 - CSR 형식**"
      ],
      "metadata": {
        "id": "DGQLXJ1QlIbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CSR(Compressed Sparse Row)** 형식은 COO 형식보다 메모리가 적게 필요하고 빠르게 연산할 수 있습니다. `csr_matrix` 클래스에 구현되어 있으며 전달할 인자는 `(0이 아닌 데이터 배열, 열 위치 배열, 행 위치 배열의 고유한 값의 시작 위치)`입니다.\n",
        "\n",
        "CSR 형식을 코드로 구현해 보겠습니다."
      ],
      "metadata": {
        "id": "iKlPHDST_OSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense2 = np.array([[0, 0, 1, 0, 0, 5],\n",
        "                   [1, 4, 0, 3, 2, 5],\n",
        "                   [0, 6, 0, 3, 0, 0],\n",
        "                   [2, 0, 0, 0, 0, 0],\n",
        "                   [0, 0, 0, 7, 0, 8],\n",
        "                   [1, 0, 0, 0, 0, 0]])\n",
        "\n",
        "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
        "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
        "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
        "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
        "\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "print(sparse_csr.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpU1MTcpIJ1D",
        "outputId": "723f6c0a-fc09-4c18-a378-0ecb7039cc48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제로 사용할 때는 생성 파라미터로 밀집 행렬만 전달하면 됩니다."
      ],
      "metadata": {
        "id": "K3bZLz4-JX3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense3 = np.array([[0, 0, 1, 0, 0, 5],\n",
        "                   [1, 4, 0, 3, 2, 5],\n",
        "                   [0, 6, 0, 3, 0, 0],\n",
        "                   [2, 0, 0, 0, 0, 0],\n",
        "                   [0, 0, 0, 7, 0, 8],\n",
        "                   [1, 0, 0, 0, 0, 0]])\n",
        "\n",
        "coo = sparse.coo_matrix(dense3)\n",
        "csr = sparse.csr_matrix(dense3)\n",
        "print(coo.toarray())\n",
        "print()\n",
        "print(csr.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvOU6tZ9Jh4w",
        "outputId": "0b110453-1cea-4a83-b53a-050125b63679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고로 `CountVectorizer` 클래스와 `TfidfVectorizer` 클래스로 변환된 피처 벡터화 행렬은 모두 CSR 형태의 희소 행렬입니다."
      ],
      "metadata": {
        "id": "Dkr-cn7_KQn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. 텍스트 분류 실습 - 20 뉴스그룹 분류**"
      ],
      "metadata": {
        "id": "qktTPqy_lIY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사이킷런의 20 뉴스그룹 데이터 세트를 사용하여 텍스트 분류를 수행해 보겠습니다. 텍스트 분류는 특정 문서의 분류를 학습 데이터를 토대로 학습하고 모델을 생성하여 다른 문서의 분류를 예측하는 것입니다.\n",
        "\n",
        "텍스트를 피처 벡터화로 변환하면 일반적으로 희소 행렬이 됩니다. 이 희소 행렬에 분류를 효과적으로 처리할 수 있는 알고리즘은 **로지스틱 회귀**, **선형 서포트 벡터 머신**, **나이브 베이즈** 등입니다. 텍스트를 기반으로 분류를 수행하려면 먼저 텍스트를 정규화하고 피처 벡터화를 적용해야 합니다. 그리고나서 알고리즘을 적용하고 분류를 학습하여 예측하고 평가합니다. \n",
        "\n",
        "이번 파트에서는 카운트 기반과 TF-IDF 기반 벡터화를 모두 적용하여 예측 성능을 비교하고 피처 백터화를 위한 파라미터와 `GridSearchCV` 기반의 하이퍼 파라미터 튜닝, `Pipeline` 인스턴스를 통한 피처 벡터화 파라미터와 `GridSearchCV` 기반의 하이퍼 파라미터 튜닝을 전부 수행해 보겠습니다."
      ],
      "metadata": {
        "id": "5TbIRrGxKjJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1. 텍스트 정규화**"
      ],
      "metadata": {
        "id": "fI4ltzX4lIWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`fetch_20newsgroups()` 메서드를 호출하여 데이터를 준비하겠습니다."
      ],
      "metadata": {
        "id": "RNi7YtkXqIJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(fetch_20newsgroups)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7a25rXjW9Yh",
        "outputId": "fe65506c-99cd-4031-9d81-29634aafd3eb"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function fetch_20newsgroups in module sklearn.datasets._twenty_newsgroups:\n",
            "\n",
            "fetch_20newsgroups(*, data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True, return_X_y=False)\n",
            "    Load the filenames and data from the 20 newsgroups dataset (classification).\n",
            "    \n",
            "    Download it if necessary.\n",
            "    \n",
            "    =================   ==========\n",
            "    Classes                     20\n",
            "    Samples total            18846\n",
            "    Dimensionality               1\n",
            "    Features                  text\n",
            "    =================   ==========\n",
            "    \n",
            "    Read more in the :ref:`User Guide <20newsgroups_dataset>`.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    data_home : str, default=None\n",
            "        Specify a download and cache folder for the datasets. If None,\n",
            "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
            "    \n",
            "    subset : {'train', 'test', 'all'}, default='train'\n",
            "        Select the dataset to load: 'train' for the training set, 'test'\n",
            "        for the test set, 'all' for both, with shuffled ordering.\n",
            "    \n",
            "    categories : array-like, dtype=str, default=None\n",
            "        If None (default), load all the categories.\n",
            "        If not None, list of category names to load (other categories\n",
            "        ignored).\n",
            "    \n",
            "    shuffle : bool, default=True\n",
            "        Whether or not to shuffle the data: might be important for models that\n",
            "        make the assumption that the samples are independent and identically\n",
            "        distributed (i.i.d.), such as stochastic gradient descent.\n",
            "    \n",
            "    random_state : int, RandomState instance or None, default=None\n",
            "        Determines random number generation for dataset shuffling. Pass an int\n",
            "        for reproducible output across multiple function calls.\n",
            "        See :term:`Glossary <random_state>`.\n",
            "    \n",
            "    remove : tuple, default=()\n",
            "        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
            "        these are kinds of text that will be detected and removed from the\n",
            "        newsgroup posts, preventing classifiers from overfitting on\n",
            "        metadata.\n",
            "    \n",
            "        'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
            "        ends of posts that look like signatures, and 'quotes' removes lines\n",
            "        that appear to be quoting another post.\n",
            "    \n",
            "        'headers' follows an exact standard; the other filters are not always\n",
            "        correct.\n",
            "    \n",
            "    download_if_missing : bool, default=True\n",
            "        If False, raise an IOError if the data is not locally available\n",
            "        instead of trying to download the data from the source site.\n",
            "    \n",
            "    return_X_y : bool, default=False\n",
            "        If True, returns `(data.data, data.target)` instead of a Bunch\n",
            "        object.\n",
            "    \n",
            "        .. versionadded:: 0.22\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    bunch : :class:`~sklearn.utils.Bunch`\n",
            "        Dictionary-like object, with the following attributes.\n",
            "    \n",
            "        data : list of shape (n_samples,)\n",
            "            The data list to learn.\n",
            "        target: ndarray of shape (n_samples,)\n",
            "            The target labels.\n",
            "        filenames: list of shape (n_samples,)\n",
            "            The path to the location of the data.\n",
            "        DESCR: str\n",
            "            The full description of the dataset.\n",
            "        target_names: list of shape (n_classes,)\n",
            "            The names of target classes.\n",
            "    \n",
            "    (data, target) : tuple if `return_X_y=True`\n",
            "        .. versionadded:: 0.22\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "news_data = fetch_20newsgroups(subset='all', random_state=156)"
      ],
      "metadata": {
        "id": "h5oj6blDVtzE"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 구성을 살펴보겠습니다. 자세한 설명은 생략합니다."
      ],
      "metadata": {
        "id": "qgdxTdQMWM9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_data.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKUd1ZA4WEH_",
        "outputId": "d4352a5a-aa3b-4dcc-9482-38db481932c3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_data.DESCR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NXWq9rh2oJx",
        "outputId": "c891f848-4b77-4e26-f34d-2d88879c5f32"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _20newsgroups_dataset:\n",
            "\n",
            "The 20 newsgroups text dataset\n",
            "------------------------------\n",
            "\n",
            "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
            "20 topics split in two subsets: one for training (or development)\n",
            "and the other one for testing (or for performance evaluation). The split\n",
            "between the train and test set is based upon a messages posted before\n",
            "and after a specific date.\n",
            "\n",
            "This module contains two loaders. The first one,\n",
            ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
            "returns a list of the raw texts that can be fed to text feature\n",
            "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
            "with custom parameters so as to extract feature vectors.\n",
            "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
            "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
            "extractor.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    =================   ==========\n",
            "    Classes                     20\n",
            "    Samples total            18846\n",
            "    Dimensionality               1\n",
            "    Features                  text\n",
            "    =================   ==========\n",
            "\n",
            "Usage\n",
            "~~~~~\n",
            "\n",
            "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
            "fetching / caching functions that downloads the data archive from\n",
            "the original `20 newsgroups website`_, extracts the archive contents\n",
            "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
            ":func:`sklearn.datasets.load_files` on either the training or\n",
            "testing set folder, or both of them::\n",
            "\n",
            "  >>> from sklearn.datasets import fetch_20newsgroups\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
            "\n",
            "  >>> from pprint import pprint\n",
            "  >>> pprint(list(newsgroups_train.target_names))\n",
            "  ['alt.atheism',\n",
            "   'comp.graphics',\n",
            "   'comp.os.ms-windows.misc',\n",
            "   'comp.sys.ibm.pc.hardware',\n",
            "   'comp.sys.mac.hardware',\n",
            "   'comp.windows.x',\n",
            "   'misc.forsale',\n",
            "   'rec.autos',\n",
            "   'rec.motorcycles',\n",
            "   'rec.sport.baseball',\n",
            "   'rec.sport.hockey',\n",
            "   'sci.crypt',\n",
            "   'sci.electronics',\n",
            "   'sci.med',\n",
            "   'sci.space',\n",
            "   'soc.religion.christian',\n",
            "   'talk.politics.guns',\n",
            "   'talk.politics.mideast',\n",
            "   'talk.politics.misc',\n",
            "   'talk.religion.misc']\n",
            "\n",
            "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
            "attribute is the integer index of the category::\n",
            "\n",
            "  >>> newsgroups_train.filenames.shape\n",
            "  (11314,)\n",
            "  >>> newsgroups_train.target.shape\n",
            "  (11314,)\n",
            "  >>> newsgroups_train.target[:10]\n",
            "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
            "\n",
            "It is possible to load only a sub-selection of the categories by passing the\n",
            "list of the categories to load to the\n",
            ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
            "\n",
            "  >>> cats = ['alt.atheism', 'sci.space']\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
            "\n",
            "  >>> list(newsgroups_train.target_names)\n",
            "  ['alt.atheism', 'sci.space']\n",
            "  >>> newsgroups_train.filenames.shape\n",
            "  (1073,)\n",
            "  >>> newsgroups_train.target.shape\n",
            "  (1073,)\n",
            "  >>> newsgroups_train.target[:10]\n",
            "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
            "\n",
            "Converting text to vectors\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "In order to feed predictive or clustering models with the text data,\n",
            "one first need to turn the text into vectors of numerical values suitable\n",
            "for statistical analysis. This can be achieved with the utilities of the\n",
            "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
            "example that extract `TF-IDF`_ vectors of unigram tokens\n",
            "from a subset of 20news::\n",
            "\n",
            "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
            "  ...               'comp.graphics', 'sci.space']\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
            "  ...                                       categories=categories)\n",
            "  >>> vectorizer = TfidfVectorizer()\n",
            "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
            "  >>> vectors.shape\n",
            "  (2034, 34118)\n",
            "\n",
            "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
            "components by sample in a more than 30000-dimensional space\n",
            "(less than .5% non-zero features)::\n",
            "\n",
            "  >>> vectors.nnz / float(vectors.shape[0])\n",
            "  159.01327...\n",
            "\n",
            ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
            "returns ready-to-use token counts features instead of file names.\n",
            "\n",
            ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
            ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
            "\n",
            "\n",
            "Filtering text for more realistic training\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "It is easy for a classifier to overfit on particular things that appear in the\n",
            "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
            "high F-scores, but their results would not generalize to other documents that\n",
            "aren't from this window of time.\n",
            "\n",
            "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
            "which is fast to train and achieves a decent F-score::\n",
            "\n",
            "  >>> from sklearn.naive_bayes import MultinomialNB\n",
            "  >>> from sklearn import metrics\n",
            "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
            "  ...                                      categories=categories)\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> clf = MultinomialNB(alpha=.01)\n",
            "  >>> clf.fit(vectors, newsgroups_train.target)\n",
            "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
            "  0.88213...\n",
            "\n",
            "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
            "the training and test data, instead of segmenting by time, and in that case\n",
            "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
            "yet of what's going on inside this classifier?)\n",
            "\n",
            "Let's take a look at what the most informative features are:\n",
            "\n",
            "  >>> import numpy as np\n",
            "  >>> def show_top10(classifier, vectorizer, categories):\n",
            "  ...     feature_names = vectorizer.get_feature_names_out()\n",
            "  ...     for i, category in enumerate(categories):\n",
            "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
            "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
            "  ...\n",
            "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
            "  alt.atheism: edu it and in you that is of to the\n",
            "  comp.graphics: edu in graphics it is for and of to the\n",
            "  sci.space: edu it that is in and space to of the\n",
            "  talk.religion.misc: not it you in is that and to of the\n",
            "\n",
            "\n",
            "You can now see many things that these features have overfit to:\n",
            "\n",
            "- Almost every group is distinguished by whether headers such as\n",
            "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
            "- Another significant feature involves whether the sender is affiliated with\n",
            "  a university, as indicated either by their headers or their signature.\n",
            "- The word \"article\" is a significant feature, based on how often people quote\n",
            "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
            "  wrote:\"\n",
            "- Other features match the names and e-mail addresses of particular people who\n",
            "  were posting at the time.\n",
            "\n",
            "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
            "barely have to identify topics from text at all, and they all perform at the\n",
            "same high level.\n",
            "\n",
            "For this reason, the functions that load 20 Newsgroups data provide a\n",
            "parameter called **remove**, telling it what kinds of information to strip out\n",
            "of each file. **remove** should be a tuple containing any subset of\n",
            "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
            "blocks, and quotation blocks respectively.\n",
            "\n",
            "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
            "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
            "  ...                                      categories=categories)\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
            "  0.77310...\n",
            "\n",
            "This classifier lost over a lot of its F-score, just because we removed\n",
            "metadata that has little to do with topic classification.\n",
            "It loses even more if we also strip this metadata from the training data:\n",
            "\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
            "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
            "  ...                                       categories=categories)\n",
            "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
            "  >>> clf = MultinomialNB(alpha=.01)\n",
            "  >>> clf.fit(vectors, newsgroups_train.target)\n",
            "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
            "  0.76995...\n",
            "\n",
            "Some other classifiers cope better with this harder version of the task. Try\n",
            "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
            "the ``--filter`` option to compare the results.\n",
            "\n",
            ".. topic:: Data Considerations\n",
            "\n",
            "  The Cleveland Indians is a major league baseball team based in Cleveland,\n",
            "  Ohio, USA. In December 2020, it was reported that \"After several months of\n",
            "  discussion sparked by the death of George Floyd and a national reckoning over\n",
            "  race and colonialism, the Cleveland Indians have decided to change their\n",
            "  name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
            "  its informal nickname -- the Tribe -- its new team name.\" \"It’s not going to\n",
            "  be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
            "  American-themed name.\"\n",
            "\n",
            "  https://www.mlb.com/news/cleveland-indians-team-name-change\n",
            "\n",
            ".. topic:: Recommendation\n",
            "\n",
            "  - When evaluating text classifiers on the 20 Newsgroups data, you\n",
            "    should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
            "    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
            "    lower because it is more realistic.\n",
            "  - This text dataset contains data which may be inappropriate for certain NLP\n",
            "    applications. An example is listed in the \"Data Considerations\" section\n",
            "    above. The challenge with using current text datasets in NLP for tasks such\n",
            "    as sentence completion, clustering, and other applications is that text\n",
            "    that is culturally biased and inflammatory will propagate biases. This\n",
            "    should be taken into consideration when using the dataset, reviewing the\n",
            "    output, and the bias should be documented.\n",
            "\n",
            ".. topic:: Examples\n",
            "\n",
            "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
            "\n",
            "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 타겟 클래스를 살펴보겠습니다."
      ],
      "metadata": {
        "id": "OeNy6IP8eGmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "print('target 클래스의 값과 분포도\\n', pd.Series(news_data.target).value_counts().sort_index())\n",
        "print()\n",
        "print('target 클래스의 이름\\n', news_data.target_names)\n",
        "print(len(news_data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4ZhOWAWWaIC",
        "outputId": "9fda2104-306f-4105-a976-b2c486a2b433"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target 클래스의 값과 분포도\n",
            " 0     799\n",
            "1     973\n",
            "2     985\n",
            "3     982\n",
            "4     963\n",
            "5     988\n",
            "6     975\n",
            "7     990\n",
            "8     996\n",
            "9     994\n",
            "10    999\n",
            "11    991\n",
            "12    984\n",
            "13    990\n",
            "14    987\n",
            "15    997\n",
            "16    910\n",
            "17    940\n",
            "18    775\n",
            "19    628\n",
            "dtype: int64\n",
            "\n",
            "target 클래스의 이름\n",
            " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "총 20개의 타겟 클래스를 가졌으며 각 클래스의 값 분포는 몇 클래스를 제외하면 고르게 분포합니다.\n",
        "\n",
        "데이터를 하나만 살펴보겠습니다."
      ],
      "metadata": {
        "id": "ujP2TVqAdZ44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_data.data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plRcujeTw3zN",
        "outputId": "e6978138-8964-438f-9ff7-9441e6f5ad28"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
            "Subject: Re: Observation re: helmets\n",
            "Organization: Sun Microsystems, RTP, NC\n",
            "Lines: 21\n",
            "Distribution: world\n",
            "Reply-To: egreen@east.sun.com\n",
            "NNTP-Posting-Host: laser.east.sun.com\n",
            "\n",
            "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
            "> \n",
            "> The question for the day is re: passenger helmets, if you don't know for \n",
            ">certain who's gonna ride with you (like say you meet them at a .... church \n",
            ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
            ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
            ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
            ">passenger? \n",
            "\n",
            "If your primary concern is protecting the passenger in the event of a\n",
            "crash, have him or her fitted for a helmet that is their size.  If your\n",
            "primary concern is complying with stupid helmet laws, carry a real big\n",
            "spare (you can put a big or small head in a big helmet, but not in a\n",
            "small one).\n",
            "\n",
            "---\n",
            "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
            "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
            "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
            " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기사 내용뿐만 아니라 제목, 작성자, 소속, 이메일 등 정보를 모두 갖고 있습니다. 모든 정보를 사용하면 어떤 머신러닝 알고리즘을 적용해도 높은 예측 성능을 발휘하게 되므로 내용을 제외한 모든 정보를 삭제하겠습니다."
      ],
      "metadata": {
        "id": "VdauSbUddd_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "train_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
        "                   random_state=156)\n",
        "X_train = train_news.data\n",
        "y_train = train_news.target\n",
        "\n",
        "test_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),\n",
        "                               random_state=156)\n",
        "X_test = test_news.data\n",
        "y_test = test_news.target\n",
        "\n",
        "print('학습 데이터 크기:', len(train_news.data))\n",
        "print('테스트 데이터 크기:', len(test_news.data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65At5_tnfyHn",
        "outputId": "1dfebf53-f77e-45a8-c48f-a2886b0536f1"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터 크기: 11314\n",
            "테스트 데이터 크기: 7532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_news.data[0])\n",
        "print()\n",
        "print(test_news.data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTIU3NoGiZkt",
        "outputId": "39e64a66-d5fc-4158-924e-93d27b564bbb"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "What I did NOT get with my drive (CD300i) is the System Install CD you\n",
            "listed as #1.  Any ideas about how I can get one?  I bought my IIvx 8/120\n",
            "from Direct Express in Chicago (no complaints at all -- good price & good\n",
            "service).\n",
            "\n",
            "BTW, I've heard that the System Install CD can be used to boot the mac;\n",
            "however, my drive will NOT accept a CD caddy is the machine is off.  How can\n",
            "you boot with it then?\n",
            "\n",
            "--Dave\n",
            "\n",
            "\n",
            "The tech support line for GCC is 1-800-231-1570.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2. 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가**"
      ],
      "metadata": {
        "id": "LRGL_QovlIUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CountVectorizer` 클래스를 사용하여 학습 데이터와 테스트 데이터의 텍스트를 피처 벡터화하겠습니다."
      ],
      "metadata": {
        "id": "WaCEHdFkjN1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cnt_vect = CountVectorizer()\n",
        "cnt_vect.fit(X_train)\n",
        "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터의 피처 개수를 같게 만들기 위해\n",
        "# 학습 데이터로 `fit()`을 적용한 인스턴스를 사용하여 테스트 데이터를 변환합니다.\n",
        "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
        "\n",
        "print('학습 데이터 텍스트의 CountVectorizer Shape:', X_train_cnt_vect.shape)\n",
        "print('테스트 데이터 텍스트의 CountVectorizer Shape:', X_test_cnt_vect.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEbkZzkskPrt",
        "outputId": "b66fd067-27b8-42a6-be09-39af5ec7848a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터 텍스트의 CountVectorizer Shape: (11314, 101631)\n",
            "테스트 데이터 텍스트의 CountVectorizer Shape: (7532, 101631)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 데이터 세트에 각각 같은 단어수인 101,631개가 피처로 생성되었습니다.\n",
        "\n",
        "로지스틱 회귀를 적용하여 뉴스그룹을 예측해 보겠습니다."
      ],
      "metadata": {
        "id": "F163TDu-oCH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_cnt_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_cnt_vect)\n",
        "\n",
        "print('CountVectorized Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCqOVEOXoVP4",
        "outputId": "29851e63-507a-42e4-e6b7-258063857088"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorized Logistic Regression의 예측 정확도: 0.6076739245884227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 `TfidfVectorizer` 클래스를 사용하여 피처 벡터화를 수행하고 로지스틱 회귀를 적용해 보겠습니다."
      ],
      "metadata": {
        "id": "I7djWXpEqG4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "\n",
        "print('TF-IDF Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2hfEyVHpQyt",
        "outputId": "f833f895-8118-4dfd-92ef-e1d573f75dcb"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Logistic Regression의 예측 정확도: 0.6736590546999469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF 기반 예측 정확도가 더 높습니다. 다량의 문서와 텍스트는 TF-IDF 기반 피처 벡터화가 더 좋은 예측 결과를 도출합니다.\n",
        "\n",
        "TA를 수행할 때 머신러닝 모델 성능을 제고하려면 최적 알고리즘을 선택하고 피처 전처리를 완벽하게 수행해야 합니다. 이번에는 다양한 파라미터를 지정하여 예측 성능을 제고해 보겠습니다."
      ],
      "metadata": {
        "id": "Hr2LKewWqVBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "\n",
        "print('TF-IDF Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1WEO3wIr5q_",
        "outputId": "23fcfb50-52f4-4910-9ea7-20d3b27bbefb"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Logistic Regression의 예측 정확도: 0.6922464152947424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "성능이 약간 향상되었습니다.\n",
        "\n",
        "그리드 서치를 활용하여 로지스틱 회귀의 최적 파라미터를 탐색(`C`만 탐색 시도)한 뒤 모델의 예측 성능을 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "YLfgXzwuurEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'C': [0.01, 0.1, 1, 5, 10]}\n",
        "grid_cv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
        "grid_cv_lr.fit(X_train_tfidf_vect, y_train)\n",
        "print('Logistic Regression best C parameter:', grid_cv_lr.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3ukQiV5tQI8",
        "outputId": "c9006ab7-e8a6-4eb1-a01f-92fed295954d"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression best C parameter: {'C': 10}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
        "print('TF-IDF Vectorized Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81M4CkRVuULG",
        "outputId": "22060ea0-1214-4fa0-8ad6-e8c2c17557d8"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vectorized Logistic Regression의 예측 정확도: 0.7010090281465746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "성능이 약간 더 향상되었습니다."
      ],
      "metadata": {
        "id": "mcialUCrwER-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3. 사이킷런 파이프라인(Pipeline) 사용 및 GridSearchCV와의 결합**"
      ],
      "metadata": {
        "id": "kD55vf6YlIRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 과정을 파이프라인으로 구현해 보겠습니다. 먼저 그리드 서치를 결합하지 않은 형태를 구현하겠습니다."
      ],
      "metadata": {
        "id": "N3wsZsIV7be3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "                     ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)),\n",
        "                     ('lr_clf', LogisticRegression(C=10))\n",
        "                     ])\n",
        "pipeline.fit(X_train, y_train)\n",
        "pred = pipeline.predict(X_test)\n",
        "\n",
        "print('Pipeline을 통한 Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JydnvNt05jre",
        "outputId": "aedf5d9d-d60d-4c02-f3c6-c05c401ec1a0"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline을 통한 Logistic Regression의 예측 정확도: 0.7010090281465746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "고민 중\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "E1r1NFS6AUiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. 감성 분석**"
      ],
      "metadata": {
        "id": "YB6lg40Pm20l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.1. 감성 분석 소개**"
      ],
      "metadata": {
        "id": "MAnaF6fVm2zN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.2. 지도학습 기반 감성 분석 실습 - IMDB 영화평**"
      ],
      "metadata": {
        "id": "M7gaY_aMm2w5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.3. 비지도학습 기반 감성 분석 소개**"
      ],
      "metadata": {
        "id": "qN-cT4KLm2uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.4. SentiWordNet을 이용한 감성 분석**"
      ],
      "metadata": {
        "id": "F2st4Z0-m2sV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4.1. WordNet Synset과 SentiWordNet SentiSynset 클래스의 이해**"
      ],
      "metadata": {
        "id": "4qT6GGiOm2pB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4.2. SentiWordNet을 이용한 영화 감상평 감성 분석**"
      ],
      "metadata": {
        "id": "k6yqLAJ5m2mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.5. VADER를 이용한 감성 분석**"
      ],
      "metadata": {
        "id": "f17w6YaYm2lH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. 토픽 모델링(Topic Modeling) - 20 뉴스그룹**"
      ],
      "metadata": {
        "id": "7xh-YI-0m2jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. 문서 군집화 소개와 실습(Opinion Review 데이터 세트)**"
      ],
      "metadata": {
        "id": "_R4yoUPwm2ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.1. 문서 군집화 개념**"
      ],
      "metadata": {
        "id": "nzRQflwLm2eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.2. Opinion Review 데이터 세트를 이용한 문서 군집화 수행하기**"
      ],
      "metadata": {
        "id": "U84ogXlInWQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.3. 군집별 핵심 단어 추출하기**"
      ],
      "metadata": {
        "id": "_ZYxqdDcnXIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. 문서 유사도**"
      ],
      "metadata": {
        "id": "66m82F1enXER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.1. 문서 유사도 측정 방법 - 코사인 유사도**"
      ],
      "metadata": {
        "id": "vpICe-EXnXCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.2. 두 벡터 사잇각**"
      ],
      "metadata": {
        "id": "s3OSzKYFnW_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3. Opinion Review 데이터 세트를 이용한 문서 유사도 측정**"
      ],
      "metadata": {
        "id": "vKmMJXmknW8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. 한글 텍스트 처리 - 네이버 영화 평점 감성 분석**"
      ],
      "metadata": {
        "id": "OdjM_H4InWnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1. 한글 NLP 처리의 어려움**"
      ],
      "metadata": {
        "id": "RbjICpkboprM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.2. KoNLPy 소개**"
      ],
      "metadata": {
        "id": "Z2t-D6c1opoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.3. 데이터 로딩**"
      ],
      "metadata": {
        "id": "sMa2Gi4Sopkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. 텍스트 분석 실습 - 캐글 Mercari Price Suggestion Challenge**"
      ],
      "metadata": {
        "id": "JgatqQtLophs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.1. 데이터 전처리**"
      ],
      "metadata": {
        "id": "5o6HaNuGpg3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.2. 피처 인코딩과 피처 벡터화**"
      ],
      "metadata": {
        "id": "Q1-_U6j0pg1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.3. 릿지 회귀 모델 구축 및 평가**"
      ],
      "metadata": {
        "id": "9VmXSu0FpgyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.4. LightGBM 회귀 모델 구축과 앙상블을 이용한 최종 예측 평가**"
      ],
      "metadata": {
        "id": "w5yvI_x-pgv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. 정리**"
      ],
      "metadata": {
        "id": "p347TSeNpgtR"
      }
    }
  ]
}