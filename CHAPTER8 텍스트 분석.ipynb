{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CHAPTER8 텍스트 분석.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNZq0DO/X/rVPso2KX4g43Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeSeungwon89/Machine-learning_Theory/blob/master/CHAPTER8%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP이냐 텍스트 분석이냐**"
      ],
      "metadata": {
        "id": "TxMLRcI2lI8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "기실 NLP와 텍스트 분석을 구분하는 것은 큰 의미가 없지만 굳이 나눈다면 아래와 같습니다.\n",
        "\n",
        "**NLP(National Language Processing, 자연어 처리)**는 머신이 인간의 언어를 이해하고 해석하는 데 더 중점을 두고 발전하고 있습니다. NLP의 영역은 언어를 해석하기 위한 기계 번역, 자동으로 질문을 해석하고 답을 해주는 질의응답 시스템 등입니다.\n",
        "\n",
        "**TA(Text Analytics, Text Mining, 텍스트 분석)**은 비정형 텍스트에서 의미 있는 정보를 추출하는 데 중점을 두고 발전하고 있습니다. NLP는 TA의 기반 기술이며, NLP가 발전하면서 TA도 더 정교하게 발전했습니다. \n",
        "\n",
        "머신러닝 기술은 NLP와 TA의 발전에 크게 기여했습니다. 예전의 텍스트를 구성하는 언어적인 룰이나 업무의 룰에 따라 텍스트를 분석하는 룰 기반 시스템에서 머신러닝의 텍스트 데이터를 기반으로 모델을 학습하고 예측하는 기반으로 변경되면서 기술이 비약적으로 발전한 것입니다.\n",
        "\n",
        "TA는 머신러닝, 언어 이해, 통계 등을 확용하여 모델을 수립하고 정보를 추출하여 비즈니스 인텔리전스나 예측 분석 등 분석 작업을 주로 수행합니다. 주로 아래와 같은 기술 영역에 집중합니다.\n",
        "\n",
        "- **텍스트 분류(Text Classification, Text Categorization)**: 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법을 통칭합니다. 지도학습을 적용하며, 신문 기사가 특정 카테고리에 속하는지 자동으로 분류하거나 스팸 메일을 검출하는 기능을 예로 들 수 있습니다. \n",
        "\n",
        "- **감성 분석(Sentiment Analysis)**: 텍스트에서 나타나는 주관적인 요소를 분석하는 기법을 총칭합니다. 지도학습과 비지도학습을 모두 적용할 수 있으며, 소셜 미디어 감정 분석, 영화나 제품에 대한 리뷰, 여론조사 의견 분석 등을 예로 들 수 있습니다. \n",
        "\n",
        "- **텍스트 요약(Summarization)**: 텍스트 내에서 중요한 주제나 중심 사상을 추출하는 기법을 의미합니다. 토픽 모델링(Topic Modeling)을 예로 들 수 있습니다.\n",
        "\n",
        "- **텍스트 군집화**와 **유사도 측정**: 비슷한 유형의 문서를 군집화하는 기법을 의미합니다. 텍스트 분류를 비지도학습으로 수행하는 방법의 일환으로 사용될 수 있습니다. 유사도 측정 역시 문서들 간 유사도를 측정하여 비슷한 문서끼리 모을 수 있는 기법입니다."
      ],
      "metadata": {
        "id": "EdvCmmfmQe6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 텍스트 분석 이해**"
      ],
      "metadata": {
        "id": "7SFqUTDUlI5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지 ML 모델은 정형 데이터 기반에서 모델을 수립하고 예측을 수행했습니다. 그리고 머신러닝 알고리즘은 숫자형 피처 기반 데이터만 입력받을 수 있으므로 텍스트를 머신러닝에 적용하려면 비정형 텍스트 데이터를 피처 형태로 추출하고 추출된 피처에 의미 있는 값을 부여하는 것이 중요합니다. 텍스트를 단어 기반(또는 단어 일부분)의 다수 피처로 추출하고 이 피처에 단어 빈도수 같은 숫자 값을 부여하면 텍스트는 단어 조합인 벡터값으로 표현될 수 있습니다. 이렇게 텍스트를 변환하는 것을 피처 벡터화(Feature Vectorization) 또는 피처 추출(Feature Extraction)이라고 합니다. 텍스트를 피처 벡터화 하여 변환하는 방법은 **BOW(Bag of Words)**와 **Word2Vec**가 대표적입니다.\n"
      ],
      "metadata": {
        "id": "gttzXbdz1W-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1. 텍스트 분석 수행 프로세스**"
      ],
      "metadata": {
        "id": "Zra4U2lflI3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TA는 아래와 같은 프로세스 순으로 수행합니다.\n",
        "\n",
        "1. 텍스트 사전 준비(텍스트 전처리): 클렌징, 대/소문자 변경, 특수문자 삭제, 단어 등의 토큰화, 무의미한 단어(Stop word) 제거, 어근 추출(Stemming/Lemmatization) 등 텍스트 정규화 작업을 수행합니다.\n",
        "\n",
        "1. 피처 벡터화 및 추출: 전처리로 가공된 텍스트에서 피처를 추출하고 벡터 값을 할당합니다. BOW와 Word2Vec가 대표적인 방법입니다. BOW는 Count 기반과 TF-IDF 기반 벡터화를 수행합니다. \n",
        "\n",
        "1. ML 모델 수립 및 학습, 예측, 평가: 피처 벡터화가 수행된 데이터 세트에 ML 모델을 적용하여 학습, 예측, 평가를 수행합니다.\n"
      ],
      "metadata": {
        "id": "C2MQRdd4YomJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. 파이썬 기반의 NLP, 텍스트 분석 패키지**"
      ],
      "metadata": {
        "id": "Ss_Da4BXlI1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "대표적으로 사용되는 패키지는 아래와 같습니다.\n",
        "\n",
        "- **NLTK(Natural Language Toolkit for Python)**: 파이썬의 가장 대표적인 NLP 패키지입니다. 방대한 데이터 세트와 서브 모듈을 가졌으며 NLP의 대부분 영역을 커버합니다. 수많은 NLP 패키지가 NLTK의 영향을 받아 작성되고 있습니다. 느린 수행 속도로 인해 대량 데이터 기반에서는 활용되지 않습니다. 아울러 수행 성능, 정확도, 신기술, 엔터프라이즈 기능 지원 등 전반적인 측면이 부족하기 때문에 실무에서 잘 활용되지 않습니다.\n",
        "\n",
        "- **Genism**: 토픽 모델링 분야에서 가장 두각을 보이며 많이 사용되는 패키지입니다. 토픽 모델링을 쉽게 구현하는 기능을 지원하며, Word2Vec 구현 등 다양한 신기능도 지원합니다.\n",
        "\n",
        "- **SpaCy**: 수행 성능이 매우 뛰어나 최근들어 주목 받는 패키지입니다. NLP 애플리케이션에서 사용하는 사례가 느는 추세입니다.\n",
        "\n",
        "참고로 사이킷런은 머신러닝 위주의 라이브러리입니다. 따라서 NLP를 위한 라이브러리, 예컨대 '어근 처리'를 수행하는 것처럼 NLP 패키지에 특화된 라이브러리는 없습니다. 다만 텍스트를 일정 수준으로 가공하고 머신러닝 알고리즘에 텍스트 데이터를 피처로 처리하기 위한 편리한 기능을 제공하므로 충분히 TA를 수행할 수 있습니다. 더 다양한 TA가 적용되어야 할 경우에는 위에서 설명한 패키지를 결합하여 애플리케이션을 작성합니다."
      ],
      "metadata": {
        "id": "_zTCuhLuaNko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화**"
      ],
      "metadata": {
        "id": "9OznRAtklIzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트를 바로 피처화할 수 없으므로 데이터 전처리 과정을 거쳐야 합니다. 텍스트 정규화는 텍스트를 머신러닝 알고리즘이나 NLP 애플리케이션에 입력 데이터로 사용할 목적으로 클렌징, 정제, 토큰화, 어근화 등 사전 작업을 수행하는 작업입니다. \n"
      ],
      "metadata": {
        "id": "UKxWRQzpkqoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1. 클렌징(Cleansing)**"
      ],
      "metadata": {
        "id": "e6bTR_EVlpht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TA를 수행하면서 불필요하거나 방해 요소로 작용할 만한 문자와 기호(예컨대 HTML, XML 태그나 특정 기호) 등을 사전에 제거하는 작업입니다. "
      ],
      "metadata": {
        "id": "JpwKLvkdlmIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2. 텍스트 토큰화(Tokenization)**"
      ],
      "metadata": {
        "id": "60OE02wklIxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 토큰화 유형은 문서에서 문장을 분리하는 문장 토큰화, 문장에서 단어를 토큰으로 분리하는 단어 토큰화로 나뉩니다. "
      ],
      "metadata": {
        "id": "HDui4Tosl8jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.1. 문장 토큰화** "
      ],
      "metadata": {
        "id": "ycsjIa34lIus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 토큰화는 문장의 마침표나 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적입니다. 아울러 정규 표현식에 따른 문장 토큰화도 가능합니다. NTLK에서 일반적으로 많이 쓰이는 `sent_tokenize` 클래스로 토큰화해 보겠습니다. 샘플 텍스트 문서를 문장 3개로 분리하는 작업입니다."
      ],
      "metadata": {
        "id": "PsoGPM1gmJNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "# 마침표, 개행 문자 등의 데이터 세트를 다운로드합니다.\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "You can see it out your window or on your television. \\\n",
        "You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences), len(sentences))\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcSnOsoJKELE",
        "outputId": "e36ec97a-dd69-4340-f868-e1aecfa748fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "리스트 객체에 문장 3개가 담겨 있습니다."
      ],
      "metadata": {
        "id": "YfZeTfK-LnKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2.2. 단어 토큰화**"
      ],
      "metadata": {
        "id": "YBzBF5QTlIsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 토큰화는 문장을 단어로 토큰화하는 작업입니다. 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리하며, 정규 표현식을 이용하여 다양한 유형으로도 토큰화할 수 있습니다. 마침표나 개행문자처럼 문장을 분리하는 구분자를 이용하여 단어를 토큰화할 수 있으므로 Bag of Word처럼 단어 순서가 중요하지 않으면 문장 토큰화를 사용하지 않고 단어 토큰화만 사용해도 충분합니다. 일반적으로 문장 토큰화는 각 문장이 가지는 의미가 중요한 요소로 사용될 때 사용합니다. NLTK의 `word_tokenize()` 클래스를 활용해 보겠습니다."
      ],
      "metadata": {
        "id": "5xy1fqyeL4lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0bo_zSBMoER",
        "outputId": "8cf0fadf-c72a-4395-8d9f-7f8d8182a53e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 토큰화와 단어 토큰화를 동시에 수행해 보겠습니다. 먼저 여러 문장을 문장별로 단어 토큰화 하는 함수를 선언하겠습니다."
      ],
      "metadata": {
        "id": "weP3lTu-NQcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return word_tokens"
      ],
      "metadata": {
        "id": "RjkrtdV8NYm4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수를 사용하여 두 작업을 한 번에 수행해 보겠습니다."
      ],
      "metadata": {
        "id": "2JIKFdhiOxKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "You can see it out your window or on your television. \\\n",
        "You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens), len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBmgvq5YOUOl",
        "outputId": "053ef058-ec74-44cf-d931-11f32f4d1abb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장을 단어별로 토큰화하면 문맥 의미를 잃게 됩니다. 이 문제를 다소 해결하는 방안으로 [n-gram](https://wikidocs.net/21692)을 사용합니다. n-gram은 연속된 n개의 단어를 하나의 토큰화 단위로 분리합니다. n개 단어 크기 윈도우를 만들어 문장 처음부터 오른쪽으로 움직이면서 토큰화합니다. 예컨대 'Agent Smith knocks the door'를 2-gram(bigram)으로 만들면 (Agent, Smith), (knocks, the), (the, door)와 같이 연속적으로 단어 2개들을 차례로 이동하면서 토큰화합니다."
      ],
      "metadata": {
        "id": "YCiOl9pQOu_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3. 스톱 워드(Stop Word) 제거**"
      ],
      "metadata": {
        "id": "sKnfqCTulIqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "고민 중\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-an7PdvhP_Cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4. Stemming과 Lemmatization**"
      ],
      "metadata": {
        "id": "FwfF0wNIlIoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Bag of Words - BOW**"
      ],
      "metadata": {
        "id": "WBs3j00flIl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1. BOW 피처 벡터화**"
      ],
      "metadata": {
        "id": "ceqQYcX0lIj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2. 사이킷런의 Count 및 TF-IDF 벡터화 구현: CountVectorizer, TfidfVectorizer**"
      ],
      "metadata": {
        "id": "brJCR7ozlIho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3. BOW 벡터화를 위한 희소 행렬**"
      ],
      "metadata": {
        "id": "qe1Qyw3_lIfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4. 희소 행렬 - COO 형식**"
      ],
      "metadata": {
        "id": "MM3suxkZlIdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.5. 희소 행렬 - CSR 형식**"
      ],
      "metadata": {
        "id": "DGQLXJ1QlIbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. 텍스트 분류 실습 - 20 뉴스그룹 분류**"
      ],
      "metadata": {
        "id": "qktTPqy_lIY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1. 텍스트 정규화**"
      ],
      "metadata": {
        "id": "fI4ltzX4lIWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2. 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가**"
      ],
      "metadata": {
        "id": "LRGL_QovlIUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3. 사이킷런 파이프라인(Pipeline) 사용 및 GridSearchCV와의 결합**"
      ],
      "metadata": {
        "id": "kD55vf6YlIRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. 감성 분석**"
      ],
      "metadata": {
        "id": "YB6lg40Pm20l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.1. 감성 분석 소개**"
      ],
      "metadata": {
        "id": "MAnaF6fVm2zN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.2. 지도학습 기반 감성 분석 실습 - IMDB 영화평**"
      ],
      "metadata": {
        "id": "M7gaY_aMm2w5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.3. 비지도학습 기반 감성 분석 소개**"
      ],
      "metadata": {
        "id": "qN-cT4KLm2uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.4. SentiWirdNet을 이용한 감성 분석**"
      ],
      "metadata": {
        "id": "F2st4Z0-m2sV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4.1. WordNet Synset과 SentiWordNet SentiSynset 클래스의 이해**"
      ],
      "metadata": {
        "id": "4qT6GGiOm2pB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4.2. SentiWordNet을 이용한 영화 감상평 감성 분석**"
      ],
      "metadata": {
        "id": "k6yqLAJ5m2mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.5. VADER를 이용한 감성 분석**"
      ],
      "metadata": {
        "id": "f17w6YaYm2lH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. 토픽 모델링(Topic Modeling) - 20 뉴스그룹**"
      ],
      "metadata": {
        "id": "7xh-YI-0m2jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. 문서 군집화 소개와 실습(Opinion Review 데이터 세트)**"
      ],
      "metadata": {
        "id": "_R4yoUPwm2ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.1. 문서 군집화 개념**"
      ],
      "metadata": {
        "id": "nzRQflwLm2eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.2. Opinion Review 데이터 세트를 이용한 문서 군집화 수행하기**"
      ],
      "metadata": {
        "id": "U84ogXlInWQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.3. 군집별 핵심 단어 추출하기**"
      ],
      "metadata": {
        "id": "_ZYxqdDcnXIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. 문서 유사도**"
      ],
      "metadata": {
        "id": "66m82F1enXER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.1. 문서 유사도 측정 방법 - 코사인 유사도**"
      ],
      "metadata": {
        "id": "vpICe-EXnXCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.2. 두 벡터 사잇각**"
      ],
      "metadata": {
        "id": "s3OSzKYFnW_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.3. Opinion Review 데이터 세트를 이용한 문서 유사도 측정**"
      ],
      "metadata": {
        "id": "vKmMJXmknW8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. 한글 텍스트 처리 - 네이버 영화 평점 감성 분석**"
      ],
      "metadata": {
        "id": "OdjM_H4InWnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1. 한글 NLP 처리의 어려움**"
      ],
      "metadata": {
        "id": "RbjICpkboprM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.2. KoNLPy 소개**"
      ],
      "metadata": {
        "id": "Z2t-D6c1opoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.3. 데이터 로딩**"
      ],
      "metadata": {
        "id": "sMa2Gi4Sopkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. 텍스트 분석 실습 - 캐글 Mercari Price Suggestion Challenge**"
      ],
      "metadata": {
        "id": "JgatqQtLophs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.1. 데이터 전처리**"
      ],
      "metadata": {
        "id": "5o6HaNuGpg3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.2. 피처 인코딩과 피처 벡터화**"
      ],
      "metadata": {
        "id": "Q1-_U6j0pg1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.3. 릿지 회귀 모델 구축 및 평가**"
      ],
      "metadata": {
        "id": "9VmXSu0FpgyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.4. LightGBM 회귀 모델 구축과 앙상블을 이용한 최종 예측 평가**"
      ],
      "metadata": {
        "id": "w5yvI_x-pgv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. 정리**"
      ],
      "metadata": {
        "id": "p347TSeNpgtR"
      }
    }
  ]
}