{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeSeungwon89/Machine-learning_Theory/blob/master/CHAPTER8%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxMLRcI2lI8U"
      },
      "source": [
        "# **NLP이냐 텍스트 분석이냐**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvCmmfmQe6Q"
      },
      "source": [
        "기실 NLP와 텍스트 분석을 구분하는 것은 큰 의미가 없지만 굳이 나눈다면 아래와 같습니다.\n",
        "\n",
        "**NLP(National Language Processing, 자연어 처리)**는 머신이 인간의 언어를 이해하고 해석하는 데 더 중점을 두고 발전하고 있습니다. NLP의 영역은 언어를 해석하기 위한 기계 번역, 자동으로 질문을 해석하고 답을 해주는 질의응답 시스템 등입니다.\n",
        "\n",
        "**TA(Text Analytics, Text Mining, 텍스트 분석)**은 비정형 텍스트에서 의미 있는 정보를 추출하는 데 중점을 두고 발전하고 있습니다. NLP는 TA의 기반 기술이며, NLP가 발전하면서 TA도 더 정교하게 발전했습니다. \n",
        "\n",
        "머신러닝 기술은 NLP와 TA의 발전에 크게 기여했습니다. 예전의 텍스트를 구성하는 언어적인 룰이나 업무의 룰에 따라 텍스트를 분석하는 룰 기반 시스템에서 머신러닝의 텍스트 데이터를 기반으로 모델을 학습하고 예측하는 기반으로 변경되면서 기술이 비약적으로 발전한 것입니다.\n",
        "\n",
        "TA는 머신러닝, 언어 이해, 통계 등을 확용하여 모델을 수립하고 정보를 추출하여 비즈니스 인텔리전스나 예측 분석 등 분석 작업을 주로 수행합니다. 주로 아래와 같은 기술 영역에 집중합니다.\n",
        "\n",
        "- **텍스트 분류(Text Classification, Text Categorization)**: 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법을 통칭합니다. 지도학습을 적용하며, 신문 기사가 특정 카테고리에 속하는지 자동으로 분류하거나 스팸 메일을 검출하는 기능을 예로 들 수 있습니다. \n",
        "\n",
        "- **감성 분석(Sentiment Analysis)**: 텍스트에서 나타나는 주관적인 요소를 분석하는 기법을 총칭합니다. 지도학습과 비지도학습을 모두 적용할 수 있으며, 소셜 미디어 감정 분석, 영화나 제품에 대한 리뷰, 여론조사 의견 분석 등을 예로 들 수 있습니다. \n",
        "\n",
        "- **텍스트 요약(Summarization)**: 텍스트 내에서 중요한 주제나 중심 사상을 추출하는 기법을 의미합니다. 토픽 모델링(Topic Modeling)을 예로 들 수 있습니다.\n",
        "\n",
        "- **텍스트 군집화**와 **유사도 측정**: 비슷한 유형의 문서를 군집화하는 기법을 의미합니다. 텍스트 분류를 비지도학습으로 수행하는 방법의 일환으로 사용될 수 있습니다. 유사도 측정 역시 문서들 간 유사도를 측정하여 비슷한 문서끼리 모을 수 있는 기법입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SFqUTDUlI5_"
      },
      "source": [
        "# **1. 텍스트 분석 이해**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gttzXbdz1W-G"
      },
      "source": [
        "지금까지 ML 모델은 정형 데이터 기반에서 모델을 수립하고 예측을 수행했습니다. 그리고 머신러닝 알고리즘은 숫자형 피처 기반 데이터만 입력받을 수 있으므로 텍스트를 머신러닝에 적용하려면 비정형 텍스트 데이터를 피처 형태로 추출하고 추출된 피처에 의미 있는 값을 부여하는 것이 중요합니다. 텍스트를 단어 기반(또는 단어 일부분)의 다수 피처로 추출하고 이 피처에 단어 빈도수 같은 숫자 값을 부여하면 텍스트는 단어 조합인 벡터값으로 표현될 수 있습니다. 이렇게 텍스트를 변환하는 것을 피처 벡터화(Feature Vectorization) 또는 피처 추출(Feature Extraction)이라고 합니다. 텍스트를 피처 벡터화 하여 변환하는 방법은 **BOW(Bag of Words)**와 **Word2Vec**가 대표적입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zra4U2lflI3q"
      },
      "source": [
        "## **1.1. 텍스트 분석 수행 프로세스**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2MQRdd4YomJ"
      },
      "source": [
        "TA는 아래와 같은 프로세스 순으로 수행합니다.\n",
        "\n",
        "1. 텍스트 사전 준비(텍스트 전처리): 클렌징, 대/소문자 변경, 특수문자 삭제, 단어 등의 토큰화, 무의미한 단어(Stop word) 제거, 어근 추출(Stemming/Lemmatization) 등 텍스트 정규화 작업을 수행합니다.\n",
        "\n",
        "1. 피처 벡터화 및 추출: 전처리로 가공된 텍스트에서 피처를 추출하고 벡터 값을 할당합니다. BOW와 Word2Vec가 대표적인 방법입니다. BOW는 Count 기반과 TF-IDF 기반 벡터화를 수행합니다. \n",
        "\n",
        "1. ML 모델 수립 및 학습, 예측, 평가: 피처 벡터화가 수행된 데이터 세트에 ML 모델을 적용하여 학습, 예측, 평가를 수행합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss_Da4BXlI1Z"
      },
      "source": [
        "## **1.2. 파이썬 기반의 NLP, 텍스트 분석 패키지**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zTCuhLuaNko"
      },
      "source": [
        "대표적으로 사용되는 패키지는 아래와 같습니다.\n",
        "\n",
        "- **NLTK(Natural Language Toolkit for Python)**: 파이썬의 가장 대표적인 NLP 패키지입니다. 방대한 데이터 세트와 서브 모듈을 가졌으며 NLP의 대부분 영역을 커버합니다. 수많은 NLP 패키지가 NLTK의 영향을 받아 작성되고 있습니다. 느린 수행 속도로 인해 대량 데이터 기반에서는 활용되지 않습니다. 아울러 수행 성능, 정확도, 신기술, 엔터프라이즈 기능 지원 등 전반적인 측면이 부족하기 때문에 실무에서 잘 활용되지 않습니다.\n",
        "\n",
        "- **Genism**: 토픽 모델링 분야에서 가장 두각을 보이며 많이 사용되는 패키지입니다. 토픽 모델링을 쉽게 구현하는 기능을 지원하며, Word2Vec 구현 등 다양한 신기능도 지원합니다.\n",
        "\n",
        "- **SpaCy**: 수행 성능이 매우 뛰어나 최근들어 주목 받는 패키지입니다. NLP 애플리케이션에서 사용하는 사례가 느는 추세입니다.\n",
        "\n",
        "참고로 사이킷런은 머신러닝 위주의 라이브러리입니다. 따라서 NLP를 위한 라이브러리, 예컨대 '어근 처리'를 수행하는 것처럼 NLP 패키지에 특화된 라이브러리는 없습니다. 다만 텍스트를 일정 수준으로 가공하고 머신러닝 알고리즘에 텍스트 데이터를 피처로 처리하기 위한 편리한 기능을 제공하므로 충분히 TA를 수행할 수 있습니다. 더 다양한 TA가 적용되어야 할 경우에는 위에서 설명한 패키지를 결합하여 애플리케이션을 작성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OznRAtklIzS"
      },
      "source": [
        "# **2. 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKxWRQzpkqoa"
      },
      "source": [
        "텍스트를 바로 피처화할 수 없으므로 데이터 전처리 과정을 거쳐야 합니다. 텍스트 정규화는 텍스트를 머신러닝 알고리즘이나 NLP 애플리케이션에 입력 데이터로 사용할 목적으로 클렌징, 정제, 토큰화, 어근화 등 사전 작업을 수행하는 작업입니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6bTR_EVlpht"
      },
      "source": [
        "## **2.1. 클렌징(Cleansing)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpwKLvkdlmIf"
      },
      "source": [
        "TA를 수행하면서 불필요하거나 방해 요소로 작용할 만한 문자와 기호(예컨대 HTML, XML 태그나 특정 기호) 등을 사전에 제거하는 작업입니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60OE02wklIxG"
      },
      "source": [
        "## **2.2. 텍스트 토큰화(Tokenization)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDui4Tosl8jy"
      },
      "source": [
        "텍스트 토큰화 유형은 문서에서 문장을 분리하는 문장 토큰화, 문장에서 단어를 토큰으로 분리하는 단어 토큰화로 나뉩니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycsjIa34lIus"
      },
      "source": [
        "### **2.2.1. 문장 토큰화** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsoGPM1gmJNl"
      },
      "source": [
        "문장 토큰화는 문장의 마침표나 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적입니다. 아울러 정규 표현식에 따른 문장 토큰화도 가능합니다. NTLK에서 일반적으로 많이 쓰이는 `sent_tokenize` 클래스로 토큰화해 보겠습니다. 샘플 텍스트 문서를 문장 3개로 분리하는 작업입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcSnOsoJKELE",
        "outputId": "e36ec97a-dd69-4340-f868-e1aecfa748fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        }
      ],
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "# 마침표, 개행 문자 등의 데이터 세트를 다운로드합니다.\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "You can see it out your window or on your television. \\\n",
        "You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences), len(sentences))\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfZeTfK-LnKa"
      },
      "source": [
        "리스트 객체에 문장 3개가 담겨 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBzBF5QTlIsg"
      },
      "source": [
        "### **2.2.2. 단어 토큰화**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xy1fqyeL4lR"
      },
      "source": [
        "단어 토큰화는 문장을 단어로 토큰화하는 작업입니다. 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리하며, 정규 표현식을 이용하여 다양한 유형으로도 토큰화할 수 있습니다. 마침표나 개행문자처럼 문장을 분리하는 구분자를 이용하여 단어를 토큰화할 수 있으므로 Bag of Word처럼 단어 순서가 중요하지 않으면 문장 토큰화를 사용하지 않고 단어 토큰화만 사용해도 충분합니다. 일반적으로 문장 토큰화는 각 문장이 가지는 의미가 중요한 요소로 사용될 때 사용합니다. NLTK의 `word_tokenize()` 클래스를 활용해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0bo_zSBMoER",
        "outputId": "8cf0fadf-c72a-4395-8d9f-7f8d8182a53e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weP3lTu-NQcb"
      },
      "source": [
        "문장 토큰화와 단어 토큰화를 동시에 수행해 보겠습니다. 먼저 여러 문장을 문장별로 단어 토큰화 하는 함수를 선언하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjkrtdV8NYm4"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return word_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JIKFdhiOxKm"
      },
      "source": [
        "이 함수를 사용하여 두 작업을 한 번에 수행해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBmgvq5YOUOl",
        "outputId": "053ef058-ec74-44cf-d931-11f32f4d1abb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ],
      "source": [
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "You can see it out your window or on your television. \\\n",
        "You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens), len(word_tokens))\n",
        "print(word_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCiOl9pQOu_i"
      },
      "source": [
        "문장을 단어별로 토큰화하면 문맥 의미를 잃게 됩니다. 이 문제를 다소 해결하는 방안으로 [n-gram](https://wikidocs.net/21692)을 사용합니다. n-gram은 연속된 n개의 단어를 하나의 토큰화 단위로 분리합니다. n개 단어 크기 윈도우를 만들어 문장 처음부터 오른쪽으로 움직이면서 토큰화합니다. 예컨대 'Agent Smith knocks the door'를 2-gram(bigram)으로 만들면 (Agent, Smith), (knocks, the), (the, door)와 같이 연속적으로 단어 2개들을 차례로 이동하면서 토큰화합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKnfqCTulIqV"
      },
      "source": [
        "## **2.3. 스톱 워드(Stop Word) 제거**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-an7PdvhP_Cg"
      },
      "source": [
        "스톱 워드는 문맥적으로 무의미하여 분석에 불필요한 단어를 말합니다. 'is', 'the', 'a', 'will' 등을 예로 들 수 있습니다. 이런 단어들을 제거해야 데이터 분석을 수행할 때 중요 단어로 인지되지 않습니다.\n",
        "\n",
        "스톱 워드는 언어별로 목록화되어 있습니다. 아래에서 NLTK의 스톱 워드 종류를 확인해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXqg1XLwRGpB",
        "outputId": "526fd8f9-9139-42ba-8459-0949346501d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "영어의 스톱 워드 개수: 179개\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print('영어의 스톱 워드 개수: {}개'.format(len(nltk.corpus.stopwords.words('english'))))\n",
        "print(nltk.corpus.stopwords.words('english')[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mQR289iUa14"
      },
      "source": [
        "유의미한 단어만 추출해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0zjUboNSq6t",
        "outputId": "60f6ef14-4604-4449-d8c6-3ecb5655abeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ],
      "source": [
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "You can see it out your window or on your television. \\\n",
        "You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in word_tokens:\n",
        "    filtered_words = []\n",
        "    for word in sentence:\n",
        "        # 단어를 소문자로 변환합니다.\n",
        "        word = word.lower()\n",
        "        # 단어가 스톱 워드 리스트에 없으면 `filtered_words` 리스트에 추가합니다.\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    # 필터링한 단어를 모은 `filtered_words` 리스트를 `all_tokens` 리스트에 추가합니다.\n",
        "    all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwfF0wNIlIoI"
      },
      "source": [
        "## **2.4. Stemming과 Lemmatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYFkRuySVWf8"
      },
      "source": [
        "**Stemming**과 **Lemmatization**은 문법적 또는 의미적으로 변화하는 단어의 원형을 찾습니니다. Stemming은 원형 단어로 변환할 때 일반적이고 더 단순한 방법을 적용하여 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출합니다. 반면 Lemmatization은 문법적인 요소와 더 의미적인 부분을 감안하여 정확한 철자로 된 어근 단어를 찾습니다. 따라서 Lemmatization이 더 정교하며 의미론적인 기반에서 단어의 원형을 찾습니다. 다만 수행 시간이 오래 걸립니다.\n",
        "\n",
        "NLTK가 가진 Stemming을 위한 대표적 Stemmer는 `stem` 패키지의 `porter`, `lancaster`, `snowball` 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BheEnlJrHjwR",
        "outputId": "46094631-ce5a-4acd-efc8-b8856d4f5e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on package nltk.stem in nltk:\n",
            "\n",
            "NAME\n",
            "    nltk.stem - NLTK Stemmers\n",
            "\n",
            "DESCRIPTION\n",
            "    Interfaces used to remove morphological affixes from words, leaving\n",
            "    only the word stem.  Stemming algorithms aim to remove those affixes\n",
            "    required for eg. grammatical role, tense, derivational morphology\n",
            "    leaving only the stem of the word.  This is a difficult problem due to\n",
            "    irregular words (eg. common verbs in English), complicated\n",
            "    morphological rules, and part-of-speech and sense ambiguities\n",
            "    (eg. ``ceil-`` is not the stem of ``ceiling``).\n",
            "    \n",
            "    StemmerI defines a standard interface for stemmers.\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    api\n",
            "    arlstem\n",
            "    isri\n",
            "    lancaster\n",
            "    porter\n",
            "    regexp\n",
            "    rslp\n",
            "    snowball\n",
            "    util\n",
            "    wordnet\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.7/dist-packages/nltk/stem/__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk.stem\n",
        "\n",
        "help(nltk.stem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBa1LktfI8PL"
      },
      "source": [
        "아래에서 사용할 `LancasterStemmer` 클래스는 `lancaster` 모듈에 포함됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpuHDaalHbri",
        "outputId": "2d8657a6-4ad2-48fb-e6c9-629ea1620b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on module nltk.stem.lancaster in nltk.stem:\n",
            "\n",
            "NAME\n",
            "    nltk.stem.lancaster\n",
            "\n",
            "DESCRIPTION\n",
            "    A word stemmer based on the Lancaster (Paice/Husk) stemming algorithm.\n",
            "    Paice, Chris D. \"Another Stemmer.\" ACM SIGIR Forum 24.3 (1990): 56-61.\n",
            "\n",
            "CLASSES\n",
            "    nltk.stem.api.StemmerI(builtins.object)\n",
            "        LancasterStemmer\n",
            "    \n",
            "    class LancasterStemmer(nltk.stem.api.StemmerI)\n",
            "     |  LancasterStemmer(rule_tuple=None, strip_prefix_flag=False)\n",
            "     |  \n",
            "     |  Lancaster Stemmer\n",
            "     |  \n",
            "     |      >>> from nltk.stem.lancaster import LancasterStemmer\n",
            "     |      >>> st = LancasterStemmer()\n",
            "     |      >>> st.stem('maximum')     # Remove \"-um\" when word is intact\n",
            "     |      'maxim'\n",
            "     |      >>> st.stem('presumably')  # Don't remove \"-um\" when word is not intact\n",
            "     |      'presum'\n",
            "     |      >>> st.stem('multiply')    # No action taken if word ends with \"-ply\"\n",
            "     |      'multiply'\n",
            "     |      >>> st.stem('provision')   # Replace \"-sion\" with \"-j\" to trigger \"j\" set of rules\n",
            "     |      'provid'\n",
            "     |      >>> st.stem('owed')        # Word starting with vowel must contain at least 2 letters\n",
            "     |      'ow'\n",
            "     |      >>> st.stem('ear')         # ditto\n",
            "     |      'ear'\n",
            "     |      >>> st.stem('saying')      # Words starting with consonant must contain at least 3\n",
            "     |      'say'\n",
            "     |      >>> st.stem('crying')      #     letters and one of those letters must be a vowel\n",
            "     |      'cry'\n",
            "     |      >>> st.stem('string')      # ditto\n",
            "     |      'string'\n",
            "     |      >>> st.stem('meant')       # ditto\n",
            "     |      'meant'\n",
            "     |      >>> st.stem('cement')      # ditto\n",
            "     |      'cem'\n",
            "     |      >>> st_pre = LancasterStemmer(strip_prefix_flag=True)\n",
            "     |      >>> st_pre.stem('kilometer') # Test Prefix\n",
            "     |      'met'\n",
            "     |      >>> st_custom = LancasterStemmer(rule_tuple=(\"ssen4>\", \"s1t.\"))\n",
            "     |      >>> st_custom.stem(\"ness\") # Change s to t\n",
            "     |      'nest'\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      LancasterStemmer\n",
            "     |      nltk.stem.api.StemmerI\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, rule_tuple=None, strip_prefix_flag=False)\n",
            "     |      Create an instance of the Lancaster stemmer.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __unicode__ = __str__(self, /)\n",
            "     |  \n",
            "     |  parseRules(self, rule_tuple=None)\n",
            "     |      Validate the set of rules used in this stemmer.\n",
            "     |      \n",
            "     |      If this function is called as an individual method, without using stem\n",
            "     |      method, rule_tuple argument will be compiled into self.rule_dictionary.\n",
            "     |      If this function is called within stem, self._rule_tuple will be used.\n",
            "     |  \n",
            "     |  stem(self, word)\n",
            "     |      Stem a word using the Lancaster stemmer.\n",
            "     |  \n",
            "     |  unicode_repr = __repr__(self)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __abstractmethods__ = frozenset()\n",
            "     |  \n",
            "     |  default_rule_tuple = ('ai*2.', 'a*1.', 'bb1.', 'city3s.', 'ci2>', 'cn1...\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from nltk.stem.api.StemmerI:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "\n",
            "DATA\n",
            "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.7/dist-packages/nltk/stem/lancaster.py\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(nltk.stem.lancaster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgNnubu0JptS"
      },
      "source": [
        "Lemmatization을 위해서는 `WordNetLemmatizer` 클래스를 사용합니다. 이 클래스는 `wordnet` 모듈에 포함됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXRNN0HbJt3-",
        "outputId": "fd703a36-06e8-483e-9197-3f5ea04b4a92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on module nltk.stem.wordnet in nltk.stem:\n",
            "\n",
            "NAME\n",
            "    nltk.stem.wordnet\n",
            "\n",
            "DESCRIPTION\n",
            "    # Natural Language Toolkit: WordNet stemmer interface\n",
            "    #\n",
            "    # Copyright (C) 2001-2017 NLTK Project\n",
            "    # Author: Steven Bird <stevenbird1@gmail.com>\n",
            "    #         Edward Loper <edloper@gmail.com>\n",
            "    # URL: <http://nltk.org/>\n",
            "    # For license information, see LICENSE.TXT\n",
            "\n",
            "CLASSES\n",
            "    builtins.object\n",
            "        WordNetLemmatizer\n",
            "    \n",
            "    class WordNetLemmatizer(builtins.object)\n",
            "     |  WordNet Lemmatizer\n",
            "     |  \n",
            "     |  Lemmatize using WordNet's built-in morphy function.\n",
            "     |  Returns the input word unchanged if it cannot be found in WordNet.\n",
            "     |  \n",
            "     |      >>> from nltk.stem import WordNetLemmatizer\n",
            "     |      >>> wnl = WordNetLemmatizer()\n",
            "     |      >>> print(wnl.lemmatize('dogs'))\n",
            "     |      dog\n",
            "     |      >>> print(wnl.lemmatize('churches'))\n",
            "     |      church\n",
            "     |      >>> print(wnl.lemmatize('aardwolves'))\n",
            "     |      aardwolf\n",
            "     |      >>> print(wnl.lemmatize('abaci'))\n",
            "     |      abacus\n",
            "     |      >>> print(wnl.lemmatize('hardrock'))\n",
            "     |      hardrock\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __unicode__ = __str__(self, /)\n",
            "     |  \n",
            "     |  lemmatize(self, word, pos='n')\n",
            "     |  \n",
            "     |  unicode_repr = __repr__(self)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "\n",
            "FUNCTIONS\n",
            "    teardown_module(module=None)\n",
            "        # unload wordnet\n",
            "\n",
            "DATA\n",
            "    NOUN = 'n'\n",
            "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
            "    wordnet = <WordNetCorpusReader in '.../corpora/wordnet' (not loaded ye...\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.7/dist-packages/nltk/stem/wordnet.py\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(nltk.stem.wordnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baH7sbK8IiHf"
      },
      "source": [
        "먼저 Stemming을 수행해 보겠습니다. 클래스 인스턴스를 생성하여 `stem(단어)` 메서드를 사용하는 방법입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0R_A-AsHUs_",
        "outputId": "d2c3dd17-2307-4f2e-c906-878c91ef6c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FUZ6dFaL95L"
      },
      "source": [
        "- 'work': 기본 단어인 'work'에 'ing', 's', 'ed'가 붙는 단순한 변화이므로 'work'를 출력했습니다.\n",
        "\n",
        "- 'amuse': 기본 단어인 'amuse'에서 'e'가 탈락하고 'ing', 'es', 'ed'가 붙는 복잡한 변화이므로 'amus'를 기본 단어로 인식하고 출력했습니다.\n",
        "\n",
        "- 'happy', 'fancy': 원형을 찾지 못하고 원형에서 철자가 다른 어근 단어로 인식하고 출력했습니다.\n",
        "\n",
        "이번에는 Lemmatization을 수행해 보겠습니다. 클래스 인스턴스를 생성하여 `lemmatize(단어, 품사)` 메서드를 사용하는 방법입니다. 품사로 전달할 인자는 동사는 `v`, 형용사는 `a`입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byhd8V1sS9ow",
        "outputId": "9454d223-2339-4b37-8123-b3a35a4ba9fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
        "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
        "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYh5rwXcUdls"
      },
      "source": [
        "정확하게 출력했습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBs3j00flIl7"
      },
      "source": [
        "# **3. Bag of Words - BOW**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI34lI9LUh-t"
      },
      "source": [
        "**BOW** 모델은 문서가 가진 모든 단어를 문맥과 순서를 무시하고 일괄적으로 단어가 출현하는 만큼 빈도 값을 부여하여 피처 값을 추출합니다. 문서에 포함된 모든 단어를 봉투(Bag)에 넣고 섞는다는 뜻에서 Bag of Words라는 표현을 채택한 것입니다.\n",
        "\n",
        "BOW 모델의 핵심 원리를 살펴보겠습니다. 중복된 단어를 가진 두 문장이 있다고 가정하겠습니다. 더 자세한 내용은 본서를 참고하시기 바랍니다.\n",
        "\n",
        "1. 중복된 단어를 하나만 남기고 모두 제거합니다. 각 단어를 피처(칼럼) 형태로 나열하고 고유 인덱스를 부여합니다.\n",
        "\n",
        "1. 각 단어가 나타난 횟수를 인덱스에 부여합니다. 예컨대 문장1과 문장2가 있다고 가정할 때, 문장1만 특정 단어가 있고 나타난 횟수가 1번이면 1을 부여하고, 문장1과 문장2 모두 특정 단어가 있고 나타난 횟수가 각각 1번과 2번이면 각각 1과 2를 부여합니다.\n",
        "\n",
        "BOW 모델의 장점은 아래와 같습니다.\n",
        "\n",
        "- 쉽고 빠르게 구축할 수 있습니다.\n",
        "\n",
        "- 문서 특징을 잘 나타내며 여러 분야에서 활용합니다.\n",
        "\n",
        "단점은 아래와 같습니다.\n",
        "\n",
        "- 문맥 의미(Semantic Context)를 반영하기에 부족합니다. 단어 순서를 고려하지 않으므로 문맥 의미를 무시합니다. 보완책으로 n_gram 기법을 활용할 수 있지만 제한적이기 때문에 문맥 해석을 처리하기 어렵습니다.\n",
        "\n",
        "- **희소 행렬(Sparse Matrix)** 문제가 있습니다. BOW로 피처 벡터화를 수행하면 희소 행렬 형태를 가진 데이터 세트가 만들어지기 쉽습니다. 많은 문서에서 단어를 추출하면 상당한 수의 칼럼이 생성되는데, 문서마다 서로 다른 단어로 칼럼이 구성되므로 단어가 문서마다 나타나지 않는 경우가 훨씬 많습니다. 예컨대 문서1에 단어가 수십만 개일 경우 문서2에 있는 단어는 문서1과 공통된 단어 수가 적을 가능성이 높기 때문에, 공통되지 않은 단어는 모두 0 값으로 채워집니다.\n",
        "\n",
        "희소 행렬은 머신러닝 알고리즘의 수행 시간과 예측 성능을 떨어뜨리므로 이를 극복하기 위한 기법이 존재합니다. 아래에서 다시 다루겠습니다. 참고로 0 값이 아니라 유의미한 값으로 채워진 행렬은 **밀집 행렬(Dense Matrix)**입니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceqQYcX0lIj4"
      },
      "source": [
        "## **3.1. BOW 피처 벡터화**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slttFhByv2vj"
      },
      "source": [
        "머신러닝 알고리즘은 숫자형 피처를 데이터로 받아서 동작하므로 텍스트 같은 데이터를 알고리즘에 바로 입력할 수 없습니다. 텍스트를 특정 의미를 가진 숫자형 값인 벡터 값으로 변환하는 **피처 벡터화** 작업을 거쳐야 합니다. 자세한 내용은 본서를 참고하시기 바랍니다. \n",
        "\n",
        "피처 벡터화의 원리는 아래와 같습니다. \n",
        "\n",
        "1. 각 문서가 가진 텍스트를 단어로 추출하여 피처로 할당합니다.\n",
        "\n",
        "1. 각 단어의 발생 빈도와 같은 값을 피처에 값으로 부여합니다.\n",
        "\n",
        "1. 단어 피처의 발생 빈도 값으로 구성된 벡터로 만듭니다.\n",
        "\n",
        "즉, BOW 모델에서 피처 벡터화를 수행하는 것은 모든 문서에서 모든 단어를 칼럼 형태로 나열하고 각 문서에서 해당 단어 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 작업을 의미합니다. 예컨대 텍스트 문서가 M개만큼 존재하고 이 텍스트 문서들에서 모든 단어 N개를 추출하여 나열한다면 단어 N개의 값이 할당된 피처의 벡터 세트가 생성되며, 단어 피처가 $M \\times N$개로 구성된 행렬이 만들어집니다.\n",
        "\n",
        "BOW의 피처 벡터화는 아래 방식이 존재합니다.\n",
        "\n",
        "- 카운트(Count) 기반 벡터화: 단어 피처에 값을 부여할 때 단어가 나타난 횟수(카운트)를 부여합니다. 카운트 값이 높을수록 중요한 단어입니다. 이 방식의 약점이라면 별다른 의미 없이 자주 사용될 수밖에 없는 단어까지 높은 값을 부여하기 때문에 문서 특징을 나타내는 중요한 단어에 집중하지 못합니다. 문서 개수가 적거나 텍스트 길이가 적은 경우에 적합한 방법입니다.\n",
        "\n",
        "- **TF-IDF(Term Frequency - Inverse Document Frequency)** 기반 벡터화: 카운트 기반 벡터화의 약점을 보완하기 위한 방법입니다. 특성 문서에서 자주 나타난 단어에 높은 가중치를 부여하지만 모든 문서에서 전반적으로 자주 나타나는 단어에는 페널티(Penalty)를 부여합니다. 예컨대 언어 특성상 모든 문서에서 보편적이고 반복적으로 사용될 수 있는 부사('많이', '상당하게', 당연하게' 등)나 명사('이름', '업무', '조직' 등)와 같은 단어들은 중요하지 않으므로(물론 형용사도 포함될 수 있습니다) 페널티를 부여하여 가중치 균형을 잡습니다. 문서 개수가 많거나 텍스트 길이가 많은 경우에 적합한 방법입니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brJCR7ozlIho"
      },
      "source": [
        "## **3.2. 사이킷런의 Count 및 TF-IDF 벡터화 구현: CountVectorizer, TfidfVectorizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbesuOOI3Bs_"
      },
      "source": [
        "`CounterVectorizer` 클래스는 카운트 기반 벡터화를 구현한 클래스이며 소문자 일괄 변환, 토큰화, 스톱 워드 필터링 등 전처리도 수행합니다. 파라미터는 아래와 같습니다. 참고로 TF-IDF를 구현한 `TfidfVectorizer` 클래스는 `CountVectorizer` 클래스의 파라미터와 변환 방법이 동일합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwQC9mbMVkH1",
        "outputId": "bb7e8c09-2f53-4448-9952-6f0794717cfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
            "\n",
            "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
            " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
            " |  \n",
            " |  Convert a collection of text documents to a matrix of token counts.\n",
            " |  \n",
            " |  This implementation produces a sparse representation of the counts using\n",
            " |  scipy.sparse.csr_matrix.\n",
            " |  \n",
            " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
            " |  that does some kind of feature selection then the number of features will\n",
            " |  be equal to the vocabulary size found by analyzing the data.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  input : {'filename', 'file', 'content'}, default='content'\n",
            " |      - If `'filename'`, the sequence passed as an argument to fit is\n",
            " |        expected to be a list of filenames that need reading to fetch\n",
            " |        the raw content to analyze.\n",
            " |  \n",
            " |      - If `'file'`, the sequence items must have a 'read' method (file-like\n",
            " |        object) that is called to fetch the bytes in memory.\n",
            " |  \n",
            " |      - If `'content'`, the input is expected to be a sequence of items that\n",
            " |        can be of type string or byte.\n",
            " |  \n",
            " |  encoding : str, default='utf-8'\n",
            " |      If bytes or files are given to analyze, this encoding is used to\n",
            " |      decode.\n",
            " |  \n",
            " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
            " |      Instruction on what to do if a byte sequence is given to analyze that\n",
            " |      contains characters not of the given `encoding`. By default, it is\n",
            " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
            " |      values are 'ignore' and 'replace'.\n",
            " |  \n",
            " |  strip_accents : {'ascii', 'unicode'}, default=None\n",
            " |      Remove accents and perform other character normalization\n",
            " |      during the preprocessing step.\n",
            " |      'ascii' is a fast method that only works on characters that have\n",
            " |      an direct ASCII mapping.\n",
            " |      'unicode' is a slightly slower method that works on any characters.\n",
            " |      None (default) does nothing.\n",
            " |  \n",
            " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
            " |      :func:`unicodedata.normalize`.\n",
            " |  \n",
            " |  lowercase : bool, default=True\n",
            " |      Convert all characters to lowercase before tokenizing.\n",
            " |  \n",
            " |  preprocessor : callable, default=None\n",
            " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
            " |      preserving the tokenizing and n-grams generation steps.\n",
            " |      Only applies if ``analyzer`` is not callable.\n",
            " |  \n",
            " |  tokenizer : callable, default=None\n",
            " |      Override the string tokenization step while preserving the\n",
            " |      preprocessing and n-grams generation steps.\n",
            " |      Only applies if ``analyzer == 'word'``.\n",
            " |  \n",
            " |  stop_words : {'english'}, list, default=None\n",
            " |      If 'english', a built-in stop word list for English is used.\n",
            " |      There are several known issues with 'english' and you should\n",
            " |      consider an alternative (see :ref:`stop_words`).\n",
            " |  \n",
            " |      If a list, that list is assumed to contain stop words, all of which\n",
            " |      will be removed from the resulting tokens.\n",
            " |      Only applies if ``analyzer == 'word'``.\n",
            " |  \n",
            " |      If None, no stop words will be used. max_df can be set to a value\n",
            " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
            " |      words based on intra corpus document frequency of terms.\n",
            " |  \n",
            " |  token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
            " |      Regular expression denoting what constitutes a \"token\", only used\n",
            " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
            " |      or more alphanumeric characters (punctuation is completely ignored\n",
            " |      and always treated as a token separator).\n",
            " |  \n",
            " |      If there is a capturing group in token_pattern then the\n",
            " |      captured group content, not the entire match, becomes the token.\n",
            " |      At most one capturing group is permitted.\n",
            " |  \n",
            " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
            " |      The lower and upper boundary of the range of n-values for different\n",
            " |      word n-grams or char n-grams to be extracted. All values of n such\n",
            " |      such that min_n <= n <= max_n will be used. For example an\n",
            " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
            " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
            " |      Only applies if ``analyzer`` is not callable.\n",
            " |  \n",
            " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
            " |      Whether the feature should be made of word n-gram or character\n",
            " |      n-grams.\n",
            " |      Option 'char_wb' creates character n-grams only from text inside\n",
            " |      word boundaries; n-grams at the edges of words are padded with space.\n",
            " |  \n",
            " |      If a callable is passed it is used to extract the sequence of features\n",
            " |      out of the raw, unprocessed input.\n",
            " |  \n",
            " |      .. versionchanged:: 0.21\n",
            " |  \n",
            " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
            " |      first read from the file and then passed to the given callable\n",
            " |      analyzer.\n",
            " |  \n",
            " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
            " |      When building the vocabulary ignore terms that have a document\n",
            " |      frequency strictly higher than the given threshold (corpus-specific\n",
            " |      stop words).\n",
            " |      If float, the parameter represents a proportion of documents, integer\n",
            " |      absolute counts.\n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
            " |      When building the vocabulary ignore terms that have a document\n",
            " |      frequency strictly lower than the given threshold. This value is also\n",
            " |      called cut-off in the literature.\n",
            " |      If float, the parameter represents a proportion of documents, integer\n",
            " |      absolute counts.\n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  max_features : int, default=None\n",
            " |      If not None, build a vocabulary that only consider the top\n",
            " |      max_features ordered by term frequency across the corpus.\n",
            " |  \n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  vocabulary : Mapping or iterable, default=None\n",
            " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
            " |      indices in the feature matrix, or an iterable over terms. If not\n",
            " |      given, a vocabulary is determined from the input documents. Indices\n",
            " |      in the mapping should not be repeated and should not have any gap\n",
            " |      between 0 and the largest index.\n",
            " |  \n",
            " |  binary : bool, default=False\n",
            " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
            " |      probabilistic models that model binary events rather than integer\n",
            " |      counts.\n",
            " |  \n",
            " |  dtype : type, default=np.int64\n",
            " |      Type of the matrix returned by fit_transform() or transform().\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  vocabulary_ : dict\n",
            " |      A mapping of terms to feature indices.\n",
            " |  \n",
            " |  fixed_vocabulary_ : bool\n",
            " |      True if a fixed vocabulary of term to indices mapping\n",
            " |      is provided by the user.\n",
            " |  \n",
            " |  stop_words_ : set\n",
            " |      Terms that were ignored because they either:\n",
            " |  \n",
            " |        - occurred in too many documents (`max_df`)\n",
            " |        - occurred in too few documents (`min_df`)\n",
            " |        - were cut off by feature selection (`max_features`).\n",
            " |  \n",
            " |      This is only available if no vocabulary was given.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  HashingVectorizer : Convert a collection of text documents to a\n",
            " |      matrix of token counts.\n",
            " |  \n",
            " |  TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
            " |      of TF-IDF features.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The ``stop_words_`` attribute can get large and increase the model size\n",
            " |  when pickling. This attribute is provided only for introspection and can\n",
            " |  be safely removed using delattr or set to None before pickling.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
            " |  >>> corpus = [\n",
            " |  ...     'This is the first document.',\n",
            " |  ...     'This document is the second document.',\n",
            " |  ...     'And this is the third one.',\n",
            " |  ...     'Is this the first document?',\n",
            " |  ... ]\n",
            " |  >>> vectorizer = CountVectorizer()\n",
            " |  >>> X = vectorizer.fit_transform(corpus)\n",
            " |  >>> vectorizer.get_feature_names_out()\n",
            " |  array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
            " |         'this'], ...)\n",
            " |  >>> print(X.toarray())\n",
            " |  [[0 1 1 1 0 0 1 0 1]\n",
            " |   [0 2 0 1 0 1 1 0 1]\n",
            " |   [1 0 0 1 1 0 1 1 1]\n",
            " |   [0 1 1 1 0 0 1 0 1]]\n",
            " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
            " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
            " |  >>> vectorizer2.get_feature_names_out()\n",
            " |  array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
            " |         'second document', 'the first', 'the second', 'the third', 'third one',\n",
            " |         'this document', 'this is', 'this the'], ...)\n",
            " |   >>> print(X2.toarray())\n",
            " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
            " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
            " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      CountVectorizer\n",
            " |      _VectorizerMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, raw_documents, y=None)\n",
            " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      y : None\n",
            " |          This parameter is ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted vectorizer.\n",
            " |  \n",
            " |  fit_transform(self, raw_documents, y=None)\n",
            " |      Learn the vocabulary dictionary and return document-term matrix.\n",
            " |      \n",
            " |      This is equivalent to fit followed by transform, but more efficiently\n",
            " |      implemented.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      y : None\n",
            " |          This parameter is ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : array of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |  \n",
            " |  get_feature_names(self)\n",
            " |      DEPRECATED: get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            " |      \n",
            " |      Array mapping from feature integer indices to feature name.\n",
            " |      \n",
            " |          Returns\n",
            " |          -------\n",
            " |          feature_names : list\n",
            " |              A list of feature names.\n",
            " |  \n",
            " |  get_feature_names_out(self, input_features=None)\n",
            " |      Get output feature names for transformation.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      input_features : array-like of str or None, default=None\n",
            " |          Not used, present here for API consistency by convention.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_names_out : ndarray of str objects\n",
            " |          Transformed feature names.\n",
            " |  \n",
            " |  inverse_transform(self, X)\n",
            " |      Return terms per document with nonzero entries in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_inv : list of arrays of shape (n_samples,)\n",
            " |          List of arrays of terms.\n",
            " |  \n",
            " |  transform(self, raw_documents)\n",
            " |      Transform documents to document-term matrix.\n",
            " |      \n",
            " |      Extract token counts out of raw text documents using the vocabulary\n",
            " |      fitted with fit or the one provided to the constructor.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : sparse matrix of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from _VectorizerMixin:\n",
            " |  \n",
            " |  build_analyzer(self)\n",
            " |      Return a callable to process input data.\n",
            " |      \n",
            " |      The callable handles that handles preprocessing, tokenization, and\n",
            " |      n-grams generation.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      analyzer: callable\n",
            " |          A function to handle preprocessing, tokenization\n",
            " |          and n-grams generation.\n",
            " |  \n",
            " |  build_preprocessor(self)\n",
            " |      Return a function to preprocess the text before tokenization.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      preprocessor: callable\n",
            " |            A function to preprocess the text before tokenization.\n",
            " |  \n",
            " |  build_tokenizer(self)\n",
            " |      Return a function that splits a string into a sequence of tokens.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      tokenizer: callable\n",
            " |            A function to split a string into a sequence of tokens.\n",
            " |  \n",
            " |  decode(self, doc)\n",
            " |      Decode the input into a string of unicode symbols.\n",
            " |      \n",
            " |      The decoding strategy depends on the vectorizer parameters.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      doc : bytes or str\n",
            " |          The string to decode.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      doc: str\n",
            " |          A string of unicode symbols.\n",
            " |  \n",
            " |  get_stop_words(self)\n",
            " |      Build or fetch the effective stop words list.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      stop_words: list or None\n",
            " |              A list of stop words.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from _VectorizerMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "help(CountVectorizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4xq2GSWWxpA"
      },
      "source": [
        "`CountVectorizer` 클래스를 활용하여 카운트 기반 피처 벡터화를 수행하는 방법은 아래와 같습니다.\n",
        "\n",
        "1. 데이터 전처리: 영어의 경우 단어를 소문자로 변경하는 등 전처리 작업을 수행합니다. 디폴트는 `lowercase=Ture`입니다.\n",
        "\n",
        "1. 디폴트 `analyzer=word`인 단어 기준으로 `n_gram_range` 파라미터를 반영하여 단어를 토큰화합니다. `n_gram_range` 파라미터는 단어 순서를 보강하기 위한 범위를 지정하는 것이며 `(1, 1)`로 지정하면 토큰화된 단어를 1개씩 피처로 추출하고 `(1, 2)`로 지정하면 토큰화된 단어를 최소 1개씩 순서대로 최대 2개씩(최대 2개) 묶어서 추출합니다. \n",
        "\n",
        "1. 텍스트 정규화를 수행합니다. 단, `stop_words='english'`로 파라미터를 지정하면 스톱 워드만 필터링 합니다. Stemming이나 Lemmatization 같은 어근 변환은 `CountVectorizer` 클래스에서 지원하진 않지만 `tokenizer` 파라미터에 어근 변환 함수를 따로 선언하여 적용하거나 외부 패키지를 적용하면 어근 변환을 수행할 수 있습니다.\n",
        "\n",
        "1. `max_df`, `min_df`, `max_features` 등 파라미터를 이용하여 토큰화된 단어를 피처로 추출하고 단어 빈도수 벡터 값을 적용합니다. \n",
        "\n",
        " - `max_df`: 높은 빈도수를 가진 단어 피처를 제외합니다. 자주 나타나는 단어는 스톱 워드와 비슷한 반복적인 단어일 가능성이 높습니다.\n",
        "\n",
        " - `min_df`: 낮은 빈도수를 가진 단어 피처를 제외합니다. 적게 나타나는 단어는 중요하지 않은 단어일 가능성이 높습니다.\n",
        "\n",
        " - `max_features`: 추출할 피처 개수를 제한합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe1Qyw3_lIfc"
      },
      "source": [
        "## **3.3. BOW 벡터화를 위한 희소 행렬**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKsNwnWadUHb"
      },
      "source": [
        "대규모의 행렬이 생성될 때 레코드의 각 문서가 가지는 단어 수는 제한적이므로 행렬 값은 대부분 0입니다. 이렇게 대규모 행렬의 값이 대부분 0인 행렬을 희소 행렬이라고 합니다. BOW 형태를 가진 언어 모델의 피처 벡터화는 대부분 희소 행렬입니다.\n",
        "\n",
        "다만 희소 행렬은 불필요한 0 값이 메모리 공간에 지나치게 많이 할당되어 많은 메모리 공간이 필요하고 행렬 크기가 커서 연산할 때 데이터 엑세스를 위한 오랜 시간이 소요됩니다. 물리적으로 적은 메모리 공간을 사용하도록 변환하는 방법은 **COO** 형식과 **CSR** 형식이 있습니다. 이 중에 CSR이 더 뛰어나고 많이 사용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM3suxkZlIdT"
      },
      "source": [
        "## **3.4. 희소 행렬 - COO 형식**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrFt_F9I5o-4"
      },
      "source": [
        "**COO(Coordinate: 좌표)** 형식은 0이 아닌 데이터만 별도 데이터 배열에 저장하고, 그 데이터가 가리키는 행과 열 위치를 별도 배열로 저장합니다. 예컨대 아래 2차원 데이터인 밀집 행렬이 있다고 가정하겠습니다.\n",
        "\n",
        "```\n",
        "[[3, 0, 1],\n",
        " [0, 2, 0]]\n",
        "```\n",
        "\n",
        "여기서 0이 아닌 데이터 `[3, 1, 2]`를 위치(row, col)로 표시하면 `(0, 0), (0, 2), (1, 1)`입니다. 로우(row)와 칼럼(col)을 따로 떼어 별도 배열로 저장하면 로우는 `[0, 0, 1]`이고 칼럼은 `[0, 2, 1]`입니다.\n",
        "\n",
        "사이파이의 `sparse` 패키지를 사용하면 희소 행렬 변환을 수행할 수 있습니다. 위 2차원 데이터를 COO 형식으로 희소 행렬로 변환해 보겠습니다. 사용할 클래스는 `coo_matrix`입니다. 참고로 `coo_matrix`의 명칭은 소문자로 시작하지만 메서드가 아닌 클래스입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEiZfCgN7KNn",
        "outputId": "d27a44a6-9710-4219-b53c-adb3056e75d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "# 밀집 행렬을 생성합니다.\n",
        "dense = np.array([[3, 0, 1], [0, 2, 0]])\n",
        "\n",
        "# 0이 아닌 데이터를 추출합니다.\n",
        "data = np.array([3, 1, 2])\n",
        "\n",
        "# 행 위치와 열 위치를 배열로 생성합니다.\n",
        "row_pos = np.array([0, 0, 1])\n",
        "col_pos = np.array([0, 2, 1])\n",
        "\n",
        "# COO 형식으로 희소 행렬을 생성하고 인스턴스화합니다.\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
        "\n",
        "# `toarray()` 메서드를 사용하여 밀집 행렬로 만듭니다.\n",
        "sparse_coo.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yaM6G0RAdhl"
      },
      "source": [
        "COO 형식의 문제점이라면 행과 열 위치를 나타내기 위해 반복적으로 위치 데이터를 사용해야 하는 것입니다. 예컨대 아래 2차원 배열이 있다고 가정하겠습니다.\n",
        "\n",
        "```\n",
        "[[0, 0, 1, 0, 0, 5],\n",
        " [1, 4, 0, 3, 2, 5],\n",
        " [0, 6, 0, 3, 0, 0],\n",
        " [2, 0, 0, 0, 0, 0],\n",
        " [0, 0, 0, 7, 0, 8],\n",
        " [1, 0, 0, 0, 0, 0]]\n",
        "```\n",
        "\n",
        "여기서 0이 아닌 데이터 배열은 아래와 같습니다.\n",
        "\n",
        "`[1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1]`\n",
        "\n",
        "행 위치 배열은 아래와 같습니다.\n",
        "\n",
        "`[0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5]`\n",
        "\n",
        "열 위치 배열은 아래와 같습니다.\n",
        "\n",
        "`[2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0]`\n",
        "\n",
        "이 중에 행 위치 배열은 같은 값이 반복적으로 나열되고 순차적으로 증가합니다. 0은 2번, 1은 5번, 2는 2번인 식입니다. 따라서 행 위치 배열의 고유한 값의 시작 위치만 표기한다면(위치의 위치를 표기한다면) 불필요한 반복을 제거할 수 있을 것입니다. 인덱스를 기준으로 삼아 0의 시작 위치는 0, 1의 시작 위치는 2, 2의 시작 위치는 7, 3의 시작 위치는 9, 4의 시작 위치는 10, 5의 시작 위치는 12이므로 `[0, 2, 7, 9, 10, 12]`가 됩니다. 최종적으로 이 배열의 맨 마지막에 데이터의 총 항목 개수를 추가하면 `[0, 2, 7, 9, 10, 12, 13]`이 됩니다. 이 일련의 방식이 바로 아래에서 기술할 **CSR(Compressed Sparse Row)** 형식입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGQLXJ1QlIbG"
      },
      "source": [
        "## **3.5. 희소 행렬 - CSR 형식**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKlPHDST_OSp"
      },
      "source": [
        "**CSR(Compressed Sparse Row)** 형식은 COO 형식보다 메모리가 적게 필요하고 빠르게 연산할 수 있습니다. `csr_matrix` 클래스에 구현되어 있으며 전달할 인자는 `(0이 아닌 데이터 배열, 열 위치 배열, 행 위치 배열의 고유한 값의 시작 위치)`입니다.\n",
        "\n",
        "CSR 형식을 코드로 구현해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpU1MTcpIJ1D",
        "outputId": "723f6c0a-fc09-4c18-a378-0ecb7039cc48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "dense2 = np.array([[0, 0, 1, 0, 0, 5],\n",
        "                   [1, 4, 0, 3, 2, 5],\n",
        "                   [0, 6, 0, 3, 0, 0],\n",
        "                   [2, 0, 0, 0, 0, 0],\n",
        "                   [0, 0, 0, 7, 0, 8],\n",
        "                   [1, 0, 0, 0, 0, 0]])\n",
        "\n",
        "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
        "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
        "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
        "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
        "\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "print(sparse_csr.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3bZLz4-JX3m"
      },
      "source": [
        "실제로 사용할 때는 생성 파라미터로 밀집 행렬만 전달하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvOU6tZ9Jh4w",
        "outputId": "0b110453-1cea-4a83-b53a-050125b63679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "dense3 = np.array([[0, 0, 1, 0, 0, 5],\n",
        "                   [1, 4, 0, 3, 2, 5],\n",
        "                   [0, 6, 0, 3, 0, 0],\n",
        "                   [2, 0, 0, 0, 0, 0],\n",
        "                   [0, 0, 0, 7, 0, 8],\n",
        "                   [1, 0, 0, 0, 0, 0]])\n",
        "\n",
        "coo = sparse.coo_matrix(dense3)\n",
        "csr = sparse.csr_matrix(dense3)\n",
        "print(coo.toarray())\n",
        "print()\n",
        "print(csr.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkr-cn7_KQn3"
      },
      "source": [
        "참고로 `CountVectorizer` 클래스와 `TfidfVectorizer` 클래스로 변환된 피처 벡터화 행렬은 모두 CSR 형태의 희소 행렬입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qktTPqy_lIY2"
      },
      "source": [
        "# **4. 텍스트 분류 실습 - 20 뉴스그룹 분류**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TbIRrGxKjJn"
      },
      "source": [
        "사이킷런의 20 뉴스그룹 데이터 세트를 사용하여 텍스트 분류를 수행해 보겠습니다. 텍스트 분류는 특정 문서의 분류를 학습 데이터를 토대로 학습하고 모델을 생성하여 다른 문서의 분류를 예측하는 것입니다.\n",
        "\n",
        "텍스트를 피처 벡터화로 변환하면 일반적으로 희소 행렬이 됩니다. 이 희소 행렬에 분류를 효과적으로 처리할 수 있는 알고리즘은 **로지스틱 회귀**, **선형 서포트 벡터 머신**, **나이브 베이즈** 등입니다. 텍스트를 기반으로 분류를 수행하려면 먼저 텍스트를 정규화하고 피처 벡터화를 적용해야 합니다. 그리고나서 알고리즘을 적용하고 분류를 학습하여 예측하고 평가합니다. \n",
        "\n",
        "이번 파트에서는 카운트 기반과 TF-IDF 기반 벡터화를 모두 적용하여 예측 성능을 비교하고 피처 백터화를 위한 파라미터와 `GridSearchCV` 기반의 하이퍼 파라미터 튜닝, `Pipeline` 인스턴스를 통한 피처 벡터화 파라미터와 `GridSearchCV` 기반의 하이퍼 파라미터 튜닝을 전부 수행해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI4ltzX4lIWl"
      },
      "source": [
        "## **4.1. 텍스트 정규화**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNi7YtkXqIJj"
      },
      "source": [
        "`fetch_20newsgroups()` 메서드를 호출하여 데이터를 준비하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5oj6blDVtzE"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "news_data = fetch_20newsgroups(subset='all', random_state=156)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7a25rXjW9Yh",
        "outputId": "d702a99f-4c53-465f-c8c3-2f3d92f93910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on function fetch_20newsgroups in module sklearn.datasets._twenty_newsgroups:\n",
            "\n",
            "fetch_20newsgroups(*, data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True, return_X_y=False)\n",
            "    Load the filenames and data from the 20 newsgroups dataset (classification).\n",
            "    \n",
            "    Download it if necessary.\n",
            "    \n",
            "    =================   ==========\n",
            "    Classes                     20\n",
            "    Samples total            18846\n",
            "    Dimensionality               1\n",
            "    Features                  text\n",
            "    =================   ==========\n",
            "    \n",
            "    Read more in the :ref:`User Guide <20newsgroups_dataset>`.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    data_home : str, default=None\n",
            "        Specify a download and cache folder for the datasets. If None,\n",
            "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
            "    \n",
            "    subset : {'train', 'test', 'all'}, default='train'\n",
            "        Select the dataset to load: 'train' for the training set, 'test'\n",
            "        for the test set, 'all' for both, with shuffled ordering.\n",
            "    \n",
            "    categories : array-like, dtype=str, default=None\n",
            "        If None (default), load all the categories.\n",
            "        If not None, list of category names to load (other categories\n",
            "        ignored).\n",
            "    \n",
            "    shuffle : bool, default=True\n",
            "        Whether or not to shuffle the data: might be important for models that\n",
            "        make the assumption that the samples are independent and identically\n",
            "        distributed (i.i.d.), such as stochastic gradient descent.\n",
            "    \n",
            "    random_state : int, RandomState instance or None, default=None\n",
            "        Determines random number generation for dataset shuffling. Pass an int\n",
            "        for reproducible output across multiple function calls.\n",
            "        See :term:`Glossary <random_state>`.\n",
            "    \n",
            "    remove : tuple, default=()\n",
            "        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
            "        these are kinds of text that will be detected and removed from the\n",
            "        newsgroup posts, preventing classifiers from overfitting on\n",
            "        metadata.\n",
            "    \n",
            "        'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
            "        ends of posts that look like signatures, and 'quotes' removes lines\n",
            "        that appear to be quoting another post.\n",
            "    \n",
            "        'headers' follows an exact standard; the other filters are not always\n",
            "        correct.\n",
            "    \n",
            "    download_if_missing : bool, default=True\n",
            "        If False, raise an IOError if the data is not locally available\n",
            "        instead of trying to download the data from the source site.\n",
            "    \n",
            "    return_X_y : bool, default=False\n",
            "        If True, returns `(data.data, data.target)` instead of a Bunch\n",
            "        object.\n",
            "    \n",
            "        .. versionadded:: 0.22\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    bunch : :class:`~sklearn.utils.Bunch`\n",
            "        Dictionary-like object, with the following attributes.\n",
            "    \n",
            "        data : list of shape (n_samples,)\n",
            "            The data list to learn.\n",
            "        target: ndarray of shape (n_samples,)\n",
            "            The target labels.\n",
            "        filenames: list of shape (n_samples,)\n",
            "            The path to the location of the data.\n",
            "        DESCR: str\n",
            "            The full description of the dataset.\n",
            "        target_names: list of shape (n_classes,)\n",
            "            The names of target classes.\n",
            "    \n",
            "    (data, target) : tuple if `return_X_y=True`\n",
            "        .. versionadded:: 0.22\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(fetch_20newsgroups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgdxTdQMWM9i"
      },
      "source": [
        "데이터 구성을 살펴보겠습니다. 자세한 설명은 생략합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKUd1ZA4WEH_",
        "outputId": "d4352a5a-aa3b-4dcc-9482-38db481932c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
          ]
        }
      ],
      "source": [
        "print(news_data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NXWq9rh2oJx",
        "outputId": "c891f848-4b77-4e26-f34d-2d88879c5f32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".. _20newsgroups_dataset:\n",
            "\n",
            "The 20 newsgroups text dataset\n",
            "------------------------------\n",
            "\n",
            "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
            "20 topics split in two subsets: one for training (or development)\n",
            "and the other one for testing (or for performance evaluation). The split\n",
            "between the train and test set is based upon a messages posted before\n",
            "and after a specific date.\n",
            "\n",
            "This module contains two loaders. The first one,\n",
            ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
            "returns a list of the raw texts that can be fed to text feature\n",
            "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
            "with custom parameters so as to extract feature vectors.\n",
            "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
            "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
            "extractor.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    =================   ==========\n",
            "    Classes                     20\n",
            "    Samples total            18846\n",
            "    Dimensionality               1\n",
            "    Features                  text\n",
            "    =================   ==========\n",
            "\n",
            "Usage\n",
            "~~~~~\n",
            "\n",
            "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
            "fetching / caching functions that downloads the data archive from\n",
            "the original `20 newsgroups website`_, extracts the archive contents\n",
            "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
            ":func:`sklearn.datasets.load_files` on either the training or\n",
            "testing set folder, or both of them::\n",
            "\n",
            "  >>> from sklearn.datasets import fetch_20newsgroups\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
            "\n",
            "  >>> from pprint import pprint\n",
            "  >>> pprint(list(newsgroups_train.target_names))\n",
            "  ['alt.atheism',\n",
            "   'comp.graphics',\n",
            "   'comp.os.ms-windows.misc',\n",
            "   'comp.sys.ibm.pc.hardware',\n",
            "   'comp.sys.mac.hardware',\n",
            "   'comp.windows.x',\n",
            "   'misc.forsale',\n",
            "   'rec.autos',\n",
            "   'rec.motorcycles',\n",
            "   'rec.sport.baseball',\n",
            "   'rec.sport.hockey',\n",
            "   'sci.crypt',\n",
            "   'sci.electronics',\n",
            "   'sci.med',\n",
            "   'sci.space',\n",
            "   'soc.religion.christian',\n",
            "   'talk.politics.guns',\n",
            "   'talk.politics.mideast',\n",
            "   'talk.politics.misc',\n",
            "   'talk.religion.misc']\n",
            "\n",
            "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
            "attribute is the integer index of the category::\n",
            "\n",
            "  >>> newsgroups_train.filenames.shape\n",
            "  (11314,)\n",
            "  >>> newsgroups_train.target.shape\n",
            "  (11314,)\n",
            "  >>> newsgroups_train.target[:10]\n",
            "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
            "\n",
            "It is possible to load only a sub-selection of the categories by passing the\n",
            "list of the categories to load to the\n",
            ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
            "\n",
            "  >>> cats = ['alt.atheism', 'sci.space']\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
            "\n",
            "  >>> list(newsgroups_train.target_names)\n",
            "  ['alt.atheism', 'sci.space']\n",
            "  >>> newsgroups_train.filenames.shape\n",
            "  (1073,)\n",
            "  >>> newsgroups_train.target.shape\n",
            "  (1073,)\n",
            "  >>> newsgroups_train.target[:10]\n",
            "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
            "\n",
            "Converting text to vectors\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "In order to feed predictive or clustering models with the text data,\n",
            "one first need to turn the text into vectors of numerical values suitable\n",
            "for statistical analysis. This can be achieved with the utilities of the\n",
            "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
            "example that extract `TF-IDF`_ vectors of unigram tokens\n",
            "from a subset of 20news::\n",
            "\n",
            "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
            "  ...               'comp.graphics', 'sci.space']\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
            "  ...                                       categories=categories)\n",
            "  >>> vectorizer = TfidfVectorizer()\n",
            "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
            "  >>> vectors.shape\n",
            "  (2034, 34118)\n",
            "\n",
            "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
            "components by sample in a more than 30000-dimensional space\n",
            "(less than .5% non-zero features)::\n",
            "\n",
            "  >>> vectors.nnz / float(vectors.shape[0])\n",
            "  159.01327...\n",
            "\n",
            ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
            "returns ready-to-use token counts features instead of file names.\n",
            "\n",
            ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
            ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
            "\n",
            "\n",
            "Filtering text for more realistic training\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "It is easy for a classifier to overfit on particular things that appear in the\n",
            "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
            "high F-scores, but their results would not generalize to other documents that\n",
            "aren't from this window of time.\n",
            "\n",
            "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
            "which is fast to train and achieves a decent F-score::\n",
            "\n",
            "  >>> from sklearn.naive_bayes import MultinomialNB\n",
            "  >>> from sklearn import metrics\n",
            "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
            "  ...                                      categories=categories)\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> clf = MultinomialNB(alpha=.01)\n",
            "  >>> clf.fit(vectors, newsgroups_train.target)\n",
            "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
            "  0.88213...\n",
            "\n",
            "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
            "the training and test data, instead of segmenting by time, and in that case\n",
            "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
            "yet of what's going on inside this classifier?)\n",
            "\n",
            "Let's take a look at what the most informative features are:\n",
            "\n",
            "  >>> import numpy as np\n",
            "  >>> def show_top10(classifier, vectorizer, categories):\n",
            "  ...     feature_names = vectorizer.get_feature_names_out()\n",
            "  ...     for i, category in enumerate(categories):\n",
            "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
            "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
            "  ...\n",
            "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
            "  alt.atheism: edu it and in you that is of to the\n",
            "  comp.graphics: edu in graphics it is for and of to the\n",
            "  sci.space: edu it that is in and space to of the\n",
            "  talk.religion.misc: not it you in is that and to of the\n",
            "\n",
            "\n",
            "You can now see many things that these features have overfit to:\n",
            "\n",
            "- Almost every group is distinguished by whether headers such as\n",
            "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
            "- Another significant feature involves whether the sender is affiliated with\n",
            "  a university, as indicated either by their headers or their signature.\n",
            "- The word \"article\" is a significant feature, based on how often people quote\n",
            "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
            "  wrote:\"\n",
            "- Other features match the names and e-mail addresses of particular people who\n",
            "  were posting at the time.\n",
            "\n",
            "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
            "barely have to identify topics from text at all, and they all perform at the\n",
            "same high level.\n",
            "\n",
            "For this reason, the functions that load 20 Newsgroups data provide a\n",
            "parameter called **remove**, telling it what kinds of information to strip out\n",
            "of each file. **remove** should be a tuple containing any subset of\n",
            "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
            "blocks, and quotation blocks respectively.\n",
            "\n",
            "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
            "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
            "  ...                                      categories=categories)\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
            "  0.77310...\n",
            "\n",
            "This classifier lost over a lot of its F-score, just because we removed\n",
            "metadata that has little to do with topic classification.\n",
            "It loses even more if we also strip this metadata from the training data:\n",
            "\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
            "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
            "  ...                                       categories=categories)\n",
            "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
            "  >>> clf = MultinomialNB(alpha=.01)\n",
            "  >>> clf.fit(vectors, newsgroups_train.target)\n",
            "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
            "  0.76995...\n",
            "\n",
            "Some other classifiers cope better with this harder version of the task. Try\n",
            "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
            "the ``--filter`` option to compare the results.\n",
            "\n",
            ".. topic:: Data Considerations\n",
            "\n",
            "  The Cleveland Indians is a major league baseball team based in Cleveland,\n",
            "  Ohio, USA. In December 2020, it was reported that \"After several months of\n",
            "  discussion sparked by the death of George Floyd and a national reckoning over\n",
            "  race and colonialism, the Cleveland Indians have decided to change their\n",
            "  name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
            "  its informal nickname -- the Tribe -- its new team name.\" \"It’s not going to\n",
            "  be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
            "  American-themed name.\"\n",
            "\n",
            "  https://www.mlb.com/news/cleveland-indians-team-name-change\n",
            "\n",
            ".. topic:: Recommendation\n",
            "\n",
            "  - When evaluating text classifiers on the 20 Newsgroups data, you\n",
            "    should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
            "    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
            "    lower because it is more realistic.\n",
            "  - This text dataset contains data which may be inappropriate for certain NLP\n",
            "    applications. An example is listed in the \"Data Considerations\" section\n",
            "    above. The challenge with using current text datasets in NLP for tasks such\n",
            "    as sentence completion, clustering, and other applications is that text\n",
            "    that is culturally biased and inflammatory will propagate biases. This\n",
            "    should be taken into consideration when using the dataset, reviewing the\n",
            "    output, and the bias should be documented.\n",
            "\n",
            ".. topic:: Examples\n",
            "\n",
            "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
            "\n",
            "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(news_data.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeNy6IP8eGmT"
      },
      "source": [
        "먼저 타겟 클래스를 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4ZhOWAWWaIC",
        "outputId": "9fda2104-306f-4105-a976-b2c486a2b433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target 클래스의 값과 분포도\n",
            " 0     799\n",
            "1     973\n",
            "2     985\n",
            "3     982\n",
            "4     963\n",
            "5     988\n",
            "6     975\n",
            "7     990\n",
            "8     996\n",
            "9     994\n",
            "10    999\n",
            "11    991\n",
            "12    984\n",
            "13    990\n",
            "14    987\n",
            "15    997\n",
            "16    910\n",
            "17    940\n",
            "18    775\n",
            "19    628\n",
            "dtype: int64\n",
            "\n",
            "target 클래스의 이름\n",
            " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "print('target 클래스의 값과 분포도\\n', pd.Series(news_data.target).value_counts().sort_index())\n",
        "print()\n",
        "print('target 클래스의 이름\\n', news_data.target_names)\n",
        "print(len(news_data.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujP2TVqAdZ44"
      },
      "source": [
        "총 20개의 타겟 클래스를 가졌으며 각 클래스의 값 분포는 몇 클래스를 제외하면 고르게 분포합니다.\n",
        "\n",
        "데이터를 하나만 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plRcujeTw3zN",
        "outputId": "e6978138-8964-438f-9ff7-9441e6f5ad28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
            "Subject: Re: Observation re: helmets\n",
            "Organization: Sun Microsystems, RTP, NC\n",
            "Lines: 21\n",
            "Distribution: world\n",
            "Reply-To: egreen@east.sun.com\n",
            "NNTP-Posting-Host: laser.east.sun.com\n",
            "\n",
            "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
            "> \n",
            "> The question for the day is re: passenger helmets, if you don't know for \n",
            ">certain who's gonna ride with you (like say you meet them at a .... church \n",
            ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
            ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
            ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
            ">passenger? \n",
            "\n",
            "If your primary concern is protecting the passenger in the event of a\n",
            "crash, have him or her fitted for a helmet that is their size.  If your\n",
            "primary concern is complying with stupid helmet laws, carry a real big\n",
            "spare (you can put a big or small head in a big helmet, but not in a\n",
            "small one).\n",
            "\n",
            "---\n",
            "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
            "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
            "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
            " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(news_data.data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdauSbUddd_w"
      },
      "source": [
        "기사 내용뿐만 아니라 제목, 작성자, 소속, 이메일 등 정보를 모두 갖고 있습니다. 모든 정보를 사용하면 어떤 머신러닝 알고리즘을 적용해도 높은 예측 성능을 발휘하게 되므로 내용을 제외한 모든 정보를 삭제하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65At5_tnfyHn",
        "outputId": "e044cdf7-69c5-419c-b50a-534fea1a469a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터 크기: 11314\n",
            "테스트 데이터 크기: 7532\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "train_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
        "                   random_state=156)\n",
        "X_train = train_news.data\n",
        "y_train = train_news.target\n",
        "\n",
        "test_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),\n",
        "                               random_state=156)\n",
        "X_test = test_news.data\n",
        "y_test = test_news.target\n",
        "\n",
        "print('학습 데이터 크기:', len(train_news.data))\n",
        "print('테스트 데이터 크기:', len(test_news.data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTIU3NoGiZkt",
        "outputId": "39e64a66-d5fc-4158-924e-93d27b564bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "What I did NOT get with my drive (CD300i) is the System Install CD you\n",
            "listed as #1.  Any ideas about how I can get one?  I bought my IIvx 8/120\n",
            "from Direct Express in Chicago (no complaints at all -- good price & good\n",
            "service).\n",
            "\n",
            "BTW, I've heard that the System Install CD can be used to boot the mac;\n",
            "however, my drive will NOT accept a CD caddy is the machine is off.  How can\n",
            "you boot with it then?\n",
            "\n",
            "--Dave\n",
            "\n",
            "\n",
            "The tech support line for GCC is 1-800-231-1570.\n"
          ]
        }
      ],
      "source": [
        "print(train_news.data[0])\n",
        "print()\n",
        "print(test_news.data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRGL_QovlIUU"
      },
      "source": [
        "## **4.2. 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaCEHdFkjN1g"
      },
      "source": [
        "`CountVectorizer` 클래스를 사용하여 학습 데이터와 테스트 데이터의 텍스트를 피처 벡터화하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEbkZzkskPrt",
        "outputId": "b66fd067-27b8-42a6-be09-39af5ec7848a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 데이터 텍스트의 CountVectorizer Shape: (11314, 101631)\n",
            "테스트 데이터 텍스트의 CountVectorizer Shape: (7532, 101631)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cnt_vect = CountVectorizer()\n",
        "cnt_vect.fit(X_train)\n",
        "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터의 피처 개수를 같게 만들기 위해\n",
        "# 학습 데이터로 `fit()`을 적용한 인스턴스를 사용하여 테스트 데이터를 변환합니다.\n",
        "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
        "\n",
        "print('학습 데이터 텍스트의 CountVectorizer Shape:', X_train_cnt_vect.shape)\n",
        "print('테스트 데이터 텍스트의 CountVectorizer Shape:', X_test_cnt_vect.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F163TDu-oCH7"
      },
      "source": [
        "두 데이터 세트에 각각 같은 단어수인 101,631개가 피처로 생성되었습니다.\n",
        "\n",
        "로지스틱 회귀를 적용하여 뉴스그룹을 예측해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCqOVEOXoVP4",
        "outputId": "29851e63-507a-42e4-e6b7-258063857088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CountVectorized Logistic Regression의 예측 정확도: 0.6076739245884227\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_cnt_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_cnt_vect)\n",
        "\n",
        "print('CountVectorized Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7djWXpEqG4s"
      },
      "source": [
        "이번에는 `TfidfVectorizer` 클래스를 사용하여 피처 벡터화를 수행하고 로지스틱 회귀를 적용해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2hfEyVHpQyt",
        "outputId": "f833f895-8118-4dfd-92ef-e1d573f75dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Logistic Regression의 예측 정확도: 0.6736590546999469\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "\n",
        "print('TF-IDF Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr2LKewWqVBl"
      },
      "source": [
        "TF-IDF 기반 예측 정확도가 더 높습니다. 다량의 문서와 텍스트는 TF-IDF 기반 피처 벡터화가 더 좋은 예측 결과를 도출합니다.\n",
        "\n",
        "TA를 수행할 때 머신러닝 모델 성능을 제고하려면 최적 알고리즘을 선택하고 피처 전처리를 완벽하게 수행해야 합니다. 이번에는 다양한 파라미터를 지정하여 예측 성능을 제고해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1WEO3wIr5q_",
        "outputId": "23fcfb50-52f4-4910-9ea7-20d3b27bbefb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Logistic Regression의 예측 정확도: 0.6922464152947424\n"
          ]
        }
      ],
      "source": [
        "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
        "pred = lr_clf.predict(X_test_tfidf_vect)\n",
        "\n",
        "print('TF-IDF Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLfgXzwuurEp"
      },
      "source": [
        "성능이 약간 향상되었습니다.\n",
        "\n",
        "그리드 서치를 활용하여 로지스틱 회귀의 최적 파라미터를 탐색(`C`만 탐색 시도)한 뒤 모델의 예측 성능을 확인해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3ukQiV5tQI8",
        "outputId": "c9006ab7-e8a6-4eb1-a01f-92fed295954d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression best C parameter: {'C': 10}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'C': [0.01, 0.1, 1, 5, 10]}\n",
        "grid_cv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
        "grid_cv_lr.fit(X_train_tfidf_vect, y_train)\n",
        "print('Logistic Regression best C parameter:', grid_cv_lr.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81M4CkRVuULG",
        "outputId": "22060ea0-1214-4fa0-8ad6-e8c2c17557d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Vectorized Logistic Regression의 예측 정확도: 0.7010090281465746\n"
          ]
        }
      ],
      "source": [
        "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
        "print('TF-IDF Vectorized Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcialUCrwER-"
      },
      "source": [
        "성능이 약간 더 향상되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD55vf6YlIRw"
      },
      "source": [
        "## **4.3. 사이킷런 파이프라인(Pipeline) 사용 및 GridSearchCV와의 결합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3wsZsIV7be3"
      },
      "source": [
        "위 과정을 파이프라인으로 구현해 보겠습니다. 먼저 그리드서치를 결합하지 않은 형태를 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JydnvNt05jre",
        "outputId": "aedf5d9d-d60d-4c02-f3c6-c05c401ec1a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline을 통한 Logistic Regression의 예측 정확도: 0.7010090281465746\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "                     ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)),\n",
        "                     ('lr_clf', LogisticRegression(C=10))\n",
        "                     ])\n",
        "pipeline.fit(X_train, y_train)\n",
        "pred = pipeline.predict(X_test)\n",
        "\n",
        "print('Pipeline을 통한 Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1r1NFS6AUiz"
      },
      "source": [
        "같은 결괏값이 도출되었습니다.\n",
        "\n",
        "이번에는 파이프라인과 그리드서치를 결합해 보겠습니다. 관련된 여러 파라미터를 추가하여 수행하되 모든 파라미터를 탐색하면 많은 수행 시간이 소요되므로 몇 개만 선별하여 시도하겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyG2oyWv6aYu",
        "outputId": "7021d0bd-fb2b-49fb-8ef8-47d37d917d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May 21 09:33:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c816v3LXCdst",
        "outputId": "438a9f3f-3937-4e3b-ae75-b7f6b83114e9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "                     ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
        "                     ('lr_clf', LogisticRegression())\n",
        "                     ])\n",
        "# 언더바 2개(`__`)를 이어서 탐색할 파라미터를 특정합니다.\n",
        "params = {'tfidf_vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "          'tfidf_vect__max_df': [100, 300, 700],\n",
        "          'lr_clf__C': [1, 5, 10]\n",
        "          }\n",
        "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
        "grid_cv_pipe.fit(X_train, y_train)\n",
        "print(grid_cv_pipe.best_params_, grid_cv_pipe.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "코랩 무료 버전으로 코드를 실행하면 도중에 램 부족 문제가 발생하여 세션이 다운됩니다. 램 12.69GB을 모두 사용할 만큼 무거운 작업인가 봅니다. 보유 중인 랩톱으로도 시도했으나 수행 시간이 10시간 이상 걸려서 연산을 중지하기로 결정했습니다. 본서에 명시된 출력 결과를 명시하는 것으로 갈음하겠습니다.\n",
        "\n",
        "```\n",
        "[Parallel(n_jobs=1)]: Done 81 out of 81 | elapsed: 31.9min finished\n",
        "{'lr_clf__C': 10, 'tfidf_vect__max_df': 700, 'tfidf_vect__ngram_range': (1, 2)} 0.755524129397207\n",
        "```\n",
        "\n",
        "예측 정확도를 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "oKasX6Zx9YoB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRF6y4qSDpY1"
      },
      "outputs": [],
      "source": [
        "pred = grid_cv_pipe.predict(X_test)\n",
        "print('Pipeline을 통한 Logistic Regression의 예측 정확도:', accuracy_score(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Pipeline을 통한 Logistic Regression의 예측 정확도: 0.702\n",
        "```\n",
        "\n",
        "이전 결과에 비하면 거의 개선되지 않았다고 볼 수 있습니다. 서포트 벡터머신과 나이브 베이즈 알고리즘도 시도해볼 가치가 있겠습니다."
      ],
      "metadata": {
        "id": "lYgQsoX29ZFQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB6lg40Pm20l"
      },
      "source": [
        "# **5. 감성 분석**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAnaF6fVm2zN"
      },
      "source": [
        "## **5.1. 감성 분석 소개**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**감성 분석(Sentiment Analysis)**은 문서의 주관적인 감성, 의견, 감정, 기분 등을 파악하기 위한 방법입니다. 소셜 미디어, 여론조사, 온라인 리뷰, 피드백 등 다양한 분야에서 활용됩니다. 문서 내 텍스트가 나타내는 주관적인 단어와 문맥을 기반으로 감성 수치(긍정 감성 지수, 부정 감성 지수)를 계산하고 도출하여 긍정과 부정을 결정합니다.\n",
        "\n",
        "감성 분석은 지도학습과 비지도학습으로 나눌 수 있습니다. 지도학습은 학습 데이터와 타깃 레이블 값을 기반으로 감성 분석 학습을 수행하고 다른 데이터의 감성 분석을 예측합니다. 텍스트 기반 분류와 동일하다고 볼 수 있습니다. 비지도학습은 `Lexicon`이라는 감성 어휘 사전 모듈을 사용합니다. 감성 분석을 위한 용어와 문맥에 대한 정보를 가진 이 모듈로 긍정과 부정을 판단합니다.\n"
      ],
      "metadata": {
        "id": "m7qHo6F_Bd2K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7gaY_aMm2w5"
      },
      "source": [
        "## **5.2. 지도학습 기반 감성 분석 실습 - IMDB 영화평**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "지도학습을 기반으로 감성 분석을 수행해 보겠습니다. [IMDB 영화평](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) 데이터를 사용하여 긍정과 부정 여부를 예측하는 모델을 생성하겠습니다.\n",
        "\n",
        "데이터를 준비하겠습니다."
      ],
      "metadata": {
        "id": "h4uUv7sZpOgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import pandas as pd\n",
        "\n",
        "review_df = pd.read_csv('/content/gdrive/MyDrive/Bag of Words Meets Bags of Popcorn/labeledTrainData.tsv',\n",
        "                        header=0, sep='\\t', quoting=3)\n",
        "review_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "RY6jZazm1Vja",
        "outputId": "66c1f9ef-295a-4389-82ec-0c3290652837"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id  sentiment                                             review\n",
              "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
              "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
              "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
              "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
              "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-793b95d4-e51a-4b88-9aee-7d8c3e76c129\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"5814_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"With all this stuff going down at the moment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2381_9\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"7759_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"3630_4\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"It must be assumed that those who praised thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"9495_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-793b95d4-e51a-4b88-9aee-7d8c3e76c129')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-793b95d4-e51a-4b88-9aee-7d8c3e76c129 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-793b95d4-e51a-4b88-9aee-7d8c3e76c129');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "리뷰가 어떻게 구성되어 있는지 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "re5jzzd44bvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(review_df['review'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTjMB1go3NRu",
        "outputId": "8af31a99-ea0e-4e26-dd77-6d8c14ad273b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`<br />` 태그가 있습니다. 불필요하므로 모두 공백으로 바꾸겠습니다."
      ],
      "metadata": {
        "id": "e2vl3f2d4fXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_df['review'] = review_df['review'].str.replace('<br />', ' ')"
      ],
      "metadata": {
        "id": "GnNJjQ8n4p8M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아울러 숫자나 특수문자도 감성 분석에는 무의미한 요소입니다. 공백으로 대체하겠습니다. `re` 모듈은 편리하게 정규 표현식을 지원하며 텍스트 분석을 위한 필수 도구입니다."
      ],
      "metadata": {
        "id": "hKIWg8Ll5L9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(re)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW5kXOfh6Yvj",
        "outputId": "93d339c9-d661-464f-9b0d-585f40e211e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on module re:\n",
            "\n",
            "NAME\n",
            "    re - Support for regular expressions (RE).\n",
            "\n",
            "MODULE REFERENCE\n",
            "    https://docs.python.org/3.7/library/re\n",
            "    \n",
            "    The following documentation is automatically generated from the Python\n",
            "    source files.  It may be incomplete, incorrect or include features that\n",
            "    are considered implementation detail and may vary between Python\n",
            "    implementations.  When in doubt, consult the module reference at the\n",
            "    location listed above.\n",
            "\n",
            "DESCRIPTION\n",
            "    This module provides regular expression matching operations similar to\n",
            "    those found in Perl.  It supports both 8-bit and Unicode strings; both\n",
            "    the pattern and the strings being processed can contain null bytes and\n",
            "    characters outside the US ASCII range.\n",
            "    \n",
            "    Regular expressions can contain both special and ordinary characters.\n",
            "    Most ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\n",
            "    regular expressions; they simply match themselves.  You can\n",
            "    concatenate ordinary characters, so last matches the string 'last'.\n",
            "    \n",
            "    The special characters are:\n",
            "        \".\"      Matches any character except a newline.\n",
            "        \"^\"      Matches the start of the string.\n",
            "        \"$\"      Matches the end of the string or just before the newline at\n",
            "                 the end of the string.\n",
            "        \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n",
            "                 Greedy means that it will match as many repetitions as possible.\n",
            "        \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n",
            "        \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n",
            "        *?,+?,?? Non-greedy versions of the previous three special characters.\n",
            "        {m,n}    Matches from m to n repetitions of the preceding RE.\n",
            "        {m,n}?   Non-greedy version of the above.\n",
            "        \"\\\\\"     Either escapes special characters or signals a special sequence.\n",
            "        []       Indicates a set of characters.\n",
            "                 A \"^\" as the first character indicates a complementing set.\n",
            "        \"|\"      A|B, creates an RE that will match either A or B.\n",
            "        (...)    Matches the RE inside the parentheses.\n",
            "                 The contents can be retrieved or matched later in the string.\n",
            "        (?aiLmsux) The letters set the corresponding flags defined below.\n",
            "        (?:...)  Non-grouping version of regular parentheses.\n",
            "        (?P<name>...) The substring matched by the group is accessible by name.\n",
            "        (?P=name)     Matches the text matched earlier by the group named name.\n",
            "        (?#...)  A comment; ignored.\n",
            "        (?=...)  Matches if ... matches next, but doesn't consume the string.\n",
            "        (?!...)  Matches if ... doesn't match next.\n",
            "        (?<=...) Matches if preceded by ... (must be fixed length).\n",
            "        (?<!...) Matches if not preceded by ... (must be fixed length).\n",
            "        (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n",
            "                           the (optional) no pattern otherwise.\n",
            "    \n",
            "    The special sequences consist of \"\\\\\" and a character from the list\n",
            "    below.  If the ordinary character is not on the list, then the\n",
            "    resulting RE will match the second character.\n",
            "        \\number  Matches the contents of the group of the same number.\n",
            "        \\A       Matches only at the start of the string.\n",
            "        \\Z       Matches only at the end of the string.\n",
            "        \\b       Matches the empty string, but only at the start or end of a word.\n",
            "        \\B       Matches the empty string, but not at the start or end of a word.\n",
            "        \\d       Matches any decimal digit; equivalent to the set [0-9] in\n",
            "                 bytes patterns or string patterns with the ASCII flag.\n",
            "                 In string patterns without the ASCII flag, it will match the whole\n",
            "                 range of Unicode digits.\n",
            "        \\D       Matches any non-digit character; equivalent to [^\\d].\n",
            "        \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v] in\n",
            "                 bytes patterns or string patterns with the ASCII flag.\n",
            "                 In string patterns without the ASCII flag, it will match the whole\n",
            "                 range of Unicode whitespace characters.\n",
            "        \\S       Matches any non-whitespace character; equivalent to [^\\s].\n",
            "        \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_]\n",
            "                 in bytes patterns or string patterns with the ASCII flag.\n",
            "                 In string patterns without the ASCII flag, it will match the\n",
            "                 range of Unicode alphanumeric characters (letters plus digits\n",
            "                 plus underscore).\n",
            "                 With LOCALE, it will match the set [0-9_] plus characters defined\n",
            "                 as letters for the current locale.\n",
            "        \\W       Matches the complement of \\w.\n",
            "        \\\\       Matches a literal backslash.\n",
            "    \n",
            "    This module exports the following functions:\n",
            "        match     Match a regular expression pattern to the beginning of a string.\n",
            "        fullmatch Match a regular expression pattern to all of a string.\n",
            "        search    Search a string for the presence of a pattern.\n",
            "        sub       Substitute occurrences of a pattern found in a string.\n",
            "        subn      Same as sub, but also return the number of substitutions made.\n",
            "        split     Split a string by the occurrences of a pattern.\n",
            "        findall   Find all occurrences of a pattern in a string.\n",
            "        finditer  Return an iterator yielding a Match object for each match.\n",
            "        compile   Compile a pattern into a Pattern object.\n",
            "        purge     Clear the regular expression cache.\n",
            "        escape    Backslash all non-alphanumerics in a string.\n",
            "    \n",
            "    Each function other than purge and escape can take an optional 'flags' argument\n",
            "    consisting of one or more of the following module constants, joined by \"|\".\n",
            "    A, L, and U are mutually exclusive.\n",
            "        A  ASCII       For string patterns, make \\w, \\W, \\b, \\B, \\d, \\D\n",
            "                       match the corresponding ASCII character categories\n",
            "                       (rather than the whole Unicode categories, which is the\n",
            "                       default).\n",
            "                       For bytes patterns, this flag is the only available\n",
            "                       behaviour and needn't be specified.\n",
            "        I  IGNORECASE  Perform case-insensitive matching.\n",
            "        L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n",
            "        M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n",
            "                       as well as the string.\n",
            "                       \"$\" matches the end of lines (before a newline) as well\n",
            "                       as the end of the string.\n",
            "        S  DOTALL      \".\" matches any character at all, including the newline.\n",
            "        X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n",
            "        U  UNICODE     For compatibility only. Ignored for string patterns (it\n",
            "                       is the default), and forbidden for bytes patterns.\n",
            "    \n",
            "    This module also defines an exception 'error'.\n",
            "\n",
            "CLASSES\n",
            "    builtins.Exception(builtins.BaseException)\n",
            "        error\n",
            "    builtins.object\n",
            "        Match\n",
            "        Pattern\n",
            "    \n",
            "    class Match(builtins.object)\n",
            "     |  The result of re.match() and re.search().\n",
            "     |  Match objects always have a boolean value of True.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __copy__(self, /)\n",
            "     |  \n",
            "     |  __deepcopy__(self, memo, /)\n",
            "     |  \n",
            "     |  __getitem__(self, key, /)\n",
            "     |      Return self[key].\n",
            "     |  \n",
            "     |  __repr__(self, /)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  end(self, group=0, /)\n",
            "     |      Return index of the end of the substring matched by group.\n",
            "     |  \n",
            "     |  expand(self, /, template)\n",
            "     |      Return the string obtained by doing backslash substitution on the string template, as done by the sub() method.\n",
            "     |  \n",
            "     |  group(...)\n",
            "     |      group([group1, ...]) -> str or tuple.\n",
            "     |      Return subgroup(s) of the match by indices or names.\n",
            "     |      For 0 returns the entire match.\n",
            "     |  \n",
            "     |  groupdict(self, /, default=None)\n",
            "     |      Return a dictionary containing all the named subgroups of the match, keyed by the subgroup name.\n",
            "     |      \n",
            "     |      default\n",
            "     |        Is used for groups that did not participate in the match.\n",
            "     |  \n",
            "     |  groups(self, /, default=None)\n",
            "     |      Return a tuple containing all the subgroups of the match, from 1.\n",
            "     |      \n",
            "     |      default\n",
            "     |        Is used for groups that did not participate in the match.\n",
            "     |  \n",
            "     |  span(self, group=0, /)\n",
            "     |      For match object m, return the 2-tuple (m.start(group), m.end(group)).\n",
            "     |  \n",
            "     |  start(self, group=0, /)\n",
            "     |      Return index of the start of the substring matched by group.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  endpos\n",
            "     |      The index into the string beyond which the RE engine will not go.\n",
            "     |  \n",
            "     |  lastgroup\n",
            "     |      The name of the last matched capturing group.\n",
            "     |  \n",
            "     |  lastindex\n",
            "     |      The integer index of the last matched capturing group.\n",
            "     |  \n",
            "     |  pos\n",
            "     |      The index into the string at which the RE engine started looking for a match.\n",
            "     |  \n",
            "     |  re\n",
            "     |      The regular expression object.\n",
            "     |  \n",
            "     |  regs\n",
            "     |  \n",
            "     |  string\n",
            "     |      The string passed to match() or search().\n",
            "    \n",
            "    class Pattern(builtins.object)\n",
            "     |  Compiled regular expression object.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __copy__(self, /)\n",
            "     |  \n",
            "     |  __deepcopy__(self, memo, /)\n",
            "     |  \n",
            "     |  __eq__(self, value, /)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __ge__(self, value, /)\n",
            "     |      Return self>=value.\n",
            "     |  \n",
            "     |  __gt__(self, value, /)\n",
            "     |      Return self>value.\n",
            "     |  \n",
            "     |  __hash__(self, /)\n",
            "     |      Return hash(self).\n",
            "     |  \n",
            "     |  __le__(self, value, /)\n",
            "     |      Return self<=value.\n",
            "     |  \n",
            "     |  __lt__(self, value, /)\n",
            "     |      Return self<value.\n",
            "     |  \n",
            "     |  __ne__(self, value, /)\n",
            "     |      Return self!=value.\n",
            "     |  \n",
            "     |  __repr__(self, /)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  findall(self, /, string, pos=0, endpos=9223372036854775807)\n",
            "     |      Return a list of all non-overlapping matches of pattern in string.\n",
            "     |  \n",
            "     |  finditer(self, /, string, pos=0, endpos=9223372036854775807)\n",
            "     |      Return an iterator over all non-overlapping matches for the RE pattern in string.\n",
            "     |      \n",
            "     |      For each match, the iterator returns a match object.\n",
            "     |  \n",
            "     |  fullmatch(self, /, string, pos=0, endpos=9223372036854775807)\n",
            "     |      Matches against all of the string.\n",
            "     |  \n",
            "     |  match(self, /, string, pos=0, endpos=9223372036854775807)\n",
            "     |      Matches zero or more characters at the beginning of the string.\n",
            "     |  \n",
            "     |  scanner(self, /, string, pos=0, endpos=9223372036854775807)\n",
            "     |  \n",
            "     |  search(self, /, string, pos=0, endpos=9223372036854775807)\n",
            "     |      Scan through string looking for a match, and return a corresponding match object instance.\n",
            "     |      \n",
            "     |      Return None if no position in the string matches.\n",
            "     |  \n",
            "     |  split(self, /, string, maxsplit=0)\n",
            "     |      Split string by the occurrences of pattern.\n",
            "     |  \n",
            "     |  sub(self, /, repl, string, count=0)\n",
            "     |      Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.\n",
            "     |  \n",
            "     |  subn(self, /, repl, string, count=0)\n",
            "     |      Return the tuple (new_string, number_of_subs_made) found by replacing the leftmost non-overlapping occurrences of pattern with the replacement repl.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  flags\n",
            "     |      The regex matching flags.\n",
            "     |  \n",
            "     |  groupindex\n",
            "     |      A dictionary mapping group names to group numbers.\n",
            "     |  \n",
            "     |  groups\n",
            "     |      The number of capturing groups in the pattern.\n",
            "     |  \n",
            "     |  pattern\n",
            "     |      The pattern string from which the RE object was compiled.\n",
            "    \n",
            "    class error(builtins.Exception)\n",
            "     |  error(msg, pattern=None, pos=None)\n",
            "     |  \n",
            "     |  Exception raised for invalid regular expressions.\n",
            "     |  \n",
            "     |  Attributes:\n",
            "     |  \n",
            "     |      msg: The unformatted error message\n",
            "     |      pattern: The regular expression pattern\n",
            "     |      pos: The index in the pattern where compilation failed (may be None)\n",
            "     |      lineno: The line corresponding to pos (may be None)\n",
            "     |      colno: The column corresponding to pos (may be None)\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      error\n",
            "     |      builtins.Exception\n",
            "     |      builtins.BaseException\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, msg, pattern=None, pos=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods inherited from builtins.Exception:\n",
            "     |  \n",
            "     |  __new__(*args, **kwargs) from builtins.type\n",
            "     |      Create and return a new object.  See help(type) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __delattr__(self, name, /)\n",
            "     |      Implement delattr(self, name).\n",
            "     |  \n",
            "     |  __getattribute__(self, name, /)\n",
            "     |      Return getattr(self, name).\n",
            "     |  \n",
            "     |  __reduce__(...)\n",
            "     |      Helper for pickle.\n",
            "     |  \n",
            "     |  __repr__(self, /)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setattr__(self, name, value, /)\n",
            "     |      Implement setattr(self, name, value).\n",
            "     |  \n",
            "     |  __setstate__(...)\n",
            "     |  \n",
            "     |  __str__(self, /)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  with_traceback(...)\n",
            "     |      Exception.with_traceback(tb) --\n",
            "     |      set self.__traceback__ to tb and return self.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __cause__\n",
            "     |      exception cause\n",
            "     |  \n",
            "     |  __context__\n",
            "     |      exception context\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |  \n",
            "     |  __suppress_context__\n",
            "     |  \n",
            "     |  __traceback__\n",
            "     |  \n",
            "     |  args\n",
            "\n",
            "FUNCTIONS\n",
            "    compile(pattern, flags=0)\n",
            "        Compile a regular expression pattern, returning a Pattern object.\n",
            "    \n",
            "    escape(pattern)\n",
            "        Escape special characters in a string.\n",
            "    \n",
            "    findall(pattern, string, flags=0)\n",
            "        Return a list of all non-overlapping matches in the string.\n",
            "        \n",
            "        If one or more capturing groups are present in the pattern, return\n",
            "        a list of groups; this will be a list of tuples if the pattern\n",
            "        has more than one group.\n",
            "        \n",
            "        Empty matches are included in the result.\n",
            "    \n",
            "    finditer(pattern, string, flags=0)\n",
            "        Return an iterator over all non-overlapping matches in the\n",
            "        string.  For each match, the iterator returns a Match object.\n",
            "        \n",
            "        Empty matches are included in the result.\n",
            "    \n",
            "    fullmatch(pattern, string, flags=0)\n",
            "        Try to apply the pattern to all of the string, returning\n",
            "        a Match object, or None if no match was found.\n",
            "    \n",
            "    match(pattern, string, flags=0)\n",
            "        Try to apply the pattern at the start of the string, returning\n",
            "        a Match object, or None if no match was found.\n",
            "    \n",
            "    purge()\n",
            "        Clear the regular expression caches\n",
            "    \n",
            "    search(pattern, string, flags=0)\n",
            "        Scan through string looking for a match to the pattern, returning\n",
            "        a Match object, or None if no match was found.\n",
            "    \n",
            "    split(pattern, string, maxsplit=0, flags=0)\n",
            "        Split the source string by the occurrences of the pattern,\n",
            "        returning a list containing the resulting substrings.  If\n",
            "        capturing parentheses are used in pattern, then the text of all\n",
            "        groups in the pattern are also returned as part of the resulting\n",
            "        list.  If maxsplit is nonzero, at most maxsplit splits occur,\n",
            "        and the remainder of the string is returned as the final element\n",
            "        of the list.\n",
            "    \n",
            "    sub(pattern, repl, string, count=0, flags=0)\n",
            "        Return the string obtained by replacing the leftmost\n",
            "        non-overlapping occurrences of the pattern in string by the\n",
            "        replacement repl.  repl can be either a string or a callable;\n",
            "        if a string, backslash escapes in it are processed.  If it is\n",
            "        a callable, it's passed the Match object and must return\n",
            "        a replacement string to be used.\n",
            "    \n",
            "    subn(pattern, repl, string, count=0, flags=0)\n",
            "        Return a 2-tuple containing (new_string, number).\n",
            "        new_string is the string obtained by replacing the leftmost\n",
            "        non-overlapping occurrences of the pattern in the source\n",
            "        string by the replacement repl.  number is the number of\n",
            "        substitutions that were made. repl can be either a string or a\n",
            "        callable; if a string, backslash escapes in it are processed.\n",
            "        If it is a callable, it's passed the Match object and must\n",
            "        return a replacement string to be used.\n",
            "    \n",
            "    template(pattern, flags=0)\n",
            "        Compile a template pattern, returning a Pattern object\n",
            "\n",
            "DATA\n",
            "    A = <RegexFlag.ASCII: 256>\n",
            "    ASCII = <RegexFlag.ASCII: 256>\n",
            "    DOTALL = <RegexFlag.DOTALL: 16>\n",
            "    I = <RegexFlag.IGNORECASE: 2>\n",
            "    IGNORECASE = <RegexFlag.IGNORECASE: 2>\n",
            "    L = <RegexFlag.LOCALE: 4>\n",
            "    LOCALE = <RegexFlag.LOCALE: 4>\n",
            "    M = <RegexFlag.MULTILINE: 8>\n",
            "    MULTILINE = <RegexFlag.MULTILINE: 8>\n",
            "    S = <RegexFlag.DOTALL: 16>\n",
            "    U = <RegexFlag.UNICODE: 32>\n",
            "    UNICODE = <RegexFlag.UNICODE: 32>\n",
            "    VERBOSE = <RegexFlag.VERBOSE: 64>\n",
            "    X = <RegexFlag.VERBOSE: 64>\n",
            "    __all__ = ['match', 'fullmatch', 'search', 'sub', 'subn', 'split', 'fi...\n",
            "\n",
            "VERSION\n",
            "    2.2.1\n",
            "\n",
            "FILE\n",
            "    /usr/lib/python3.7/re.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# `re`모듈의 `sub()` 메서드를 적용하여 숫자와 특수문자를 공백으로 변환합니다.\n",
        "# '[^a-zA-Z]'는 영어 대소문자가 아닌 모든 문자를 찾는 것을 의미합니다.\n",
        "review_df['review'] = review_df['review'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))"
      ],
      "metadata": {
        "id": "DaRr_oOk513f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(review_df['review'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEaBZmby8yNN",
        "outputId": "701a80af-b173-4b33-b52f-c08d82f99379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay   Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him   The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music   Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene   Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'review' 피처를 피처 데이터 세트로 생성하고, 'sentiment' 피처를 결정 값 데이터 세트로 생성하겠습니다. 이를 토대로 학습 데이터 세트와 테스트 데이터 세트로 분할하겠습니다."
      ],
      "metadata": {
        "id": "FMBId6sn89iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_df = review_df.drop(['id', 'sentiment'], axis=1, inplace=False)\n",
        "class_df = review_df['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_df, class_df,\n",
        "                                                    test_size=0.3, random_state=156)\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZCo2FV7AONG",
        "outputId": "13a72ec3-a169-4bfc-de91-09d2250a4ab9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17500, 1) (7500, 1)\n",
            "(17500,) (7500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "리뷰 텍스트를 카운트 기반으로 피처 벡터화를 수행하고 머신러닝 알고리즘을 적용하여 예측 성능을 확인하겠습니다."
      ],
      "metadata": {
        "id": "TEGoDcbqC5eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "pipeline = Pipeline([\n",
        "                     ('cnt_vect', CountVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
        "                     ('lr_clf', LogisticRegression(C=10))\n",
        "])\n",
        "pipeline.fit(X_train['review'], y_train)\n",
        "pred = pipeline.predict(X_test['review'])\n",
        "pred_probs = pipeline.predict_proba(X_test['review'])[:, 1]\n",
        "\n",
        "print('예측 정확도:', accuracy_score(y_test, pred))\n",
        "print('ROC-AUC:', roc_auc_score(y_test, pred_probs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxJpGOPDDE_S",
        "outputId": "9f81b3f5-228f-4508-dcdb-f59b8f6dfff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 정확도: 0.886\n",
            "ROC-AUC: 0.9502705298201685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번엔 TF-IDF 기반으로 피처 벡터화를 수행하고 같은 과정을 구현해 보겠습니다."
      ],
      "metadata": {
        "id": "VN04EM8_Gk6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "                     ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
        "                     ('lr_clf', LogisticRegression(C=10))\n",
        "])\n",
        "pipeline.fit(X_train['review'], y_train)\n",
        "pred = pipeline.predict(X_test['review'])\n",
        "pred_probs = pipeline.predict_proba(X_test['review'])[:, 1]\n",
        "\n",
        "print('예측 정확도:', accuracy_score(y_test, pred))\n",
        "print('ROC-AUC:', roc_auc_score(y_test, pred_probs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugB7e58iGKpd",
        "outputId": "295bc9ca-e209-4064-a17c-465d8fb17abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 정확도: 0.8936\n",
            "ROC-AUC: 0.959799823582973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "성능이 약간 향상되었습니다."
      ],
      "metadata": {
        "id": "q7sIKAmrG6H1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN-cT4KLm2uo"
      },
      "source": [
        "## **5.3. 비지도학습 기반 감성 분석 소개**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "일반적으로 감성 분석용 데이터는 결정된 레이블 값을 갖고 있지 않으므로 비지도 감성 분석은 **Lexicon(어휘집)**을 기반으로 수행합니다. 참고로 한글은 Lexicon이 없습니다.\n",
        "\n",
        "비지도 학습에서 Lexicon은 주로 감성만을 분석하기 위해 지원하는 감성 어휘 사전입니다. 이 감성 사전은 긍정 감성이나 부정 감성의 정도를 나타내는 수치인 **감성 지수(Polarity score)**를 갖고 있습니다. 감성 지수는 단어 위치나 주변 단어, 문맥, POS(Part of Speech, 품사) 등을 고려하여 결정됩니다. 감성 사전을 구현한 패키지는 `NLTK`이며 많은 서브 모듈이 포함되어 있고, 감성 사전을 구현한 모듈은 `Lexicon`입니다.\n",
        "\n",
        "NLP 패키지의 `WordNet` 모듈은 영어 어휘 사전이며 시맨틱(Semantic, 문맥상 의미) 분석을 제공합니다. 같은 어휘라도 달리 사용되는 시맨틱 정보를 제공하며, 여러 품사로 구성된 개별 단어를 **Synset(Sets of cognitive synonyms)**이라는 개념을 토대로 표현합니다. Synset은 단순한 단어 하나가 아닌 그 단어가 가진 시맨틱 정보를 제공하는 개념입니다.\n",
        "\n",
        "다만 `NLTK` 패키지의 감성 사전은 좋지 않은 예측 성능을 보이기 때문에 실무에서는 `NLTK` 패키지가 아닌 다른 감성 사전을 적용합니다. 아래는 `NLTK`를 포함한 대표적인 감성 사전 몇 가지 목록입니다.\n",
        "\n",
        "- `SentiWordNet`: 감성 단어 전용의 `WordNet`을 구현했으며, `WordNet`의 Synset 개념을 감성 분석에 적용했습니다(`sentiwordnet` 인스턴스로 구현되어 있습니다). 이 Synset별로 3가지 감성 점수(sentiment score)인 긍정 감성 지수, 부정 감성 지수, 객관성 지수를 할당합니다. 문장별 단어들의 긍정 감성 지수와 부정 감성 지수를 합산하여 최종 감성 지수를 계산하고 긍정과 부정 여부를 판단합니다.\n",
        "\n",
        "- `VADER`: 소셜 미디어의 텍스트에 대한 감성 분석을 제공하는 패키지입니다. 뛰어난 감성 분석 결과를 제공하고 빠른 수행 시간을 보장하여 대용량 텍스트 데이터에 유용합니다.\n",
        "\n",
        "- `Pattern` 예측 성능 측면에서 가장 주목받는 패키지입니다. 다만 파이썬 3 버전에서는 호환이 되지 않습니다. "
      ],
      "metadata": {
        "id": "SCqD72lfHDjK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2st4Z0-m2sV"
      },
      "source": [
        "## **5.4. SentiWordNet을 이용한 감성 분석**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SentiWordNet`은 예측 정확도가 높지 않아서 잘 사용되지는 않습니다. 다만 감성 사전의 구성 방식과 시맨틱 기반 사전 구축 방식을 이해할 목적이라면 한 번쯤 살펴볼 가치가 있습니다. 실사용은 `sentiwordnet` 인스턴스를 호출하여 사용합니다. 이 인스턴스는 `LazyCorpusLoader` 클래스의 인스턴스입니다. "
      ],
      "metadata": {
        "id": "5prVppoCkkDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "help(swn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIxVN2zbjutX",
        "outputId": "76b17d0b-fd47-4e44-860e-3bdcb484e16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on LazyCorpusLoader in module nltk.corpus.util object:\n",
            "\n",
            "sentiwordnet = class LazyCorpusLoader(builtins.object)\n",
            " |  sentiwordnet(name, reader_cls, *args, **kwargs)\n",
            " |  \n",
            " |  To see the API documentation for this lazily loaded corpus, first\n",
            " |  run corpus.ensure_loaded(), and then run help(this_corpus).\n",
            " |  \n",
            " |  LazyCorpusLoader is a proxy object which is used to stand in for a\n",
            " |  corpus object before the corpus is loaded.  This allows NLTK to\n",
            " |  create an object for each corpus, but defer the costs associated\n",
            " |  with loading those corpora until the first time that they're\n",
            " |  actually accessed.\n",
            " |  \n",
            " |  The first time this object is accessed in any way, it will load\n",
            " |  the corresponding corpus, and transform itself into that corpus\n",
            " |  (by modifying its own ``__class__`` and ``__dict__`` attributes).\n",
            " |  \n",
            " |  If the corpus can not be found, then accessing this object will\n",
            " |  raise an exception, displaying installation instructions for the\n",
            " |  NLTK data package.  Once they've properly installed the data\n",
            " |  package (or modified ``nltk.data.path`` to point to its location),\n",
            " |  they can then use the corpus object without restarting python.\n",
            " |  \n",
            " |  :param name: The name of the corpus\n",
            " |  :type name: str\n",
            " |  :param reader_cls: The specific CorpusReader class, e.g. PlaintextCorpusReader, WordListCorpusReader\n",
            " |  :type reader: nltk.corpus.reader.api.CorpusReader\n",
            " |  :param nltk_data_subdir: The subdirectory where the corpus is stored.\n",
            " |  :type nltk_data_subdir: str\n",
            " |  :param *args: Any other non-keywords arguments that `reader_cls` might need.\n",
            " |  :param *kargs: Any other keywords arguments that `reader_cls` might need.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __getattr__(self, attr)\n",
            " |  \n",
            " |  __init__(self, name, reader_cls, *args, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __unicode__ = __str__(self, /)\n",
            " |  \n",
            " |  unicode_repr = __repr__(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qT6GGiOm2pB"
      },
      "source": [
        "### **5.4.1. WordNet Synset과 SentiWordNet SentiSynset 클래스의 이해**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SentiwordNet`은 `WordNet` 기반의 Synset을 이용하므로 Synset 개념부터 살피겠습니다. `nltk` 패키지의 모든 데이터 세트와 패키지를 다운로드하겠습니다.\n"
      ],
      "metadata": {
        "id": "c4V6KUrUlmwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "dbAFMGiWmn1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`wordnet` 모듈을 토대로 `synsets()` 메서드를 사용하여 단어의 Synset을 추출하겠습니다. "
      ],
      "metadata": {
        "id": "DIe4G7_SoJo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "term = 'present'\n",
        "synsets = wn.synsets(term)\n",
        "print('synsets type:', type(synsets))\n",
        "print('synsets 값 개수:', len(synsets))\n",
        "print('synsets 값 :', synsets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM7PG3berak1",
        "outputId": "3c06e395-eafa-45a6-c427-eb5b2cce8576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "synsets type: <class 'list'>\n",
            "synsets 값 개수: 18\n",
            "synsets 값 : [Synset('present.n.01'), Synset('present.n.02'), Synset('present.n.03'), Synset('show.v.01'), Synset('present.v.02'), Synset('stage.v.01'), Synset('present.v.04'), Synset('present.v.05'), Synset('award.v.01'), Synset('give.v.08'), Synset('deliver.v.01'), Synset('introduce.v.01'), Synset('portray.v.04'), Synset('confront.v.03'), Synset('present.v.12'), Synset('salute.v.06'), Synset('present.a.01'), Synset('present.a.02')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Synset` 클래스를 리스트 형태로 반환했슨비다. `synsets` 객체의 값 중에 `Synset('present.n.01')`만 살펴보겠습니다. 이 값은 POS(품사) 태그를 의미하며, 'present'는 의미, 'n'은 품사(명사를 의미함), '01'은 'present'가 명사로서 가지는 여러 의미를 구분하는 인덱스입니다.\n",
        "\n",
        "`synsets` 객체의 속성을 확인해 보겠습니다. Synset은 POS, 정의(Definition), 부명제(Lemma) 등으로 시맨틱적인 요소를 표현할 수 있습니다."
      ],
      "metadata": {
        "id": "5JQGegf42E62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for synset in synsets:\n",
        "    print('- Synset name:', synset.name(), '-')\n",
        "    print('POS:', synset.lexname())\n",
        "    print('Definition:', synset.definition())\n",
        "    print('Lemmas:', synset.lemma_names())\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNC_edOQ0gx4",
        "outputId": "ad993825-87d6-4372-e50f-3fb3de2b953d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Synset name: present.n.01 -\n",
            "POS: noun.time\n",
            "Definition: the period of time that is happening now; any continuous stretch of time including the moment of speech\n",
            "Lemmas: ['present', 'nowadays']\n",
            "\n",
            "- Synset name: present.n.02 -\n",
            "POS: noun.possession\n",
            "Definition: something presented as a gift\n",
            "Lemmas: ['present']\n",
            "\n",
            "- Synset name: present.n.03 -\n",
            "POS: noun.communication\n",
            "Definition: a verb tense that expresses actions or states at the time of speaking\n",
            "Lemmas: ['present', 'present_tense']\n",
            "\n",
            "- Synset name: show.v.01 -\n",
            "POS: verb.perception\n",
            "Definition: give an exhibition of to an interested audience\n",
            "Lemmas: ['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
            "\n",
            "- Synset name: present.v.02 -\n",
            "POS: verb.communication\n",
            "Definition: bring forward and present to the mind\n",
            "Lemmas: ['present', 'represent', 'lay_out']\n",
            "\n",
            "- Synset name: stage.v.01 -\n",
            "POS: verb.creation\n",
            "Definition: perform (a play), especially on a stage\n",
            "Lemmas: ['stage', 'present', 'represent']\n",
            "\n",
            "- Synset name: present.v.04 -\n",
            "POS: verb.possession\n",
            "Definition: hand over formally\n",
            "Lemmas: ['present', 'submit']\n",
            "\n",
            "- Synset name: present.v.05 -\n",
            "POS: verb.stative\n",
            "Definition: introduce\n",
            "Lemmas: ['present', 'pose']\n",
            "\n",
            "- Synset name: award.v.01 -\n",
            "POS: verb.possession\n",
            "Definition: give, especially as an honor or reward\n",
            "Lemmas: ['award', 'present']\n",
            "\n",
            "- Synset name: give.v.08 -\n",
            "POS: verb.possession\n",
            "Definition: give as a present; make a gift of\n",
            "Lemmas: ['give', 'gift', 'present']\n",
            "\n",
            "- Synset name: deliver.v.01 -\n",
            "POS: verb.communication\n",
            "Definition: deliver (a speech, oration, or idea)\n",
            "Lemmas: ['deliver', 'present']\n",
            "\n",
            "- Synset name: introduce.v.01 -\n",
            "POS: verb.communication\n",
            "Definition: cause to come to know personally\n",
            "Lemmas: ['introduce', 'present', 'acquaint']\n",
            "\n",
            "- Synset name: portray.v.04 -\n",
            "POS: verb.creation\n",
            "Definition: represent abstractly, for example in a painting, drawing, or sculpture\n",
            "Lemmas: ['portray', 'present']\n",
            "\n",
            "- Synset name: confront.v.03 -\n",
            "POS: verb.communication\n",
            "Definition: present somebody with something, usually to accuse or criticize\n",
            "Lemmas: ['confront', 'face', 'present']\n",
            "\n",
            "- Synset name: present.v.12 -\n",
            "POS: verb.communication\n",
            "Definition: formally present a debutante, a representative of a country, etc.\n",
            "Lemmas: ['present']\n",
            "\n",
            "- Synset name: salute.v.06 -\n",
            "POS: verb.communication\n",
            "Definition: recognize with a gesture prescribed by a military regulation; assume a prescribed position\n",
            "Lemmas: ['salute', 'present']\n",
            "\n",
            "- Synset name: present.a.01 -\n",
            "POS: adj.all\n",
            "Definition: temporal sense; intermediate between past and future; now existing or happening or in consideration\n",
            "Lemmas: ['present']\n",
            "\n",
            "- Synset name: present.a.02 -\n",
            "POS: adj.all\n",
            "Definition: being or existing in a specified place\n",
            "Lemmas: ['present']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Synset('present.n.01')`과 `Synset('present.n.02')`는 같은 명사지만 각각 다른 의미를 지닙니다. 이처럼 synset은 단어 하나가 지닌 여러 시맨틱 정보를 개별 클래스로 나타냅니다.\n",
        "\n",
        "`WordNet`은 단어 간 관계를 유사도로 나타낼 수 있습니다. `synsets` 객체는 단어 간 유사도를 나타내는 `path_similarity()` 메서드를 가집니다. 이 메서드를 사용하여 여러 단어의 상호 유사도를 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "uFw8mYQW-CKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "tree = wn.synset('tree.n.01')\n",
        "lion = wn.synset('lion.n.01')\n",
        "tiger = wn.synset('tiger.n.01')\n",
        "cat = wn.synset('cat.n.01')\n",
        "dog = wn.synset('dog.n.01')\n",
        "\n",
        "entities = [tree, lion, tiger, cat, dog]\n",
        "similarities = []\n",
        "entity_names = [entity.name().split('.')[0] for entity in entities]\n",
        "\n",
        "for entity in entities:\n",
        "    similarity = [round(entity.path_similarity(compared_entity), 2) \\\n",
        "                  for compared_entity in entities]\n",
        "    similarities.append(similarity)\n",
        "\n",
        "similarity_df = pd.DataFrame(similarities, columns=entity_names, index=entity_names)\n",
        "similarity_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sqvTrL_XCOtr",
        "outputId": "ad22627b-055f-4766-f200-1aacecda429b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       tree  lion  tiger   cat   dog\n",
              "tree   1.00  0.07   0.14  0.08  0.12\n",
              "lion   0.07  1.00   0.08  0.25  0.17\n",
              "tiger  0.14  0.08   1.00  0.09  0.17\n",
              "cat    0.08  0.25   0.09  1.00  0.20\n",
              "dog    0.12  0.17   0.17  0.20  1.00"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea0a81c3-a5ce-44d5-a592-f390c2600af3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tree</th>\n",
              "      <th>lion</th>\n",
              "      <th>tiger</th>\n",
              "      <th>cat</th>\n",
              "      <th>dog</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>tree</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lion</th>\n",
              "      <td>0.07</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tiger</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.08</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.09</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dog</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea0a81c3-a5ce-44d5-a592-f390c2600af3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea0a81c3-a5ce-44d5-a592-f390c2600af3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea0a81c3-a5ce-44d5-a592-f390c2600af3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1에 가까울수록 상호 유사도가 높습니다.\n",
        "\n",
        "`SentiWordNet`은 `Wordnet`의 `synsets()` 메서드와 비슷한 `senti_synsets()` 메서드를 가집니다. 사용해 보겠습니다."
      ],
      "metadata": {
        "id": "jf4gz7G4Lwa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "# 리스트 형태로 변환하지 않으면 `filter` 형태입니다.\n",
        "senti_synsets = list(swn.senti_synsets('slow'))\n",
        "print('senti_synsets type:', type(senti_synsets))\n",
        "print('senti_synsets 값 개수:', len(senti_synsets))\n",
        "print('senti_synsets 값 :', senti_synsets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6xvHRwWMlrv",
        "outputId": "d78ddaff-1a90-4fa1-9c27-826a28b4544d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "senti_synsets type: <class 'list'>\n",
            "senti_synsets 값 개수: 11\n",
            "senti_synsets 값 : [SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'), SentiSynset('slow.v.03'), SentiSynset('slow.a.01'), SentiSynset('slow.a.02'), SentiSynset('dense.s.04'), SentiSynset('slow.a.04'), SentiSynset('boring.s.01'), SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'), SentiSynset('behind.r.03')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SentiSynset` 클래스를 리스트 형태로 반환했습니다. 이 클래스는 감성 지수와 객관성 지수를 출력하는 메서드 3개를 가집니다. 긍정 감성 지수를 출력하는 `pos_score()`, 부정 감성 지수를 출력하는 `neg_score()`, 객관성 지수를 출력하는 `odj_score()` 메서드입니다. 몇몇 단어에 적용해 보겠습니다."
      ],
      "metadata": {
        "id": "sUkRT6nlRaGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "term = 'mother'\n",
        "synsets = wn.synsets(term)\n",
        "print('synsets 값 개수:', len(synsets))\n",
        "print('synsets 값 :', synsets)\n",
        "print()\n",
        "\n",
        "for synset in synsets:\n",
        "    print('- Synset name:', synset.name(), '-')\n",
        "    print('POS:', synset.lexname())\n",
        "    print('Definition:', synset.definition())\n",
        "    print('Lemmas:', synset.lemma_names())\n",
        "    print()\n",
        "\n",
        "senti_synsets = list(swn.senti_synsets('mother'))\n",
        "print('senti_synsets 값 개수:', len(senti_synsets))\n",
        "print('senti_synsets 값 :', senti_synsets)\n",
        "print()\n",
        "\n",
        "mother = swn.senti_synset('mother.n.01')\n",
        "print('mother.n.01 긍정 감성 지수:', mother.pos_score())\n",
        "print('mother.n.01 부정 감성 지수:', mother.neg_score())\n",
        "print('mother.n.01 객관성 지수:', mother.obj_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ6CEIK4UbHH",
        "outputId": "35756814-34a0-470d-f395-d3ab55657ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "synsets 값 개수: 7\n",
            "synsets 값 : [Synset('mother.n.01'), Synset('mother.n.02'), Synset('mother.n.03'), Synset('mother.n.04'), Synset('mother.n.05'), Synset('mother.v.01'), Synset('beget.v.01')]\n",
            "\n",
            "- Synset name: mother.n.01 -\n",
            "POS: noun.person\n",
            "Definition: a woman who has given birth to a child (also used as a term of address to your mother)\n",
            "Lemmas: ['mother', 'female_parent']\n",
            "\n",
            "- Synset name: mother.n.02 -\n",
            "POS: noun.substance\n",
            "Definition: a stringy slimy substance consisting of yeast cells and bacteria; forms during fermentation and is added to cider or wine to produce vinegar\n",
            "Lemmas: ['mother']\n",
            "\n",
            "- Synset name: mother.n.03 -\n",
            "POS: noun.person\n",
            "Definition: a term of address for an elderly woman\n",
            "Lemmas: ['mother']\n",
            "\n",
            "- Synset name: mother.n.04 -\n",
            "POS: noun.person\n",
            "Definition: a term of address for a mother superior\n",
            "Lemmas: ['mother']\n",
            "\n",
            "- Synset name: mother.n.05 -\n",
            "POS: noun.cognition\n",
            "Definition: a condition that is the inspiration for an activity or situation\n",
            "Lemmas: ['mother']\n",
            "\n",
            "- Synset name: mother.v.01 -\n",
            "POS: verb.social\n",
            "Definition: care for like a mother\n",
            "Lemmas: ['mother', 'fuss', 'overprotect']\n",
            "\n",
            "- Synset name: beget.v.01 -\n",
            "POS: verb.body\n",
            "Definition: make children\n",
            "Lemmas: ['beget', 'get', 'engender', 'father', 'mother', 'sire', 'generate', 'bring_forth']\n",
            "\n",
            "senti_synsets 값 개수: 7\n",
            "senti_synsets 값 : [SentiSynset('mother.n.01'), SentiSynset('mother.n.02'), SentiSynset('mother.n.03'), SentiSynset('mother.n.04'), SentiSynset('mother.n.05'), SentiSynset('mother.v.01'), SentiSynset('beget.v.01')]\n",
            "\n",
            "mother.n.01 긍정 감성 지수: 0.0\n",
            "mother.n.01 부정 감성 지수: 0.0\n",
            "mother.n.01 객관성 지수: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "term = 'outstanding'\n",
        "synsets = wn.synsets(term)\n",
        "print('synsets 값 개수:', len(synsets))\n",
        "print('synsets 값 :', synsets)\n",
        "print()\n",
        "\n",
        "for synset in synsets:\n",
        "    print('- Synset name:', synset.name(), '-')\n",
        "    print('POS:', synset.lexname())\n",
        "    print('Definition:', synset.definition())\n",
        "    print('Lemmas:', synset.lemma_names())\n",
        "    print()\n",
        "\n",
        "senti_synsets = list(swn.senti_synsets('outstanding'))\n",
        "print('senti_synsets 값 개수:', len(senti_synsets))\n",
        "print('senti_synsets 값 :', senti_synsets)\n",
        "print()\n",
        "\n",
        "# `'great.s.02'`와 같은 지수입니다.\n",
        "outstanding = swn.senti_synset('outstanding.s.04')\n",
        "print('outstanding.s.04 긍정 감성 지수:', outstanding.pos_score())\n",
        "print('outstanding.s.04 부정 감성 지수:', outstanding.neg_score())\n",
        "print('outstanding.s.04 객관성 지수:', outstanding.obj_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-gn1uryUtze",
        "outputId": "108cf3b5-c053-49ab-b2b2-f6a137ce8aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "synsets 값 개수: 4\n",
            "synsets 값 : [Synset('outstanding.s.01'), Synset('outstanding.s.02'), Synset('outstanding.s.03'), Synset('great.s.02')]\n",
            "\n",
            "- Synset name: outstanding.s.01 -\n",
            "POS: adj.all\n",
            "Definition: distinguished from others in excellence\n",
            "Lemmas: ['outstanding']\n",
            "\n",
            "- Synset name: outstanding.s.02 -\n",
            "POS: adj.all\n",
            "Definition: having a quality that thrusts itself into attention\n",
            "Lemmas: ['outstanding', 'prominent', 'salient', 'spectacular', 'striking']\n",
            "\n",
            "- Synset name: outstanding.s.03 -\n",
            "POS: adj.all\n",
            "Definition: owed as a debt\n",
            "Lemmas: ['outstanding', 'owing', 'undischarged']\n",
            "\n",
            "- Synset name: great.s.02 -\n",
            "POS: adj.all\n",
            "Definition: of major significance or importance\n",
            "Lemmas: ['great', 'outstanding']\n",
            "\n",
            "senti_synsets 값 개수: 4\n",
            "senti_synsets 값 : [SentiSynset('outstanding.s.01'), SentiSynset('outstanding.s.02'), SentiSynset('outstanding.s.03'), SentiSynset('great.s.02')]\n",
            "\n",
            "outstanding.s.04 긍정 감성 지수: 0.75\n",
            "outstanding.s.04 부정 감성 지수: 0.0\n",
            "outstanding.s.04 객관성 지수: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6yqLAJ5m2mz"
      },
      "source": [
        "### **5.4.2. SentiWordNet을 이용한 영화 감상평 감성 분석**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB 영화 감상평 분석을 수행해 보겠습니다. `SentiWordNet`을 이용한 감성 분석은 아래 순서로 수행합니다.\n",
        "\n",
        "1. 문서를 문장 단위로 분해합니다.\n",
        "\n",
        "1. 다시 문장을 단어 단위로 토큰화하고 품사 태깅을 수행합니다(`WordNet` 사용).\n",
        "\n",
        "1. 품사 태깅을 수행한 단어를 기반으로 `synset` 객체와 `senti_synset` 객체를 생성합니다.\n",
        "\n",
        "1. `senti_synset`으로 감성 지수를 구하고 합산하여 특정 임계치 이상이면 긍정, 그렇지 않으면 부정으로 판단합니다.\n",
        "\n",
        "`SentiWordNet`을 이용하려면 `WordNet`을 이용하여 문서를 다시 단어로 토큰화하고 어근 추출(Lemmatization)과 품사 태깅(POS Tagging)을 적용해야 합니다. 먼저 품사 태깅을 수행하는 내부 함수를 선언하겠습니다."
      ],
      "metadata": {
        "id": "267Pyqi0YMxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# NLTK PennTreebank Tag를 기반으로 `WordNet` 기반 품사 Tag로 변환합니다.\n",
        "def penn_to_wn(tag):\n",
        "    # 형용사로 변환합니다.\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    # 명사로 변환합니다.\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    # 부사로 변환합니다.\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    # 동사로 변환합니다.\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB"
      ],
      "metadata": {
        "id": "FrgOzFllhd5Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 문서를 문장에서 단어 토큰화를 수행하고 품사 태깅을 수행한 후, `SentiSynset` 클래스를 생성하고, Polarity Score를 합산하는 함수를 선언하겠습니다. 총 감성 지수가 0 이상이면 긍정, 그렇지 않으면 부정으로 예측합니다. 함수 로직은 주석으로 설명하겠습니다."
      ],
      "metadata": {
        "id": "wGJvMYBMiZsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "\n",
        "def swn_polarity(text):\n",
        "    # 감성 지수를 초기화합니다.\n",
        "    sentiment = 0.0\n",
        "    # 토큰 개수를 초기화합니다.\n",
        "    tokens_count = 0\n",
        "\n",
        "    # 단어의 원형을 찾는 인스턴스를 생성합니다.\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # 문장을 토큰화하는 인스턴스를 생성합니다.\n",
        "    raw_sentences = sent_tokenize(text)\n",
        "    \n",
        "    # 분해된 문장별로 단어를 토큰화하고 품사 태깅을 수행한 후 `SentiSynset`을 생성하여\n",
        "    # 감성 지수를 합산합니다.\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # NLTK 기반의 품사 태깅 문장을 추출합니다.\n",
        "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
        "        for word, tag in tagged_sentence:\n",
        "            # WordNet 기반의 품사 태깅과 어근 추출을 수행합니다.\n",
        "            wn_tag = penn_to_wn(tag)\n",
        "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "                continue\n",
        "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "            if not lemma:\n",
        "                continue\n",
        "            # 어근을 추출한 단어와 WordNet 기반의 품사 태깅을 입력하여 Synset 객체를 생성합니다.\n",
        "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
        "            if not synsets:\n",
        "                continue\n",
        "            # `sentiwordnet`의 감성 단어 분석으로 감성 synset을 추출합니다.\n",
        "            # 긍정 감성 지수는 +, 부정 감성 지수는 -로 합산하여 감성 지수를 계산합니다.\n",
        "            synset = synsets[0]\n",
        "            swn_synset = swn.senti_synset(synset.name())\n",
        "            sentiment += (swn_synset.pos_score() - swn_synset.neg_score())\n",
        "            tokens_count += 1\n",
        "\n",
        "        if not tokens_count:\n",
        "            return 0\n",
        "\n",
        "        # 총 score가 0 이상이면 긍정 1, 그렇지 않으면 부정 0을 반환합니다.\n",
        "        if sentiment >= 0:\n",
        "            return 1\n",
        "        return 0"
      ],
      "metadata": {
        "id": "M7B9TwBPjPCx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수들을 토대로 개별 문서에 적용하여 긍정과 부정을 예측하겠습니다. 'preds' 칼럼을 추가하여 감성 평가를 넣고 `SentiWordNet`의 감성 분석 예측 성능을 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "OoMGoCF7Kdgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_df['preds'] = review_df['review'].apply(lambda x: swn_polarity(x))\n",
        "y_target = review_df['sentiment'].values\n",
        "preds = review_df['preds'].values"
      ],
      "metadata": {
        "id": "LefirGxIXXPv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
        "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "print(confusion_matrix(y_target, preds))\n",
        "print('정확도:', np.round(accuracy_score(y_target, preds), 4))\n",
        "print('정밀도:', np.round(precision_score(y_target, preds), 4))\n",
        "print('재현율:', np.round(recall_score(y_target, preds), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C20fXyEbNsF",
        "outputId": "147d8fe6-21d3-4f8d-85aa-1fd47b7ceeb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7668 4832]\n",
            " [3636 8864]]\n",
            "정확도: 0.6613\n",
            "정밀도: 0.6472\n",
            "재현율: 0.7091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "만족스러운 수치는 아닙니다."
      ],
      "metadata": {
        "id": "i3iAqsxqmdf3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17w6YaYm2lH"
      },
      "source": [
        "## **5.5. VADER를 이용한 감성 분석**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VADER**는 소셜 미디어의 감성 분석 용도로 개발된 룰 기반의 Lexicon입니다. `SentimentIntensityAnalyzer` 클래스를 이용하여 쉽게 감성 분석을 제공합니다. VADER는 NLTK 패키지의 서브 모듈로 제공되거나 단독 패키지로 제공됩니다. VADER를 NLTK의 서브 모듈로 설치하려면 위에서 수행한 `nltk.download('all')` 코드를 취하면 됩니다. 별도 모듈로 사용하려면 `pip install vaderSentiment` 코드를 취하고, `from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer` 코드로 클래스를 임포트합니다.\n",
        "\n",
        "이제 VADER의 사용법을 살펴보겠습니다. `nltk` 패키지에 포함된 서브 모듈 `vader`의 `SentimentIntensityAnalyzer` 클래스를 사용하겠습니다."
      ],
      "metadata": {
        "id": "Qoe0yuIhLtHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "senti_analyzer = SentimentIntensityAnalyzer()\n",
        "# `polarity_scores()` 메서드로 감성 점수를 반환합니다.\n",
        "senti_scores = senti_analyzer.polarity_scores(review_df['review'][0])\n",
        "print(senti_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqklB49SQBHS",
        "outputId": "c65c7923-3864-4948-ee5d-da59581805ca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.13, 'neu': 0.743, 'pos': 0.127, 'compound': -0.7943}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'neg'는 부정 감성 지수, 'neu'는 중립 감성 지수, 'pos'는 긍정 감성 지수입니다. 'compound'는 세 지수를 조합하여 -1 ~ 1 사이의 감성 지수를 표현한 값입니다. 0.1 이상이면 긍정이고, 그렇지 않으면 부정으로 판단하지만 상황에 따라 조정하여 예측 성능을 조절할 수 있습니다.\n",
        "\n",
        "VADER를 사용하여 IMDB 감성 분석을 수행해 보겠습니다. `vader_polarity()` 함수를 선언하여 감성 분석에 활용하겠습니다. 함수 로직에 대한 설명은 주석을 토대로 하겠습니다."
      ],
      "metadata": {
        "id": "yZ0norW1RU5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 영화 감상평 텍스트(`review`), 긍정과 부정을 결정하는 임곗값(`threshold`)을 받습니다.\n",
        "def vader_polarity(review, threshold=0.1):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    scores = analyzer.polarity_scores(review)\n",
        "\n",
        "    # 'compound' 값을 토대로 `threshold` 파라미터보다 크면 1, 그렇지 않으면 0을 반환합니다.\n",
        "    agg_score = scores['compound']\n",
        "    final_sentiment = 1 if agg_score >= threshold else 0\n",
        "    return final_sentiment"
      ],
      "metadata": {
        "id": "lD_YhYYvURwe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수들을 토대로 개별 문서에 적용하여 긍정과 부정을 예측하겠습니다. 'vader_preds' 칼럼을 추가하여 감성 평가를 넣고, 이 감성 평가로 VADER의 예측 성능을 확인하겠습니다.\n"
      ],
      "metadata": {
        "id": "M6wbm1uMVOBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
        "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "review_df['vader_preds'] = review_df['review'].apply(lambda x: vader_polarity(x, 0.1))\n",
        "y_target = review_df['sentiment'].values\n",
        "vader_preds = review_df['vader_preds'].values\n",
        "\n",
        "print(confusion_matrix(y_target, vader_preds))\n",
        "print('정확도:', np.round(accuracy_score(y_target, vader_preds), 4))\n",
        "print('정밀도:', np.round(precision_score(y_target, vader_preds), 4))\n",
        "print('재현율:', np.round(recall_score(y_target, vader_preds), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81qJzHYEY4u0",
        "outputId": "f3444c76-5773-48aa-cee7-0dc5de04b59a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6736  5764]\n",
            " [ 1867 10633]]\n",
            "정확도: 0.6948\n",
            "정밀도: 0.6485\n",
            "재현율: 0.8506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SentiWordNet`보다 전반적으로 향상됐고 특히 재현율은 높은 수치로 올랐습니다. 지도학습을 기반으로 한 감성 분석보다는 낮은 성능을 보이지만 결정 클래스 값이 주어지지 않은 상황을 고려하면 적당히 만족할 만한 성능이라고 판단할 수 있습니다."
      ],
      "metadata": {
        "id": "y0r7RC_NarIb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xh-YI-0m2jc"
      },
      "source": [
        "# **6. 토픽 모델링(Topic Modeling) - 20 뉴스그룹**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**토픽 모델링**은 문서 집합에 숨어 있는 주제를 찾아내는 작업입니다. 사람이 수행하는 토픽 모델링은 더 함축적인 의미로 문장을 요약하는 것뿐이지만 ML 기반 토픽 모델링은 숨어 있는 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출합니다.\n",
        "\n",
        "ML 기반 토픽 모델링에 자주 사용되는 기법은 **LSA(Latent Semantic Analysis)**와 **LDA(Latent Dirichlet Allocation)**입니다. 위에서 사용했던 20 뉴스그룹 데이터를 사용하여 LSA를 토대로 토픽 모델링을 수행해 보겠습니다. 20 뉴스그룹 데이터가 가진 주제 20가지 목록(`target_names` 속성)은 아래와 같습니다.\n",
        "\n",
        "```\n",
        "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
        "```\n",
        "\n",
        "여기에서 8가지 주제를 추출하고 텍스트에 LDA 기반 토픽 모델링을 적용하겠습니다. LDA는 카운트 기반 벡터화만 사용하므로 카운트 기반으로 벡터화하겠습니다."
      ],
      "metadata": {
        "id": "HrwBIim3cnM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# 모토사이클, 야구, 그래픽스, 윈도우, 중동, 기독교, 전자공학, 의학 주제를 추출합니다.\n",
        "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x',\n",
        "        'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med']\n",
        "news_df = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'),\n",
        "                             categories=cats, random_state=0)\n",
        "\n",
        "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english',\n",
        "                             ngram_range=(1, 2))\n",
        "feat_vect = count_vect.fit_transform(news_df.data)\n",
        "print('CountVectorizer Shape:', feat_vect.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji20yvMYwmPV",
        "outputId": "6c824f09-504e-4b67-e7e7-d110af46cc0f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer Shape: (7862, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문서 7862개에 피처 1000개로 구성된 행렬 데이터입니다. 이 피처 벡터화가 수행된 데이터 세트를 기반으로 LDA 토픽 모델링을 수행합니다. 토픽 개수는 8개로 정할 것이므로 `LatentDirichletAllocation` 클래스의 `n_components` 파라미터에 `8`을 지정하겠습니다.\n"
      ],
      "metadata": {
        "id": "dETxwuVsynZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
        "lda.fit(feat_vect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1PI8wd64938",
        "outputId": "db329169-b368-4ca2-cf53-fda46305a1ae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(n_components=8, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 코드 과정을 통해 선언한 인스턴스는 `components_` 속성을 갖습니다. 이 속성은 토픽별로 각 단어 피처가 얼마나 많이 그 토픽에 할당됐는지를 의미하는 수치를 가집니다. 높은 값일수록 해당 단어 피처는 그 토픽의 중심 단어가 됩니다. 속성을 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "qSnFliE65WX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda.components_)\n",
        "print(lda.components_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D9k1yiK5wgm",
        "outputId": "7ea3d331-42d1-43cb-c1d3-38980852b332"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.60992018e+01 1.35626798e+02 2.15751867e+01 ... 3.02911688e+01\n",
            "  8.66830093e+01 6.79285199e+01]\n",
            " [1.25199920e-01 1.44401815e+01 1.25045596e-01 ... 1.81506995e+02\n",
            "  1.25097844e-01 9.39593286e+01]\n",
            " [3.34762663e+02 1.25176265e-01 1.46743299e+02 ... 1.25105772e-01\n",
            "  3.63689741e+01 1.25025218e-01]\n",
            " ...\n",
            " [3.60204965e+01 2.08640688e+01 4.29606813e+00 ... 1.45056650e+01\n",
            "  8.33854413e+00 1.55690009e+01]\n",
            " [1.25128711e-01 1.25247756e-01 1.25005143e-01 ... 9.17278769e+01\n",
            "  1.25177668e-01 3.74575887e+01]\n",
            " [5.49258690e+01 4.47009532e+00 9.88524814e+00 ... 4.87048440e+01\n",
            "  1.25034678e-01 1.25074632e-01]]\n",
            "(8, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다만 이 속성 값만으로 토픽별 단어 연관도를 확인하기 어렵습니다. 새 함수를 선언하여 토픽별로 연관도가 높은 단어를 순서대로 나타내 보겠습니다.\n",
        "\n",
        "---\n",
        "\n",
        "정리 중\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XYyZXPzP65fM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R4yoUPwm2ge"
      },
      "source": [
        "# **7. 문서 군집화 소개와 실습(Opinion Review 데이터 세트)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzRQflwLm2eR"
      },
      "source": [
        "## **7.1. 문서 군집화 개념**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U84ogXlInWQq"
      },
      "source": [
        "## **7.2. Opinion Review 데이터 세트를 이용한 문서 군집화 수행하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZYxqdDcnXIM"
      },
      "source": [
        "## **7.3. 군집별 핵심 단어 추출하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66m82F1enXER"
      },
      "source": [
        "# **8. 문서 유사도**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpICe-EXnXCZ"
      },
      "source": [
        "## **8.1. 문서 유사도 측정 방법 - 코사인 유사도**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3OSzKYFnW_J"
      },
      "source": [
        "## **8.2. 두 벡터 사잇각**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKmMJXmknW8o"
      },
      "source": [
        "## **8.3. Opinion Review 데이터 세트를 이용한 문서 유사도 측정**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdjM_H4InWnq"
      },
      "source": [
        "# **9. 한글 텍스트 처리 - 네이버 영화 평점 감성 분석**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbjICpkboprM"
      },
      "source": [
        "## **9.1. 한글 NLP 처리의 어려움**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2t-D6c1opoS"
      },
      "source": [
        "## **9.2. KoNLPy 소개**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMa2Gi4Sopkr"
      },
      "source": [
        "## **9.3. 데이터 로딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgatqQtLophs"
      },
      "source": [
        "# **10. 텍스트 분석 실습 - 캐글 Mercari Price Suggestion Challenge**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o6HaNuGpg3W"
      },
      "source": [
        "## **10.1. 데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1-_U6j0pg1B"
      },
      "source": [
        "## **10.2. 피처 인코딩과 피처 벡터화**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VmXSu0FpgyW"
      },
      "source": [
        "## **10.3. 릿지 회귀 모델 구축 및 평가**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5yvI_x-pgv2"
      },
      "source": [
        "## **10.4. LightGBM 회귀 모델 구축과 앙상블을 이용한 최종 예측 평가**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p347TSeNpgtR"
      },
      "source": [
        "# **11. 정리**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CHAPTER8 텍스트 분석.ipynb",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMVABzyck/chidzDaiYGD2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}